[{"doctype":"documentation","id":"references/FluxTraining.Callback","title":"Callback","text":" abstract   type   Callback  Supertype of all callbacks .  Callbacks add custom functionality to the training loop by hooking into different  Events.Event s Any  Callback  can be used by passing it to  Learner .  See subtypes(FluxTraining.Callback)  for implementations . Extending See  Custom callbacks  for a less succinct tutorial format . Create a  struct MyCallback  that subtypes  FluxTraining.Callback . Add event handlers by implementing methods for on (event, phase, callback, learner) .  Methods should always dispatch on your callback, and may dispatch on specific  Phases.Phase s and  Events.Event s . For example, to implement an event handler that runs at the end of every step during training:  on(::StepEnd, ::AbstractTrainingPhase, ::MyCallback, learner) . Define what state the callback accesses and/or modifies by implementing stateaccess (::MyCallback) .  While  learner  is always passed as an argument to  on  event handlers, by default a callback can not read or write to its fields .  See  stateaccess  for more detail . If a callback needs to write some state that other callbacks should be able to access, it can store it in  learner.cbstate  if you add a permission in stateaccess . If the callback needs some one - time initialization, you can implement  init! which will be run at least once before any step is run ."},{"doctype":"documentation","id":"references/FluxTraining.LogHyperParams","title":"LogHyperParams","text":" LogHyperParams ( backends ... )  <:  Callback Callback that logs hyperparameters to one or more  LoggerBackend s . See also  LoggerBackend ,  Loggables.Loggable ,  log_to , TensorBoardBackend Example logcb   =  LogHyperParams ( TensorBoardBackend ( \"tblogs\" ) )\n schedule   =  ...\n Learner ( model ,  data ,  opt ,  lossfn ,  Scheduler ( LearningRate   =>  schedule ) ,  logcb )"},{"doctype":"documentation","id":"references/FluxTraining.UnsafeCallback","title":"UnsafeCallback","text":""},{"doctype":"documentation","id":"references/FluxTraining.iterpairs","title":"iterpairs","text":" iterpairs ( a ) Iterators over the Cartesian product of  a  with itself, skipping any pairs  (a, b)  where  a == b ."},{"doctype":"document","id":"documents/docs/background/dataiterator.md","title":"Data iterators","text":" Data iterators A data iterator is an iterator over batches of the data that is used for one step of fitting .  You can use different data iterators with this package, as long as they have the following properties . Firstly, you must be able to iterate over a data iterator: for   batch   in  dataiter \n    # step\n  end The data iterator must also be compatible with the other components of the  Learner .  For the standard supervised learning step ( TrainingPhase  and  ValidationPhase ), this means batch  is a tuple  (xs, ys)  of encoded inputs and targets, xs  is a valid input to the  model , so  ŷs = model(xs) ; and ys  can be compared to the model output with the  loss function , i . e .   lossfn(ŷs, ys) If you are working with a  custom training loop , you may need to satisfy additional or different properties . Creating data iterators The simplest data iterator is a vector of preloaded batches .  This is what we ’ re using in the  MNIST tutorial .  This is a fine approach for smaller datasets, but has some limitations . First of all, there is no principled way for doing things like splitting, subsetting and shuffling data .  For this, we recommend using  MLDataPattern . jl  which provides this functionality and many more utilities for defining and working with datasets . Another issue is that of memory load: if the whole dataset is too large to be preloaded in to memory, we have to load individual batches during training .  To do this in a way that doesn ’ t slow down the training itself, we suggest using  DataLoaders . jl .  DataLoaders . jl is compatible with MLDataPattern . jl and allows you to easily create efficient data iterators for out - of - memory datasets .  The documentation of DataLoaders . jl also has a lot more information on working with large dataset for deep learning ."},{"doctype":"documentation","id":"references/FluxTraining.ES.StoppingCriterion","title":"StoppingCriterion","text":""},{"doctype":"document","id":"documents/docs/callbacks/usage.md","title":"How to use callbacks","text":" using   FluxTraining \n using   FluxTraining :  Callback ,  Read ,  Write ,  stateaccess \n model ,  data ,  lossfn ,  optimizer   =  nothing ,  ( ) ,  nothing ,  nothing   How to use callbacks Callbacks allow injecting functionality at many points during the training loop . To use them, simply pass each callback to  Learner : learner   =  Learner (\n     model ,  data ,  optimizer ,  lossfn ,  # required arguments\n     ToGPU ( ) ,  Metrics ( accuracy ) )      # pass any number of callbacks as additional arguments  Some useful callbacks are added by default: learner . callbacks . cbs  See  callback reference  for a list of all callbacks included in  FluxTraining . jl  and their documentation . Ordering The order the callbacks are passed in doesn ’ t matter .   FluxTraining . jl  creates a dependency graph that makes sure the callbacks are run in the correct order .  Read  custom callbacks  to find out how to create callbacks yourself ."},{"doctype":"documentation","id":"references/FluxTraining.Schedule","title":"Schedule","text":" Schedule ( ts ,  values ,  interpolations;   unit   =  : epoch;   kwargs ... ) Describes how the values of a hyperparameter change over the training . Examples # one-cycle schedule over 10 epochs from 0.01 over 0.1 to 0.001\n es   =  length ( learner . data . training )\n Schedule ( [ 0  es ,  2  es ,  10  es ] ,  [ 0.01 ,  0.1 ,  0.001 ] ,  [ sineio ( ) ,  sineio ( ) ] ) Schedule  is an alias for  Animations.Animation , see the Animations . jl documentation for more detailed information ."},{"doctype":"document","id":"documents/LICENSE.md","title":"LICENSE","text":" MIT License Copyright (c) 2020 lorenzoh  lorenz.ohly@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the  “ Software ” ), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software . THE SOFTWARE IS PROVIDED  “ AS IS ” , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT .  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE ."},{"doctype":"document","id":"documents/docs/background/lossfunction.md","title":"Loss functions","text":" Loss functions A loss function compares model outputs to true targets, resulting in a loss .  For a loss function to be compatible with the standard supervised training loop, the following properties must hold . Firstly, the loss function should accept the model outputs and targets, and return a single scalar value .  Given a  data iterator   dataiter  and a  model   model : xs ,  ys   =  dataiter \n ŷs   =  model ( xs )\n lossfn ( ŷs ,  ys )  isa  Number The loss function must also be differentiable, so that gradients can be calculated during training .  See  models  for more on how to check this . Creating loss functions Flux . jl comes with a lot of commonly used loss functions built - in in its submodule  Flux.Losses .  See  Flux . jl loss functions  for a complete reference . You can also write your own loss functions .  If you are using non - mutating array operations, there is a good chance that it will be differentiable and also be compatible with GPU arrays from  CUDA . jl ."},{"doctype":"documentation","id":"references/FluxTraining.RunFirst","title":"RunFirst","text":" abstract   type   RunFirst   <:  ConflictResolution  Return  RunFirst(cb1/cb2)  from  resolveconflict (cb1, cb2)  to indicate that one of the callbacks should always run before the other ."},{"doctype":"documentation","id":"references/FluxTraining.CancelEpochException","title":"CancelEpochException","text":" CancelEpochException ( message ) Throw during fitting to cancel the currently running epoch . This prematurely ends the current epoch without throwing an error . Must be thrown inside the context of  runepoch . Examples runepoch ( learner ,  phase )  do   _ \n     for   batch   in  batches \n         step! ( learner ,  phase ,  batch )\n         if   learner . step . loss   <  1.\n             throw ( CancelEpochException ( \"Reached target loss\" ) )\n         end \n     end \n end"},{"doctype":"documentation","id":"references/FluxTraining.TensorBoardBackend","title":"TensorBoardBackend","text":" TensorBoardBackend ( logdir [ ,  tb_overwrite ];\n     time = time ( ) ,\n     purge_step = nothing ,\n     step_increment = 1 ,\n     min_level = Logging . Info ) TensorBoard backend for logging callbacks .  Takes the same arguments as  TensorBoardLogger.TBLogger ."},{"doctype":"documentation","id":"references/FluxTraining.TimeThrottle","title":"TimeThrottle","text":""},{"doctype":"documentation","id":"references/FluxTraining.print_epoch_table","title":"print_epoch_table","text":""},{"doctype":"documentation","id":"references/FluxTraining.Write","title":"Write","text":""},{"doctype":"documentation","id":"references/FluxTraining.CHECKS","title":"CHECKS","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.Patience","title":"Patience","text":""},{"doctype":"document","id":"documents/docs/tutorials/hyperparameters.md","title":"Hyperparameter scheduling","text":" Hyperparameter scheduling When training neural networks, you often have to tune hyperparameters . In  FluxTraining . jl  the following definition is used: A hyperparameter is any state that influences the training and is not a parameter of the model . Common hyperparameters to worry about are the learning rate, batch size and regularization strength . In recent years, it has also become common practice to schedule some hyperparameters .  The cyclical learning rate schedule introduced in  L .  Smith 2015 , for example, changes the learning rate every step to speed up convergence . FluxTraining . jl  provides an extensible interface for hyperparameter scheduling that is not restricted to optimizer hyperparameters as in many other training frameworks .  To use it, you have to create a  Scheduler , a callback that can be passed to a  Learner . Scheduler ’ s constructor takes pairs of hyperparameter types and associated schedules . As an example LearningRate  is a hyperparameter type representing the optimizer ’ s step size; and schedule = Schedule([0, 10], [0.01, 0.001])  represents a linear scheduling over 10 epochs from  0.01  to  0.001 We can create the callback scheduling the learning rate according to  Scheduler(LearningRate => schedule) . Schedule s are built around  Animations . jl .  See that package ’ s documentation or  Schedule ’ s for more details on how to construct them . One - cycle learning rate Let ’ s define a  Schedule  that follows the above - mentioned cyclical learning rate schedule . The idea is to start with a small learning rate, gradually increase it, and then slowly decrease it again . For example, we could start with a learning rate of 0 . 01, increase it to 0 . 1 over 3 epochs, and then down to 0 . 001 over 7 epochs .  Let ’ s also use cosine annealing, a common practice that makes sure the values are interpolated more smoothly . In code, that looks like this: using   Animations :  sineio   # for cosine annealing\n\n es   =  length ( traindl )     # number of steps in an epoch\n\n schedule   =  Schedule (\n     [ 0  es ,  3  es ,  10  es ] ,          # the time steps (in training steps)\n     [ 0.01 ,  0.1 ,  0.001 ] ,  # the valus at the time steps\n     sineio ( ) ,            # the annealing function\n )\n\n learner   =  model ( model ,  data ,  opt ,  lossfn ,  Scheduler ( LearningRate   =>  schedule ) ) For convenience, you can also use the  onecycle  helper to create this  Schedule . Extending You can create and schedule your own hyperparameters . To do this, you will need to define a type for your hyperparameter, e . g .   abstract type MyParam <: HyperParameter end , how to set the hyperparameter by implementing  sethyperparameter! (learner, ::Type{MyParam}, value) what state needs to be accessed to set the hyperparameter by implementing  stateaccess (::Type{MyParam}) .  See  custom callbacks  for more info on why this is needed . Kinds of hyperparameters Hyperparameters don ’ t need to belong to the optimizer !  For example, you could create a hyperparameter for batch size .  That is not implemented here because this package is agnostic of the data iterators and the implementation would differ for every type of iterator ."},{"doctype":"documentation","id":"references/FluxTraining.CallbackCondition","title":"CallbackCondition","text":" abstract   type   CallbackCondition  Supertype for conditions to use with  ConditionalCallback . To implement a  CallbackCondition , implement shouldrun (::MyCondition, event, phase) . See  FrequencyThrottle ,  TimeThrottle  and  throttle ."},{"doctype":"document","id":"documents/docs/overview.md","title":"Getting started","text":" Getting started There are some user - centric tutorials that will introduce you to features of  FluxTraining . jl  one at a time Alternatively you can read more about how  FluxTraining . jl  is implemented and how you can extend it Tutorials MNIST training Hyperparameter scheduling Interfaces Training loop Find out how  the training loop  is built and how to customize it . Callbacks Callbacks are a powerful feature of  FluxTraining . jl , allowing you to add functionality to the training loop at different points . Find out how to  use callbacks  when training what callbacks come  included with  FluxTraining . jl ; or how callbacks work and how to  implement your own"},{"doctype":"documentation","id":"references/FluxTraining.SanityCheckException","title":"SanityCheckException","text":""},{"doctype":"documentation","id":"references/FluxTraining.ToGPU","title":"ToGPU","text":" ToGPU ( ) Callback that moves model and batch data to the GPU during training . Convenience for  ToDevice (Flux.gpu) ."},{"doctype":"documentation","id":"references/FluxTraining.Loggables","title":"Loggables","text":""},{"doctype":"documentation","id":"references/FluxTraining.stepvalue","title":"stepvalue","text":""},{"doctype":"documentation","id":"references/FluxTraining.findconflicts","title":"findconflicts","text":""},{"doctype":"document","id":"documents/docs/background/model.md","title":"Models","text":" Models FluxTraining . jl works with all  Flux . jl - compatible models .  Unless you are using a  custom training loop , a  model  is expected to take a single input  xs , which corresponds to the encoded inputs returned by your  data iterator .  This means the following has to work: xs ,  ys   =  first ( dataiter )\n ŷs   =  model ( xs ) model  also has to be differentiable .  If you ’ re composing Flux . jl layers, this is likely the case .  You can always make sure by testing: using   Flux ,  Zygote \n\n xs ,  ys   =  first ( dataiter )\n lossfn   =  Flux . mse \n grads   =  Zygote . gradient ( Flux . params ( model ) )  do \n      lossfn ( model ( xs ) ,  ys )\n end Creating models The simplest way to create a Flux . jl - compatible model is to use layers from Flux . jl .  A good entrypoint is  this tutorial in Flux ’ s documentation . There is also a large number of packages that provide complete model architectures or domain - specific layers .  Below is a non - exhaustive list: Metalhead . jl  implements common model architectures for computer vision, GraphNeuralNetworks . jl  provides layers and utilities for graph neural networks, Transformers . jl  implements transformer models including pretrained language models"},{"doctype":"documentation","id":"references/FluxTraining.log_parameters","title":"log_parameters","text":""},{"doctype":"documentation","id":"references/FluxTraining.CustomCallback","title":"CustomCallback","text":" CustomCallback ( f ,  Event ,  [ TPhase   =  Phase ,  access   =  (;  ) ] ) A callback that runs  f(learner)  every time an event of type  Event during a phase of type in  Phase . If  f  needs to access learner state, pass  access , a named tuple in the same form as  stateaccess . Instead of using  CustomCallback  it is recommended to properly implement a  Callback . Examples We can get a quick idea of when a new epoch starts as follows: cb   =  CustomCallback ( learner   ->  println ( \"New epoch!\" ) ,  EpochBegin )"},{"doctype":"documentation","id":"references/FluxTraining.ES.UP","title":"UP","text":""},{"doctype":"documentation","id":"references/FluxTraining.step!","title":"step!","text":" step! ( learner ,  phase :: Phase ,  batch ) Run one step of training for  learner  on batch . Behavior is customized through  phase . Extending This is a required method for custom  Phase s to implement . To implement  step! , it is recommended you make use of  runstep to get begin and end events as well as proper handling of CancelStepException s . See the implementations of  TrainingPhase  and  ValidationPhase for reference ."},{"doctype":"documentation","id":"references/FluxTraining.ToDevice","title":"ToDevice","text":" ToDevice ( movefn [ ,  movemodelfn ] )  <:  Callback Moves model and step data to a device using  movedatafn  for step data and  movemodelfn  for the model .  For example  ToDevice(Flux.gpu, Flux.gpu) , moves them to a GPU if available .  See  ToGPU . By default, only moves  step.xs  and  step.ys , but this can be extended to other state by implementing  on(::StepBegin, ::MyCustomPhase, ::ToDevice, learner) ."},{"doctype":"documentation","id":"references/FluxTraining.FitException","title":"FitException","text":" abstract   type   FitException  Abstract types for exceptions that can be thrown during fitting, to change its control flow . See  CancelStepException ,  CancelEpochException ,  CancelFittingException ."},{"doctype":"documentation","id":"references/FluxTraining.errorwriteconflict","title":"errorwriteconflict","text":""},{"doctype":"documentation","id":"references/FluxTraining.TrainingPhase","title":"TrainingPhase","text":""},{"doctype":"documentation","id":"references/FluxTraining.NotDefined","title":"NotDefined","text":" abstract   type   NotDefined   <:  ConflictResolution  The default implementation of  resolveconflict .  If a conflict is detected, this ensures an error message is printed ."},{"doctype":"documentation","id":"references/FluxTraining.ES.PQ","title":"PQ","text":""},{"doctype":"documentation","id":"references/FluxTraining.ValidationPhase","title":"ValidationPhase","text":""},{"doctype":"documentation","id":"references/FluxTraining.LogHistograms","title":"LogHistograms","text":" LogHistograms ( backends ... [;  freq   =  100 ] )  <:  Callback Callback that logs histograms of model weights to  LoggerBackend s backends  every  freq  steps . If histograms should be logged every step, pass  freq = nothing"},{"doctype":"documentation","id":"references/FluxTraining.reset!","title":"reset!","text":""},{"doctype":"documentation","id":"references/FluxTraining.MetricsPrinter","title":"MetricsPrinter","text":" MetricsPrinter ( )  <:  Callback Callback that prints metrics after every epoch .  Relies on the metrics computed by Metrics , so will error if no  Metrics  callback is used . This callback is added by default to every  Learner  unless you pass in usedefaultcallbacks = false ."},{"doctype":"documentation","id":"references/FluxTraining","title":"FluxTraining","text":""},{"doctype":"documentation","id":"references/FluxTraining.runafter","title":"runafter","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.InvalidValue","title":"InvalidValue","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Audio","title":"Audio","text":""},{"doctype":"documentation","id":"references/FluxTraining.HyperParameter","title":"HyperParameter","text":" HyperParameter { T } A hyperparameter is any state that influences the training and is not a parameter of the model . Hyperparameters can be scheduled using the  Scheduler callback ."},{"doctype":"documentation","id":"references/FluxTraining.Metric","title":"Metric","text":" Metric ( metricfn [;  statistic ,  device ,  name ] ) Implementation of  AbstractMetric  that can be used with the Metrics  callback . Arguments Positional: metricfn(ŷs, ys)  should return a number . Keyword: statistic  is a  OnlineStats.Statistic  that is updated after every step . The default is  OnlineStats.Mean() name  is used for printing . device  is a function applied to  ŷs  and  ys before passing them to  metricfn .  The default is  Flux.cpu  so that the callback works if  metricfn  doesn ’ t support arrays from other device types .  If, for example,  metricfn  works on  CurArray s, you can pass device = Flux.gpu . phase = Phase : a (sub)type of  Phase  that restricts for which phases the metric is computed . Examples Metric(accuracy) Metric(Flux.mse, device = gpu, name = \"Mean Squared Error\") Metric(Flux.mae, device = gpu) cb   =  Metric ( Flux . mse ,  device   =  gpu ,  name   =  \"Mean Squared Error\" ) If a metric is expensive to compute and you don ’ t want it to slow down the training phase, you can compute it on the validation phase only: cb   =  Metric ( expensivemetric ,  P   =  ValidationPhase )"},{"doctype":"documentation","id":"references/FluxTraining.runstep","title":"runstep","text":" runstep ( stepfn ,  learner ,  phase )  ->  state Run  stepfn  inside the context of a step .  Calls  stepfn(handle, state) where  handle(e)  can be called to dispatch events and  state  is a  PropDict which step data, gradients and losses can be written to .  Return  state . Takes care of dispatching  StepBegin  and  StepEnd events as well as handling  CancelStepException s ."},{"doctype":"documentation","id":"references/FluxTraining.@pack_History!","title":"@pack_History!","text":""},{"doctype":"document","id":"documents/toc.md","title":"toc","text":" README Getting started Features Tutorials Training an image classifier Hyperparameter scheduling Custom callbacks Custom training loops How to Use callbacks Tips  &  tricks Reference Docstrings Callbacks Changelog"},{"doctype":"documentation","id":"references/FluxTraining.setlearningrate!","title":"setlearningrate!","text":""},{"doctype":"documentation","id":"references/FluxTraining.Checkpointer","title":"Checkpointer","text":" Checkpointer ( folder ) Saves  learner.model  to  folder  after every  AbstractTrainingPhase . Use  FluxTraining. loadmodel  to load a model ."},{"doctype":"documentation","id":"references/FluxTraining.CheckModelLossStep","title":"CheckModelLossStep","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.Never","title":"Never","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.Threshold","title":"Threshold","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.AbstractValidationPhase","title":"AbstractValidationPhase","text":" abstract   type   AbstractValidationPhase   <:  Phase  An abstract type for phases where no parameter updates are being made .  This exists so callbacks can dispatch on it and work with custom validation phases . The default implementation for supervised tasks is  ValidationPhase ."},{"doctype":"documentation","id":"references/FluxTraining.paramsrec","title":"paramsrec","text":""},{"doctype":"documentation","id":"references/FluxTraining.sethyperparameter!","title":"sethyperparameter!","text":" sethyperparameter! ( learner ,  H ,  value ) Sets hyperparameter  H  to  value  on  learner ."},{"doctype":"documentation","id":"references/FluxTraining.ProtectedException","title":"ProtectedException","text":""},{"doctype":"document","id":"documents/docs/custom_training.ipynb","title":"Custom training loops","text":" Custom training loops How can we compose changes to the training loop together? As an example, we want to be able to do data - parallel training or GAN training, but also data - parallel GAN training . I want to give some thoughts on this and propose possible solutions to make this possible in  FastAI . jl . Currently, implementing custom training behavior is possible by subtyping  FitPhase  and implementing  fitbatchphase!(learner, phase::MyPhase) .  However, this is not composable, i . e .  it doesn ’ t allow you to combine a  DataParallelTrainingPhase  and  GANTrainingPhase . Below are some examples that show how the contents of  fitbatchphase!  can be changed to illustrate this .  For simplicity, they don ’ t include callbacks and state handling . Regular training (1) # inputs: model, xs, ys, lossfn, optim, params\ngrads = gradient(params) do\n    return lossfn(model(xs), ys)\nend\nupdate!(optim, params, sum(grads)) Data - parallel training on CPU (2) # inputs: model, xs, ys, lossfn, optim, params\n\ngrads = Array{Grads}(undef, Threads.nthreads())\n\n# run equally-sized slices of the batch in parallel (naive pseudocode)\nThreads.@threads for (i, (xs_, ys_)) in enumerate(scatter((xs, ys), Threads.nthreads()))\n    grads[i] = gradient(params) do\n        return lossfn(model(xs_), ys_)\n    end\nend\ngs = sum(grads)\n\n# do parameter update with summed gradients\nupdate!(optim, params, gs) GAN training (3) # inputs: mgen, mcrit, paramsgen, paramscrit, xs_true, lossfngen, lossfncrit, optim, batchsize\n\n# critic step\nxs_fake = mgen(batchsize)\nxs = cat(xs_true, xs_fake)\nys = onehot.(vcat(trues(batchsize), falses(batchsize)))\n\ngrads = gradient(paramscrit) do\n    return lossfncrit(mcrit(xs), ys)\nend\nupdate!(optim, paramscrit, grads)\n\n# generator step\ngrads = gradient(paramsgen) do\n    xs_fake = mgen(batchsize)\n    ys_fake = onehot.(falses(batchsize))\n    return -lossfncrit(crit(xs_fake), ys_fake)\nend Solutions I have found two approaches to deal with this .   Both focus on removing execution logic from  fitbatchphase! , making them composable with custom  Phase s  like  GANTrainingPhase  that change the  semantics  of the training loop . On one hand, there are extensions to the training loop that change the  execution  (e . g .  parallel and distributed CPU and GPU training), on the other hand you have those that change the  semantics  (e . g .  GAN training) . The proposed solutions make the assumption that different  semantics  don ’ t need to be composed, but should be composable with different execution contexts . (S1) Abstract gradient step (and possibly others) out Modifications to the execution of the training loop could be implemented by wrapping in an execution context . In the below example  gradientphase  could dispatch to the regular gradient calculation in (1) or the data - parallel approach (2) depending on  executionctx . This would mean that only  semantic  changes to the training loop would use overloading of  fitbatchphase!  with a custom  FitPhase .  Changes to the  execution  work by dispatching on execution contexts, e . g .   gradientphase(::Linear, ...)  or  gradientphase(::DataParallel, ...) . # gradientphase passes (possibly modified) state to a closure\n# in the case executionctx::DataParallel, xs_, ys_ will be slices\n# of the batch.\ngrads = gradient(executionctx, params) do model_, params_, xs_, ys_, \n    return lossfn(model(xs_), ys_)\nend\nupdate!(optim, params, grads) Advantages implementation definitely doable Disadvantages implementation dependent on requirements, i . e .  unsure which pieces of the training step need to be overloadable and which state needs to be passed to closures . (S2) Wrapper for  model The idea is to wrap the  model  in an execution context, e . g .   DataParallel(model) .  The wrapper is then responsible for exhibiting the correct behavior on the forward and backward pass .  This is  what PyTorch does . No changes to the training loop would need to be made .  The implementation for the forward pass should be straightforward and similar to the above sketch (2), however I ’ m not sure how to make sure that the backward pass is also computed in parallel (can custom gradient definitions include multi - threading code?, what about the loss function that is not wrapped?) . # before training\nmodel = DataParallel(model)\n\n# training step doesn't change at all\ngrads = gradient(params) do\n    return lossfn(model(xs), ys)\nend\nupdate!(optim, params, sum(grads)) Advantages no changes needed to state and event handling Disadvantages not sure if such a simple API is possible to implement for all scenarios bit unelegant; model is not a pure function anymore"},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Loggable","title":"Loggable","text":" abstract   type   Loggable  Abstract type for data that  LoggerBackend s can log . See  subtypes(FluxTraining.Loggables.Loggable)  and  LoggerBackend"},{"doctype":"documentation","id":"references/FluxTraining.runepoch","title":"runepoch","text":" runepoch ( epochfn ,  learner ,  phase ) Run  epochfn  inside the context of an epoch .  Calls  epochfn(handle) where  handle(e)  can be called to dispatch events . Takes care of dispatching  EpochBegin  and  EpochEnd events as well as handling  CancelEpochException s ."},{"doctype":"documentation","id":"references/FluxTraining.formataccess","title":"formataccess","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Histogram","title":"Histogram","text":""},{"doctype":"document","id":"documents/docs/features.md","title":"Features","text":" Features This page gives a run - down of many features  FluxTraining . jl  brings to the table .   Most features are implemented as callbacks and using them is as simple as passing the callback when constructing a  Learner  and training with  fit! : cb   =  CoolFeature🕶️Callback ( )\n learner   =  Learner ( model ,  data ,  opt ,  lossfn ,  cb )\n fit! ( learner ,  nepochs ) Metrics By default,  Learner  will track only the loss function .  You can track other metric with the  Metrics  callback .  See also  Metric ,  AbstractMetric . Hyperparameter scheduling The  Scheduler  callback takes care of hyperparameter scheduling .  See the  Hyperparameter scheduling tutorial  and also  Scheduler ,  Schedule ,  HyperParameter . Logging For logging, use the logging callbacks: LogMetrics LogHyperParams LogHistograms They each can have multiple logging backends, but right now the only one implemented in  FluxTraining . jl  is  TensorBoardBackend .  See also  LoggerBackend ,  log_to , and  Loggables.Loggable . There is also an external package  Wandb . jl  that implements a logging backend for  Weights & Biases . Checkpointing Use the  Checkpointer  callback to create model checkpoints after every epoch . Early Stopping Use  EarlyStopping  to stop when a stopping criterion is met .  Supports all criteria in  EarlyStopping . jl ."},{"doctype":"documentation","id":"references/FluxTraining.removecallback!","title":"removecallback!","text":" removecallback! ( learner ,  C ) Remove the first callback of type  C  from  learner  and return it . If there is none, return  nothing ."},{"doctype":"documentation","id":"references/FluxTraining.metricname","title":"metricname","text":""},{"doctype":"documentation","id":"references/FluxTraining.PropDict","title":"PropDict","text":" PropDict ( dict ) Like a  Dict{Symbol} , but attribute syntax can be used to access values ."},{"doctype":"documentation","id":"references/FluxTraining.CancelFittingException","title":"CancelFittingException","text":" CancelFittingException ( msg ) Throw during fitting to cancel it ."},{"doctype":"documentation","id":"references/FluxTraining.setfieldperm!","title":"setfieldperm!","text":""},{"doctype":"document","id":"documents/README.md","title":"FluxTraining.jl","text":" FluxTraining . jl Docs (master) A Julia package for using and writing powerful, extensible training loops for deep learning models . What does it do? Implements a training loop to take the boilerplate out of training deep learning models Lets you add features to training loops through reusable  callbacks Comes with callbacks for many common use cases like  hyperparameter scheduling ,  metrics  tracking and  logging ,  checkpointing ,  early stopping , and  more … Is extensible by creating  custom, reusable callbacks  or even  custom training loops When should you use FluxTraining . jl? You don ’ t want to implement your own metrics tracking and hyperparameter scheduling or  insert common training feature here  for the 10th time You want to use composable and reusable components that enhance your training loop You want a simple training loop with reasonable defaults that can grow to the needs of your project How do you use it? Install like any other Julia package using the package manager: ] add   FluxTraining After installation, import it, create a  Learner  from a  Flux . jl  model, data iterators, an optimizer, and a loss function .  Finally train with  fit! . using   FluxTraining \n\n learner   =  Learner ( model ,  ( trainiter ,  validiter ) ,  optim ,  lossfn )\n fit! ( learner ,  10 ) Next, you may want to read Getting started A full example training an image classifier on the MNIST dataset The  documentation of FastAI . jl  which features many end - to - end examples Acknowledgements The design of FluxTraining . jl ’ s two - way callbacks is adapted from  fastai ’ s training loop ."},{"doctype":"document","id":"documents/docs/tutorials/mnist.ipynb","title":"Training an image classifier","text":" Training an image classifier Let ’ s put  FluxTraining . jl  to train a model on the MNIST dataset . MNIST is simple enough that we can focus on the part where  FluxTraining . jl  comes in, the training .  If you want to see examples of using FluxTraining . jl on larger datasets, see the documentation of  FastAI . jl . Setup If you want to run this tutorial yourself, you can find the notebook file  here . To make data loading and batching a bit easier, we ’ ll install some additional dependencies: using   Pkg;   Pkg . add ( [ \"MLDataPattern\" ,  \"DataLoaders\" ] ) Now we can import everything we ’ ll need . using DataLoaders: DataLoader\nusing MLDataPattern: splitobs\nusing Flux\nusing FluxTraining Overview There are 4 pieces that you always need to construct and train a  Learner : a model data an optimizer; and a loss function Building a  Learner Let ’ s look at the  data  first . FluxTraining . jl  is agnostic of the data source .  The only requirements are: it is iterable and each iteration returns a tuple  (xs, ys) the model can take in  xs , i . e .   model(xs)  works; and the loss function can take model outputs and  ys , i . e .   lossfn(model(xs), ys)  returns a scalar Glossing over the details as it ’ s not the focus of this tutorial, here ’ s the code for getting a data iterator of the MNIST dataset .  We use  DataLoaders.DataLoader  to create an iterator of batches from our dataset . xs, ys = (\n    # convert each image into h*w*1 array of floats \n    [Float32.(reshape(img, 28, 28, 1)) for img in Flux.Data.MNIST.images()],\n    # one-hot encode the labels\n    [Float32.(Flux.onehot(y, 0:9)) for y in Flux.Data.MNIST.labels()],\n)\n\n# split into training and validation sets\ntraindata, valdata = splitobs((xs, ys))\n\n# create iterators\ntrainiter, valiter = DataLoader(traindata, 128, buffered=false), DataLoader(valdata, 256, buffered=false); Next, let ’ s create a simple  Flux . jl   model  that we ’ ll train to classify the MNIST digits . model = Chain(\n    Conv((3, 3), 1 => 16, relu, pad = 1, stride = 2),\n    Conv((3, 3), 16 => 32, relu, pad = 1),\n    GlobalMeanPool(),\n    Flux.flatten,\n    Dense(32, 10),\n)  We ’ ll use  categorical cross entropy  as a  loss function  and  ADAM  as an  optimizer . lossfn = Flux.Losses.logitcrossentropy\noptim = Flux.ADAM(); Now we ’ re ready to create a  Learner .  At this point you can also add any callbacks, like  ToGPU  to run the training on your GPU if you have one available .  Some callbacks are also  included by default . Since we ’ re classifying digits, we also use the  Metrics  callback to track the accuracy of the model ’ s predictions: learner = Learner(model, (trainiter, valiter), optim, lossfn, Metrics(accuracy))  Training With a  Learner  inplace, training is as simple as calling  fit! (learner, nepochs) . FluxTraining.fit!(learner, 10) \u001b[32mEpoch 1 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:46\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   1.0 │ 2.04939 │  0.25204 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 1 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │   1.0 │ 1.70353 │   0.3821 │\n└─────────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 2 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:19\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   2.0 │ 1.58615 │  0.44849 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 2 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │   2.0 │ 1.44792 │  0.50544 │\n└─────────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 3 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:18\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   3.0 │ 1.36495 │  0.57273 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 3 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │   3.0 │ 1.25941 │  0.59525 │\n└─────────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 4 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:20\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   4.0 │ 1.18935 │  0.64891 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 4 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m   Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼────────┼──────────┤\n│ ValidationPhase │   4.0 │ 1.1076 │  0.66347 │\n└─────────────────┴───────┴────────┴──────────┘\n\u001b[32mEpoch 5 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:19\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   5.0 │ 1.05506 │  0.69386 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 5 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │   5.0 │ 0.99203 │  0.70275 │\n└─────────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 6 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:18\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   6.0 │ 0.95282 │  0.72533 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 6 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │   6.0 │ 0.90209 │  0.73058 │\n└─────────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 7 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:19\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   7.0 │ 0.87621 │  0.74563 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 7 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │   7.0 │ 0.83402 │  0.74781 │\n└─────────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 8 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:18\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   8.0 │ 0.81399 │  0.76282 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 8 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │   8.0 │ 0.77623 │  0.76568 │\n└─────────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 9 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:18\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │   9.0 │ 0.76236 │  0.77835 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 9 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │   9.0 │ 0.72606 │  0.78079 │\n└─────────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 10 TrainingPhase(): 100%|█████████████████████████| Time: 0:00:18\u001b[39m\n┌───────────────┬───────┬─────────┬──────────┐\n│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├───────────────┼───────┼─────────┼──────────┤\n│ TrainingPhase │  10.0 │ 0.71684 │  0.79175 │\n└───────────────┴───────┴─────────┴──────────┘\n\u001b[32mEpoch 10 ValidationPhase(): 100%|███████████████████████| Time: 0:00:02\u001b[39m\n┌─────────────────┬───────┬─────────┬──────────┐\n│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n├─────────────────┼───────┼─────────┼──────────┤\n│ ValidationPhase │  10.0 │ 0.68353 │  0.79449 │\n└─────────────────┴───────┴─────────┴──────────┘\n"},{"doctype":"documentation","id":"references/FluxTraining.Callbacks","title":"Callbacks","text":""},{"doctype":"documentation","id":"references/FluxTraining.CancelStepException","title":"CancelStepException","text":" CancelStepException ( message ) Throw during fitting to cancel the currently running step . This prematurely ends the current step without throwing an error . Must be thrown inside the context of  runstep . Examples runepoch ( learner ,  phase )  do   _ \n     for   ( xs ,  ys )  in  batches \n         runstep ( learner ,  phase ,  (;  xs ,  ys ) )  do   _ ,  state \n            # training logic...\n             if   isnan ( state . loss ) .\n                 throw ( CancelStepException ( \"Skipping NaN loss\" ) )\n              end \n         end \n     end \n end"},{"doctype":"documentation","id":"references/FluxTraining.Check","title":"Check","text":""},{"doctype":"documentation","id":"references/FluxTraining.testbatches","title":"testbatches","text":""},{"doctype":"documentation","id":"references/FluxTraining.EarlyStopping","title":"EarlyStopping","text":" EarlyStopping ( criteria ...;  kwargs ... )\n EarlyStopping ( n ) Stop training early when  criteria  are met .  See EarlyStopping . jl  for available stopping criteria . Passing an integer  n  uses the simple patience criterion: stop if the validation loss hasn ’ t increased for  n  epochs . You can control which phases are taken to measure the out - of - sample loss and the training loss with keyword arguments  trainphase  (default AbstractTrainingPhase ) and  testphase  (default  AbstractValidationPhase ) . Examples Learner ( model ,  data ,  optimizer ,  lossfn ,  EarlyStopping ( 3 ) ) import   FluxTraining . ES :  Disjunction ,  InvalidValue ,  TimeLimit \n\n callback   =  EarlyStopping ( Disjunction ( InvalidValue ( ) ,  TimeLimit ( 0.5 ) ) )\n Learner ( model ,  data ,  optimizer ,  lossfn ,  callback )"},{"doctype":"documentation","id":"references/FluxTraining.GarbageCollect","title":"GarbageCollect","text":" GarbageCollect ( nsteps ) Every  nsteps  steps, forces garbage collection . Use this if you get memory leaks from, for example, parallel data loading . Performs an additional C - call on Linux systems that can sometimes help ."},{"doctype":"documentation","id":"references/FluxTraining.Read","title":"Read","text":""},{"doctype":"documentation","id":"references/FluxTraining.ProgressPrinter","title":"ProgressPrinter","text":" ProgressPrinter ( ) Prints a progress bar of the currently running epoch ."},{"doctype":"documentation","id":"references/FluxTraining.Events.StepEnd","title":"StepEnd","text":" StepEnd ( ) Event  called at the end of a batch ."},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Loggables","title":null,"text":" module    Loggables   Defines  Loggables.Loggable  and its subtypes ."},{"doctype":"documentation","id":"references/FluxTraining.FrequencyThrottle","title":"FrequencyThrottle","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.AbstractTrainingPhase","title":"AbstractTrainingPhase","text":" abstract   type   AbstractTrainingPhase   <:  Phase  An abstract type for phases where parameter updates are being made .  This exists so callbacks can dispatch on it and work with custom training phases . The default implementation for supervised tasks is  TrainingPhase ."},{"doctype":"documentation","id":"references/FluxTraining.ConditionalCallback","title":"ConditionalCallback","text":" ConditionalCallback ( callback ,  condition )  <:  Callback Wrapper callback that only forwards events to the wrapped callback if  CallbackCondition   condition  is met .  See  throttle ."},{"doctype":"documentation","id":"references/FluxTraining.handle","title":"handle","text":""},{"doctype":"documentation","id":"references/FluxTraining._resolveconflict","title":"_resolveconflict","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.NumberLimit","title":"NumberLimit","text":""},{"doctype":"documentation","id":"references/FluxTraining.CheckIteratesTuples","title":"CheckIteratesTuples","text":""},{"doctype":"documentation","id":"references/FluxTraining.getindexperm","title":"getindexperm","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.GL","title":"GL","text":""},{"doctype":"documentation","id":"references/FluxTraining.LogMetrics","title":"LogMetrics","text":" LogMetrics ( backends ... )  <:  Callback Callback that logs step and epoch metrics to one or more  LoggerBackend s . See also  LoggerBackend ,  Loggables.Loggable ,  log_to , TensorBoardBackend Example: logcb   =  LogMetrics ( TensorBoardBackend ( \"tblogs\" ) )\n Learner ( model ,  data ,  opt ,  lossfn ,  Metrics ( accuracy ) ,  logcb )"},{"doctype":"documentation","id":"references/FluxTraining.@unpack_History","title":"@unpack_History","text":""},{"doctype":"document","id":"documents/docs/tutorials/training.md","title":"Training loop","text":" Training loop FluxTraining . jl comes with a training loop for standard supervised learning problems, but for different tasks like self - supervised learning, being able to write custom training logic is essential .  The package ’ s training loop API requires little boilerplate to convert a regular Flux . jl training loop while making it compatible with existing callbacks . Supervised training, step - by - step We ’ ll explore the API step - by - step by converting a basic training loop and then discuss ways in which more complex training loops can be implemented using the same approach .  The central piece of a training loop is the logic for a single training step, and in many cases, that will be all you need to implement .  Below is the definition of a basic vanilla Flux . jl training step .  It takes a batch of data, calculates the loss, gradients and finally updates the parameters of the model . function   step! ( model ,  batch ,  params ,  optimizer ,  lossfn )\n     xs ,  ys   =  batch \n     grads   =  gradient ( params )  do \n          ŷs   =  model ( xs )\n         loss   =  lossfn ( ŷs ,  ys )\n         return   loss \n     end \n     update! ( optimizer ,  params ,  grads )\n end To make a training step work with FluxTraining . jl and its callbacks, we need to store data for a step so that callbacks can access it (e . g .   Metrics  uses  ys  and  ŷs  to evaluate metrics for each step); and dispatch events so the callbacks are triggered We first need to create a  Phase  and implement a method for  FluxTraining.step!  that dispatches on the phase type .   Phase s are used to define different training behaviors using the same API and to define callback functionality that is only run during certain phases .  For example,  Scheduler  only runs during  AbstractTrainingPhase s but not during  ValidationPhase .  Let ’ s implement such a phase and method, moving the arguments inside a  Learner  in the process . struct    MyTrainingPhase   <:  FluxTraining . AbstractTrainingPhase \n\n function   FluxTraining . step! ( learner ,  phase :: MyTrainingPhase ,  batch )\n     xs ,  ys   =  batch \n     grads   =  gradient ( learner . params )  do \n          ŷs   =  learner . model ( xs )\n         loss   =  learner . lossfn ( ŷs ,  ys )\n         return   loss \n     end \n     update! ( learner . optimizer ,  learner . params ,  grads )\n end  Now we can already train a model using this implementation, for example using  epoch! (learner, MyTrainingPhase(), dataiter) .  However, no callbacks would be called, since we haven ’ t yet put in any logic that dispatches events or stores the step state .  We can do both by using the helper function  runstep  which takes care of runnning our step logic, dispatching a  StepBegin  and  StepEnd  event before and after and handling control flow exceptions like  CancelStepException .  Additionally,  runstep  gives us a function  handle  which we can use to dispatch events inside the step, and  state  a container for storing step state .  Let ’ s use  runstep  and store the variables of interest inside  state : function   step! ( learner ,  phase :: MyTrainingPhase ,  batch )\n     xs ,  ys   =  batch \n     runstep ( learner ,  phase ,  ( xs = xs ,  ys = ys ) )  do   handle ,  state \n         state . grads   =  gradient ( learner . params )  do \n              state . ŷs   =  learner . model ( state . xs )\n             state . loss   =  learner . lossfn ( state . ŷs ,  state . ys )\n             return   loss \n         end \n         update! ( learner . optimizer ,  learner . params ,  grads )\n     end \n end Now callbacks like  Metrics  can access variables like  ys  through  learner.step  (which is set to the last  state ) .  Finally, we can use  handle  to dispatch additional events: using   FluxTraining . Events :  LossBegin ,  BackwardBegin ,  BackwardEnd \n\n function   step! ( learner ,  phase :: MyTrainingPhase ,  batch )\n     xs ,  ys   =  batch \n     runstep ( learner ,  phase ,  ( xs = xs ,  ys = ys ) )  do   handle ,  state \n         state . grads   =  gradient ( learner . params )  do \n              state . ŷs   =  learner . model ( state . xs )\n             handle ( LossBegin ( ) )\n             state . loss   =  learner . lossfn ( state . ŷs ,  state . ys )\n             handle ( BackwardBegin ( ) )\n             return   loss \n         end \n         handle ( BackwardEnd ( ) )\n         update! ( learner . optimizer ,  learner . params ,  grads )\n     end \n end The result is the full implementation of FluxTraining . jl ’ s own  TrainingPhase !  Now we can use  epoch!  to train a  Learner  with full support for  all callbacks : for   i   in  1 : 10\n     epoch! ( learner ,  MyTrainingPhase ( ) ,  dataiter )\n end Validation The implementation of  ValidationPhase  is even simpler; it runs the forward pass and stores variables so that callbacks like  Metrics  can access them . struct    ValidationPhase   <:  AbstractValidationPhase    end \n\n function   step! ( learner ,  phase :: ValidationPhase ,  batch )\n     xs ,  ys   =  batch \n     runstep ( learner ,  phase ,  ( xs = xs ,  ys = ys ) )  do   _ ,  state \n         state . ŷs   =  learner . model ( state . xs )\n         state . loss   =  learner . lossfn ( state . ŷs ,  state . ys )\n     end \n end Epoch logic We didn ’ t need to implement a custom  epoch!  method for our phase since the default is fine here: it just iterates over every batch and calls  step! .  In fact, let ’ s have a look at how  epoch!  is implemented: function   epoch! ( learner ,  phase :: Phase ,  dataiter )\n     runepoch ( learner ,  phase )  do   handle \n         for   batch   in  dataiter \n             step! ( learner ,  phase ,  batch )\n         end \n     end \n end Here,  runepoch , similarly to  runstep , takes care of epoch start/stop events and control flow .  If you want more control over your training loop, you can use it to write training loops that directly use  step! : phase   =  MyTrainingPhase ( )\n withepoch ( learner ,  phase )  do   handle \n     for   batch   in  dataiter \n         step! ( learner ,  phase ,  batch )\n         if   learner . step . loss   <  0.1\n             throw ( CancelFittingException ( \"Low loss reached.\" ) )\n         end \n     end \n end Tips Here are some additional tips for making it easier to implement complicated training loops . You can pass (named) tuples of models to the  Learner  constructor .  For example, for generative adversarial training, you can pass in  (generator = ..., critic = ...)  and then refer to them inside the  step!  implementation, e . g .  using  learner.model.generator .  The models ’  parameters will have the same structure, i . e .   learner.params.generator  corresponds to  params(learner.model.generator) . You can store any data you want in  state . When defining a custom phase, instead of subtyping  Phase  you can subtype  AbstractTrainingPhase  or  AbstractValidationPhase  so that some context - specific callbacks will work out of the box with your phase type .  For example,  Scheduler  sets hyperparameter values only during  AbstractTrainingPhase ."},{"doctype":"documentation","id":"references/FluxTraining.Events.BackwardEnd","title":"BackwardEnd","text":" BackwardEnd ( ) Event  called between calculating gradients and updating parameters ."},{"doctype":"documentation","id":"references/FluxTraining.LogVisualization","title":"LogVisualization","text":" LogVisualization ( visfn ,  backends ... [;  freq   =  100 ] ) Logs images created by  visfn(learner.step)  to  backends  every  freq  steps ."},{"doctype":"documentation","id":"references/FluxTraining.ES.NumberSinceBest","title":"NumberSinceBest","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.EarlyStopping","title":"EarlyStopping","text":""},{"doctype":"documentation","id":"references/FluxTraining.resolveconflict","title":"resolveconflict","text":" resolveconflict ( cb1 ,  cb2 ) Define a conflict resolution strategy for resolving a write/write conflict between two callbacks . The default is  [ NotDefined() ] , which will result in an error and a message to implement this method . To implement, dispatch on the callback types that you which to resolve (in any order) and return one of the following: Unresolvable ()  if the callbacks must not be used together RunFirst (cb)  if one of the callbacks needs to run first; or NoConflict ()  if the callbacks may run together in any order"},{"doctype":"documentation","id":"references/FluxTraining.ConflictResolution","title":"ConflictResolution","text":" abstract   type   ConflictResolution  A conflict resolution strategy for resolving write/write conflicts of two callbacks . See  resolveconflict ."},{"doctype":"documentation","id":"references/FluxTraining.savemodel","title":"savemodel","text":""},{"doctype":"documentation","id":"references/FluxTraining.LoggerBackend","title":"LoggerBackend","text":" abstract   type   LoggerBackend  Backend for logging callbacks like . To add support for logging  Loggables.Loggable   L  to backend  B , implement log_to (backend::B, loggable::L, names, i) See also  LogMetrics ,  LogHyperParams ,  log_to"},{"doctype":"documentation","id":"references/FluxTraining.runchecks","title":"runchecks","text":""},{"doctype":"documentation","id":"references/FluxTraining.LearningRate","title":"LearningRate","text":" abstract   type   LearningRate   <:  HyperParameter  Hyperparameter for the optimizer ’ s learning rate . See  Scheduler  and  hyperparameter scheduling ."},{"doctype":"documentation","id":"references/FluxTraining.setindexperm!","title":"setindexperm!","text":""},{"doctype":"document","id":"documents/CHANGELOG.md","title":"News","text":" News [ 0 . 2 . 0 ]  – Unreleased This is a  breaking  release . Added New training loop API that is easier to extend .  Defining a  Phase  and  step!  is all you need .  See  the new tutorial  and  the new reference . Relevant functions:  epoch! ,  step! ,  runstep ,  runepoch Added  CHANGELOG.md  (this file) AbstractValidationPhase  as supertype for validation phases Documentation for callback helpers on  reference page Changed Batch*  renamed to  Step* : events:  BatchBegin  now  StepBegin ,  BatchEnd  now  StepEnd CancelBatchException  now  CancelStepException . field  Learner.batch  now  Learner.step Learner.step/batch  is no longer a special  struct  but now a  PropDict , allowing you to set arbitrary fields . Learner.model  can now be a  NamedTuple/Tuple  of models for use in custom training loops .  Likewise,  learner.params  now resembles the structure of  learner.model , allowing separate access to parameters of different models . Callbacks Added  init!  method for callback initilization, replacing the  Init  event which required a  Phase  to implement . Scheduler  now has internal step counter and no longer relies on  Recorder ’ s history .  This makes it easier to replace the scheduler without needing to offset the new schedules . EarlyStopping  callback now uses criteria from  EarlyStopping . jl Removed Removed old training API .  Methods  fitbatch! ,  fitbatchphase! ,  fitepoch! ,  fitepochphase!  have all been removed ."},{"doctype":"documentation","id":"references/FluxTraining.stateaccess","title":"stateaccess","text":" stateaccess ( :: Type { HyperParameter } ) Defines what  Learner  state is accessed when calling sethyperparameter!  and  gethyperparameter .  This is needed so that  Scheduler  can access the state . stateaccess ( callback ) Return a named tuple determining what learner state  callback can access .  The default is  (;) , the empty named tuple, meaning no state can be accessed .  Implementations of  stateaccess  should always return the least permissions possible . Extending For example, the  ToGPU  callback needs to write both the model and the batch data, so its  stateaccess  implementation is: stateaccess ( :: ToGPU )  =  (\n     model   =  Write ( ) ,\n     params   =  Write ( ) ,\n     step   =  ( xs   =  Write ( ) ,  ys   =  Write ( ) ) ,\n ) When defining  stateaccess , be careful that you do return a  NamedTuple . (x = Read(),)  is one but  (x = Read())  (without the comma) is parsed as an assignment with value  Read() ."},{"doctype":"documentation","id":"references/FluxTraining.Loss","title":"Loss","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.EpochBegin","title":"EpochBegin","text":" EpochBegin ( ) Event  called at the beginning of an epoch ."},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Image","title":"Image","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.TrainingPhase","title":"TrainingPhase","text":" TrainingPhase ( )  <:  AbstractTrainingPhase A regular training phase for supervised learning .  It iterates over batches in  learner.data.training  and updates the model parameters using  learner.optim  after calculating the gradients . Throws the following events in this order: EpochBegin  when an epoch starts, StepBegin  when a step starts, LossBegin  after the forward pass but before loss calculation, BackwardBegin  after loss calculation but before backward pass, BackwardEnd  after the bacward pass but before the optimization step, StepEnd  when a step ends; and EpochEnd  when an epoch ends It writes the following step state to  learner.state , grouped by the event from which on it is available . StepBegin : xs  and  ys : encoded input and target (batch) LossBegin : ŷs : model output BackwardBegin : loss : loss BackwardEnd : grads : calculated gradients"},{"doctype":"documentation","id":"references/FluxTraining.ES.Warmup","title":"Warmup","text":""},{"doctype":"documentation","id":"references/FluxTraining.Learner","title":"Learner","text":" Learner ( model ,  data ,  optimizer ,  lossfn ,  [ callbacks ...;  kwargs ... ] ) Holds and coordinates all state of the training .   model  is trained by optimizing  lossfn  with  optimizer  on  data . Arguments model : A Flux . jl model or a  NamedTuple  of models . data : Data iterators .  A 2 - tuple will be treated as  (trainingdataiter, validdataiter) . You can also pass in an empty tuple  ()  and use the  epoch!  method with a dataiter  as third argument . A data iterator is an iterable over batches .  For regular supervised training, each batch should be a tuple  (xs, ys) . lossfn : Function with signature  lossfn(model(x), y) -> Number optimizer callbacks... : Any other unnamed arguments are callbacks Keyword arguments usedefaultcallbacks = true : Whether to add some basic callbacks .  Included are  Metrics ,  Recorder ,  ProgressPrinter , StopOnNaNLoss , and  MetricsPrinter . cbrunner = LinearRunner() : Callback runner to use . Fields (Use this as a reference when implementing callbacks) model ,  optimizer , and  lossfn  are stored as passed in data  is a  PropDict  of data iterators, usually  :training  and  :validation . params : An instance of  model ’ s parameters of type  Flux.Params .  If  model  is a  NamedTuple , then  params  is a  NamedTuple  as well . step:: PropDict : State of the last step .  Contents depend on the last run Phase . cbstate:: PropDict : Special state container that callbacks can save state to for other callbacks .  Its keys depend on what callbacks are being used .  See the  custom callbacks guide for more info ."},{"doctype":"documentation","id":"references/FluxTraining.SanityCheck","title":"SanityCheck","text":" SanityCheck ( [ checks;   usedefault   =  true ] ) Callback that runs sanity  Check s when the  Learner  is initialized . If  usedefault  is  true , it will run all checks in FluxTraining . CHECKS in addition to the ones you pass in ."},{"doctype":"documentation","id":"references/FluxTraining.AbstractCallback","title":"AbstractCallback","text":" abstract   type   AbstractCallback  Supertype of  SafeCallback / Callback .  When implementing callbacks, you should subtype  SafeCallback  instead ."},{"doctype":"documentation","id":"references/FluxTraining.CheckDataIteratorTrain","title":"CheckDataIteratorTrain","text":""},{"doctype":"document","id":"documents/docs/background/optimizer.md","title":"Optimizers","text":" Optimizers An optimizer takes the calculated gradients from a training step and uses them to update the parameters of a  model .  FluxTraining . jl currently only supports optimizers from  Flux . jl . A complete reference of optimizers in Flux . jl can be found  here ."},{"doctype":"documentation","id":"references/FluxTraining.SmoothLoss","title":"SmoothLoss","text":""},{"doctype":"documentation","id":"references/FluxTraining.LinearRunner","title":"LinearRunner","text":""},{"doctype":"documentation","id":"references/FluxTraining.Unresolvable","title":"Unresolvable","text":" abstract   type   Unresolvable   <:  ConflictResolution  Return from  resolveconflict  to indicate that two callbacks are incompatible and cannot be used together ."},{"doctype":"documentation","id":"references/FluxTraining.CheckDataIteratorValid","title":"CheckDataIteratorValid","text":""},{"doctype":"document","id":"documents/docs/callbacks/tipstricks.md","title":"Tips & tricks","text":" using   FluxTraining \n using   FluxTraining . Events ,  FluxTraining . Phases   Tips  &  tricks Listing event handlers for a callback Use  Base.methods  to check what events a callback handles: methods ( FluxTraining . on ,  ( Any ,  Any ,  Recorder ,  Any ) )  Visualize the callback dependency graph You can use  GraphPlot . jl  to visualize the dependencies between callbacks: learner   =  Learner (\n     nothing ,  ( nothing ,  nothing ) ,  nothing ,  nothing ,  # dummy arguments\n     ToGPU ( ) ,\n );   using   GraphPlot \n gplot ( learner . callbacks . graph ,  nodelabel   =  learner . callbacks . cbs ,  layout   =  stressmajorize_layout )  (the target of an arrow depends on the origin) As an example for a detected dependency, we can see that  MetricsPrinter  runs after  Metrics .   MetricsPrinter  prints the values of all metrics, so  [ Metrics ]  needs to run first ."},{"doctype":"documentation","id":"references/FluxTraining.ES.message","title":"message","text":""},{"doctype":"documentation","id":"references/FluxTraining.log_to","title":"log_to","text":" log_to ( backend ,  loggable ,  group ,  i )\n log_to ( backends ,  loggable ,  group ,  i ) Log  loggable  to  backend  with  group  to index  i . loggable  is any  Loggables.Loggable group  can be a  String  or a tuple of  String s implying some grouping which can be used by a supporting backend . i  is a step counter and unique for every group ."},{"doctype":"documentation","id":"references/FluxTraining.AbstractMetric","title":"AbstractMetric","text":" abstract   type   AbstractMetric  Abstract type for metrics passed to  Metrics . For most use cases, you should use  Metric , the standard implementation . Interface If  Metric  doesn ’ t fit your use case, you can create a new subtype of  AbstractMetric  and implement the following methods to make it compatible with  Metrics : reset! (metric) step! (metric, learner) stepvalue (metric) epochvalue (metric) metricname (metric)"},{"doctype":"documentation","id":"references/FluxTraining.ES.Disjunction","title":"Disjunction","text":""},{"doctype":"documentation","id":"references/FluxTraining.SafeCallback","title":"SafeCallback","text":""},{"doctype":"documentation","id":"references/FluxTraining.phasedataiter","title":"phasedataiter","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.NotANumber","title":"NotANumber","text":""},{"doctype":"documentation","id":"references/FluxTraining.History","title":"History","text":""},{"doctype":"documentation","id":"references/FluxTraining.runtests","title":"runtests","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.stopping_time","title":"stopping_time","text":""},{"doctype":"documentation","id":"references/FluxTraining.epochvalue","title":"epochvalue","text":""},{"doctype":"documentation","id":"references/FluxTraining._dataiters","title":"_dataiters","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.done!","title":"done!","text":""},{"doctype":"documentation","id":"references/FluxTraining.epoch!","title":"epoch!","text":" epoch! ( learner ,  phase [ ,  dataiter ] ) Train  learner  for one epoch on  dataiter .  Iterates through dataiter  and  step! s for each batch/item . If no data iterator is passed in, use  learner.data[phasedataiter(phase)] . Extending The default implementation iterates over every batch in  dataiter and calls  step!  for each .  This behavior can be overloaded by implementing  epoch!(learner, ::MyPhase, dataiter) . If you ’ re implementing a custom  epoch!  method, it is recommended you make use of  runepoch  to get begin and end events as well as proper handling of  CancelEpochException s . See the default implementation for reference ."},{"doctype":"documentation","id":"references/FluxTraining.StopOnNaNLoss","title":"StopOnNaNLoss","text":" StopOnNaNLoss ( ) Stops the training when a NaN loss is encountered . This callback is added by default to every  Learner  unless you pass in usedefaultcallbacks = false ."},{"doctype":"documentation","id":"references/FluxTraining.Events.LossBegin","title":"LossBegin","text":" LossBegin ( ) Event  called between calculating  y_pred  and calculating loss"},{"doctype":"documentation","id":"references/FluxTraining.edgesrunafter","title":"edgesrunafter","text":" edgesrunafter ( callbacks ) Return a vector of  Edge s representing dependencies defined by  runafter ."},{"doctype":"documentation","id":"references/FluxTraining.on","title":"on","text":" on ( event :: Event ,  phase :: Phase ,  callback :: AbstractCallback ,  learner ) Handle  event  with  Callback   callback . By default, this event handler does nothing for a callback . To see events which an  AbstractCallback  handles, use methods ( Training . on ,  ( Any ,  Any ,  MyCallbackType ,  Any )  Extending You can add event handlers to  Callback s by implementing a method for  on . See also  Callback  and  custom callbacks . A method of  on  should  always  dispatch on the callback type, i . e . on(event, phase, cb::MyCallback, learner) .  It may also dispatch on specific Event s and  Phase .  It should not dispatch on a specific type for learner ."},{"doctype":"documentation","id":"references/FluxTraining.Phases","title":"Phases","text":""},{"doctype":"documentation","id":"references/FluxTraining.TestModel","title":"TestModel","text":""},{"doctype":"document","id":"documents/docs/getting_started.md","title":"Getting started","text":" Getting started Let ’ s look at a simple training example .  In  FluxTraining . jl , a  Learner  holds all state necessary for training .  To get started, you need a  model training and validation  data iterators a  loss function ; and an  optimizer First we define the necessary pieces: model   =  ...\n traindata ,  valdata   =  ...\n lossfn   =  Flux . Losses . mse \n opt   =  Flux . ADAM ( 0.01 ) Then we construct a  Learner : learner   =  Learner ( model ,  ( traindata ,  valdata ) ,  opt ,  lossfn ) And train for 10 epochs: fit! ( learner ,  10 )"},{"doctype":"documentation","id":"references/FluxTraining.accuracy","title":"accuracy","text":""},{"doctype":"documentation","id":"references/FluxTraining.defaultcallbacks","title":"defaultcallbacks","text":""},{"doctype":"document","id":"documents/docs/callbacks/custom.md","title":"Custom callbacks","text":" Custom callbacks FluxTraining . jl ’ s callback system is built around multiple dispatch, so you specify which part of the training you want to  “ hook into ”  by dispatching on  Phase s and  Event s .  See  Training loop  and  Events  as a reference to phases and events . A guided example There are 4 things you need to do to implement a custom callback: Create a callback  struct  that subtypes  Callback Write event handlers with  on Define what state the callback accesses by implementing  stateaccess (Optionally) define dependencies on other callbacks with  runafter Let ’ s go through them one at a time by implementing a simple callback that prints something after every batch . Callback  struct A callback definition has to subtype the abstract  Callback  type .  It can include fields to use as internal state, but we don ’ t need that here . struct    Printer   <:  Callback \n  end Event handlers Now we need to add an event handler so that  Printer  can run some code when a step ends .  Event handlers can be defined by adding a method to  FluxTraining.on .  It takes as arguments an  Event , a  Phase , the callback and the learner: on(event::Event, phase::Phase, callback::Callback, learner) The  event ,  phase  and  callback  are used to dispatch . In this case, we want to run code at the end of a step, so the event we need to dispatch on is  StepEnd .  We want it to run in any phase, so we use the abstract type  Phase .  The third argument type is the callback we want to add an event handler to .  This gives us: function   FluxTraining . on (\n         event :: StepEnd ,\n         phase :: Phase ,\n         printer :: Printer ,\n         learner )\n     println ( \"Hello, World!\" )\n end We can now pass an instance of  Printer  when creating a  Learner  and the message will be printed at the end of every step . State As seen above, the callback handler  on  receives as the last argument a  Learner  instance, allowing the callback to access and modify state .  If we wanted to print the last step ’ s loss instead of a generic message, we could update our definition of  on : function   FluxTraining . on (\n         event :: EpochEnd ,\n         phase :: Phase ,\n         printer :: Printer ,\n         learner )\n     println ( \"Step loss:\" ,  learner . step . loss )\n end (see  Learner  for in - depth documentation of the  Learner ’ s state) The ability to modify any state is very powerful, but it can quickly become problematic when it is unclear which callbacks modify what state and what the correct order should be . Because of that,  FluxTraining . jl  prevents callbacks from reading and modifying state by default .  If we tried to use the above redefinition of  on , we would get the following error: FluxTraining . ProtectedException ( \"Read access to Learner.step.loss disallowed.\" ) To fix that error, we need to implement  stateaccess , a function that specifies what state a callback is allowed to read and write .  In our case, we want to read the loss of the current step: FluxTraining . stateaccess ( :: Printer )  =  ( step   =  ( loss   =  Read ( ) , ) , ) (see  stateaccess  for more information on how to implement it) After that definition, the above code will run fine .  This might seem bothersome, but this extra information makes it possible to analyze state dependencies before any code is run and saves you from running into nasty, hard - to - find bugs that can occur when using many callbacks together . Dependencies Let ’ s improve our callback a bit by adding the current step number to the printed message, so it will look like this:  \"Step 14 loss: 0.0032\" .  For that we need to know what the current number of steps is .  One way to go about this is to add a field to  Printer  that starts at  0  and is incremented every step . Luckily, there already is a callback that tracks this kind of statistics, the  Recorder .  It uses a special piece of state,  learner.cbstate , to store a  History  with this information . Callback state learner.cbstate  is an object where callbacks can store state that they want to make available to other callbacks .  Like any other piece of state, the callback writing to it needs to add a  Write()  permission to it using  stateaccess . What makes  cbstate  special is that when creating the callback graph, it is checked that every entry in  cbstate  that is accessed is being created first . The update to the event handler looks like this: function   FluxTraining . on (\n         event :: EpochEnd ,\n         phase :: Phase ,\n         printer :: Printer ,\n         learner )\n     step   =  learner . cbstate . history [ phase ] . stepsepoch   # steps completed in current epoch\n     println ( \"Step \" ,   ,  \" loss:\" ,  learner . step . loss )\n end We also need to update the definition of  stateaccess  now: FluxTraining . stateaccess ( :: Printer )  =  (\n     step   =  ( loss   =  Read ( ) , ) ,\n     cbstate   =  ( history   =  Read ( ) , ) ,\n ) Since  Printer  depends on  Recorder  now, an error will be thrown if you try to use  Printer  without  Recorder .  And that ’ s it, pass  Printer  to a  Learner  and test it out !  The upside of jumping through some additional hoops is that using the callback in the wrong context will always result in an error, so the user can have peace of mind . Conflict resolution When creating a  Learner , a dependency graph is created .  The graph is then analyzed to find possible conflicts (for example, when two callbacks update the same state) .  Conflicts are detected automatically and will result in an error .  Conflicts happen when the same state is being modified by multiple callbacks and it is unclear which order of running them (if any) is valid . Resolving conflicts There are two methods for resolving conflicts,  runafter  and  resolveconflict . runafter  allows you to define list of callbacks that should run before the callback .  For example,  Recorder  needs to run after all metrics: FluxTraining . runafter ( :: Recorder )  =  ( AbstractMetric , ) resolveconflict  provides more granular control to handle a possible conflict between two callbacks .  It takes two callbacks and defines how to resolve a conflict: # the default, errors with a helpful message\n resolveconflict ( :: C1 ,  :: C2 )  =  NotImplemented ( )    \n# two callbacks can never be used together:\n resolveconflict ( :: C1 ,  :: C2 )  =  Unresolvable ( )      \n resolveconflict ( :: C1 ,  :: C2 )  =  NoConflict ( )        # there is no conflict, any run order is fine\n resolveconflict ( cb1 :: C1 ,  cb2 :: C2 )  =  RunFirst ( cb1 ) # `cb1` must run before `cb2`.\n                                                  # Equivalent to `runafter(::C2) = (C1,) Callback execution By default, a topological ordering of the callbacks is created from the dependency graph and the callbacks are executed serially .  This behavior can be overwritten with custom callback executors, for example to create a  Dagger . jl  node from the graph to allow callbacks to safely run in parallel where valid ."},{"doctype":"documentation","id":"references/FluxTraining.model!","title":"model!","text":""},{"doctype":"documentation","id":"references/FluxTraining.callbackgraph","title":"callbackgraph","text":" callbackgraph ( callbacks )  ->  SimpleDiGraph Creates a directed acyclic graph from a list of  callbacks . Ordering is given through  runafter  and  resolveconflict . If a write conflict cannot be resolved (i . e .   resolveconflict ) is not implemented), throws an error ."},{"doctype":"documentation","id":"references/FluxTraining.onecycle","title":"onecycle","text":" onecycle ( nsteps ,  max_val ,  [ start_val ,  end_val;   pct_start ] ) Creates a one - cycle  Schedule  over  nsteps  steps from  start_val over  max_val  to  end_val . Examples epochlength   =  length ( traindataiter )\n cb   =  Scheduler ( LearningRate   =>  onecycle ( 10  epochlength ,  0.01 ) )\n learner   =  Learner ( < args > ... ,  cb )"},{"doctype":"documentation","id":"references/FluxTraining.CallbackRunner","title":"CallbackRunner","text":""},{"doctype":"documentation","id":"references/FluxTraining._on","title":"_on","text":""},{"doctype":"documentation","id":"references/FluxTraining.fit!","title":"fit!","text":" fit! ( learner ,  nepochs )\n fit! ( learner ,  nepochs ,  ( trainiter ,  validiter ) ) Train  learner  for  nepochs  of training and validation each .  Use data iterators that are passed in .  If none are given, use  learner.data.training and  learner.data.validation . Examples fit! ( learner ,  10 )\n fit! ( learner ,  10 ,  ( traindl ,  valdl ) )"},{"doctype":"documentation","id":"references/FluxTraining.init!","title":"init!","text":" init! ( callback ,  learner ) Initialize a callback .  Default is to do nothing . Extending To extend for a callback, implement  init!(cb::MyCallback, learner) . init!  can set up internal state of a callback that depends on  learner and can also initialize shared callback state in  learner.cbstate . Just like  on  event handlers, the state access permissions must be correctly defined using  stateaccess  to do so . init!  must also be idempotent, i . e .  running it twice on the same  Learner should have the same effect as runnning it once ."},{"doctype":"documentation","id":"references/FluxTraining.testbatch","title":"testbatch","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES.TimeLimit","title":"TimeLimit","text":""},{"doctype":"documentation","id":"references/FluxTraining.protect","title":"protect","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Text","title":"Text","text":""},{"doctype":"documentation","id":"references/FluxTraining.accesses","title":"accesses","text":" accesses ( ) Enumerate all valid state accesses of permissions of kind  perm . accesses((x = Read(),), Read()) === [(:x,)] accesses((x = Read(),), Write()) === []"},{"doctype":"documentation","id":"references/FluxTraining.Events.Event","title":"Event","text":" abstract   type   Event  Abstract type for events that callbacks can hook into"},{"doctype":"documentation","id":"references/FluxTraining.replacecallback!","title":"replacecallback!","text":" replacecallback! ( learner ,  callback :: C ) Replace existing callback of type  C  on learner with  callback . Return the replaced callback . If  learner  doesn ’ t have a callback of type  C , add  callback  and return  nothing ."},{"doctype":"documentation","id":"references/FluxTraining.Metrics","title":"Metrics","text":" Metrics ( metrics ... )  <:  Callback Callback that tracks metrics during training . You can pass any number of  metrics  with every argument being an  AbstractMetric  like  Metric ; or a function  f(ŷs, ys) -> val This callback is added by default to every  Learner  unless you pass in usedefaultcallbacks = false .  A metric tracking  learner.lossfn   Loss is included by default . The computed metrics can be access in  learner.cbstate.metricsstep  and learner.cbstate.metricsepoch  for steps and epochs, respectively . Examples Track  accuracy : cb   =  Metrics ( accuracy ) Pass in  [ Metric ] s: cb   =  Metrics (\n     Metric ( Flux . mse ,  device   =  gpu ) ,\n     Metric ( Flux . mae ,  device   =  gpu )\n )"},{"doctype":"documentation","id":"references/FluxTraining.throttle","title":"throttle","text":" throttle ( callback ,  Event ,  freq   =  1 )\n throttle ( callback ,  Event ,  seconds   =  1 ) Throttle  Event  type for  callback  so that it is triggered either only every freq ’ th time  or every  seconds  seconds . Examples If you want to only sporadically log metrics ( LogMetrics ) or images ( LogVisualization ),  throttle  can be used as follows . Every 10 steps: callback   =  throttle ( LogMetrics ( TensorBoardBackend ( ) ) ,  StepEnd ,  freq   =  10 )\n learner   =  Learner ( < args > ,  callback ) Or every 5 seconds: callback   =  throttle ( LogMetrics ( TensorBoardBackend ( ) ) ,  StepEnd ,  seconds   =  5 )\n learner   =  Learner ( < args > ,  callback )"},{"doctype":"documentation","id":"references/FluxTraining.Events.EpochEnd","title":"EpochEnd","text":" EpochEnd ( ) Event  called at the end of an epoch ."},{"doctype":"documentation","id":"references/FluxTraining.Recorder","title":"Recorder","text":" Recorder ( ) Maintains a  History .  It ’ s stored in  learner.cbstate.history ."},{"doctype":"document","id":"documents/docs/callbacks/reference.md","title":"Callback reference","text":" Callback reference Included callbacks FluxTraining . jl  comes with many callbacks included .  Some of them are added to  Learner  by default, here marked with a  * .  Callback   Description   Metrics *  Tracks loss and additional metrics on a per - step and per - epoch base  Recorder *  Records training stats like number of steps and epochs  ProgressPrinter *  Prints a progress bar for the current epoch during training  MetricsPrinter *  Prints out metrics after every epoch  SanityCheck *  Performs sanity checks on data, model and loss before training  StopOnNaNLoss  Stops training early if a step loss is  NaN   ToGPU  Trains using a CUDA GPU if available  Checkpointer  Saves the model after every epoch  EarlyStopping  Stops training early when a criterion is met  Scheduler  Schedules hyperparameters  LogMetrics  Logs metrics to a logging backend  LogHyperParams  Logs hyperparameters to a logging backend  LogVisualization  Logs visualization to a logging backend  LogHistograms  Logs model weight histograms to a logging backend There are also some utilities for creating callbacks: CustomCallback  to quickly hook a function into an event throttle  to run a callback only after every  n  events or every  t  seconds And for working with callbacks on an existing  Learner : setcallbacks! addcallback! getcallback replacecallback! removecallback! Extension API The following types and functions can be used to create custom callbacks .  Read the  custom callbacks guide  for more context . Callback stateaccess runafter resolveconflict"},{"doctype":"documentation","id":"references/FluxTraining.ES.EarlyStopper","title":"EarlyStopper","text":""},{"doctype":"documentation","id":"references/FluxTraining.getfieldperm","title":"getfieldperm","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.Events","title":null,"text":" module    Events   Provides the abstract  Event  type and concrete event types . Events in  TrainingPhase  and  ValidationPhase : EpochBegin  and  EpochEnd , called at the beginning and end of each epoch . StepBegin  and  StepEnd , called at the beginning and end of each batch . LossBegin , called after the forward pass but before the loss calculation . TrainingPhase  only: BackwardBegin , called after forward pass and loss calculation but before gradient calculation . BackwardEnd , called after gradient calculation but before parameter update ."},{"doctype":"documentation","id":"references/FluxTraining.testlearner","title":"testlearner","text":" testlearner ( callbacks ... [;  opt ,  nbatches ,  coeff ,  batchsize ,  kwargs ... ] ) Construct a  Learner  with a simple optimization problem .  This learner should be used in tests that require training a model, e . g . for callbacks ."},{"doctype":"documentation","id":"references/FluxTraining.Protected","title":"Protected","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Graph","title":"Graph","text":""},{"doctype":"documentation","id":"references/FluxTraining.garbagecollect","title":"garbagecollect","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.ValidationPhase","title":"ValidationPhase","text":" ValidationPhase ( ) A regular validation phase .  It iterates over batches in  learner.data.validation  and performs a forward pass . Throws the following events:  EpochBegin ,  StepBegin , LossBegin ,  StepEnd ,  EpochEnd . Throws the following events in this order: EpochBegin  when an epoch starts, StepBegin  when a step starts, LossBegin  after the forward pass but before loss calculation, StepEnd  when a step ends; and EpochEnd  when an epoch ends It writes the following step state to  learner.state , grouped by the event from which on it is available . StepBegin : xs  and  ys : encoded input and target (batch) LossBegin : ŷs : model output StepEnd : loss : loss"},{"doctype":"documentation","id":"references/FluxTraining.Events.StepBegin","title":"StepBegin","text":" StepBegin ( ) Event  called at the beginning of a batch ."},{"doctype":"documentation","id":"references/FluxTraining.@pack_History","title":"@pack_History","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.BackwardBegin","title":"BackwardBegin","text":" BackwardBegin ( ) Event  called between calculating loss and calculating gradients"},{"doctype":"documentation","id":"references/FluxTraining.loadmodel","title":"loadmodel","text":" loadmodel ( path ) Loads a model that was saved to  path  using  FluxTraining. savemodel ."},{"doctype":"documentation","id":"references/FluxTraining.hasconflict","title":"hasconflict","text":""},{"doctype":"documentation","id":"references/FluxTraining.addcallback!","title":"addcallback!","text":" addcallback! ( learner ,  callback ) Adds  callback  to  learner  and updates the dependency graph ."},{"doctype":"documentation","id":"references/FluxTraining.getcallback","title":"getcallback","text":" getcallback ( learner ,  C ) Find callback of type  C  in  learner ’ s callbacks and return it . If there is none, return  nothing ."},{"doctype":"document","id":"documents/docs/reference/training.md","title":"Training loop API reference","text":" Training loop API reference The training loop API centers around the abstract type  Phase  and the function  step! .  To  implement a custom training , you need to Usage fit for  n  epochs of supervised training and validation using  fit! (learner, n) train for an epoch using  epoch! (learner, phase, dataiter) Extending subtype  Phase implement  step! You can optionally overwrite default  epoch!  implementation implement  phasedataiter  to define which data iterator should be used when  epoch!  is called without one . create custom  Callback  and  Event s with event handlers that dispatch on your  Phase  subtype . Control flow Inside callback handlers and  step!  implementations, you can throw  CancelFittingException  to stop the training and  CancelEpochException  and  CancelStepException  to skip the current epoch or step ."},{"doctype":"documentation","id":"references/FluxTraining.shouldrun","title":"shouldrun","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.File","title":"File","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events","title":"Events","text":""},{"doctype":"documentation","id":"references/FluxTraining.NoConflict","title":"NoConflict","text":" abstract   type   NoConflict   <:  ConflictResolution  Return from  resolveconflict  to indicate that, while the callbacks modify the same state, they can be used together without any problems ."},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Value","title":"Value","text":""},{"doctype":"documentation","id":"references/FluxTraining.setcallbacks!","title":"setcallbacks!","text":" setcallbacks! ( learner ,  callbacks ) Set  learner ’ s callbacks to  callbacks , removing all current callbacks ."},{"doctype":"documentation","id":"references/FluxTraining.ES.reset!","title":"reset!","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES","title":"ES","text":""},{"doctype":"documentation","id":"references/FluxTraining._combinename","title":"_combinename","text":""},{"doctype":"document","id":"documents/docs/imagenette_demo.ipynb","title":"imagenette_demo","text":" using DataLoaders\nusing Flux\nusing DataAugmentation\nusing DeepLearningTasks\nusing DLDatasets\nusing MLDataPattern\nusing LearnBase\nusing ProgressBars\nusing FluxTraining\nusing FluxModels task = ImageClassification(10, sz = (224, 224))\nlabeltoint = metadata(ImageNette).labeltoclass\nobsfn((image, label)) = (image, labeltoint[label])  trainds, valds = DLDatasets.loaddataset(ImageNette, \"v2_160px\", split = (\"train\", \"val\"))\nbs = 64\ntraindl = taskdataloader(task, trainds, bs; obsfn)\nvaldl = taskdataloader(task, valds, 2bs; obsfn); model = gpu(Chain(xresnet18(), FluxModels.classificationhead(task.nclasses, 512))); learner = Learner(\n    model,\n    (traindl, valdl),\n    ADAM(),\n    Flux.Losses.logitcrossentropy,\n    callbacks = [ToGPU()],\n    metrics = [Metric(accuracy)],\n    schedule = Schedules(onecycleschedule(10 * length(traindl), 0.01))\n); FluxTraining.fit!(learner, 10) \u001b[32mEpoch 1 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:02\u001b[39m\nloss: 1.3166474935148849\naccuracy: 0.5757865646258504\n\u001b[32mEpoch 2 ValidationPhase(): 100%|████████████████████████| Time: 0:00:13\u001b[39m\nloss: 1.9421841899553935\naccuracy: 0.471875\n\u001b[32mEpoch 2 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:05\u001b[39m\nloss: 1.001848922700298\naccuracy: 0.678146258503401\n\u001b[32mEpoch 3 ValidationPhase(): 100%|████████████████████████| Time: 0:00:13\u001b[39m\nloss: 1.044497122367223\naccuracy: 0.6622395833333337\n\u001b[32mEpoch 3 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:06\u001b[39m\nloss: 0.8365915733940748\naccuracy: 0.7292729591836736\n\u001b[32mEpoch 4 ValidationPhase(): 100%|████████████████████████| Time: 0:00:13\u001b[39m\nloss: 0.9936108271280925\naccuracy: 0.6783854166666666\n\u001b[32mEpoch 4 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:04\u001b[39m\nloss: 0.7450118934621617\naccuracy: 0.7589285714285718\n\u001b[32mEpoch 5 ValidationPhase(): 100%|████████████████████████| Time: 0:00:13\u001b[39m\nloss: 1.0524701436360677\naccuracy: 0.6640625000000001\n\u001b[32mEpoch 5 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:05\u001b[39m\nloss: 0.6489803444366066\naccuracy: 0.7909226190476191\n\u001b[32mEpoch 6 ValidationPhase(): 100%|████████████████████████| Time: 0:00:12\u001b[39m\nloss: 0.8496215959390004\naccuracy: 0.7190104166666665\n\u001b[32mEpoch 6 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:06\u001b[39m\nloss: 0.5658585832637995\naccuracy: 0.8167517006802718\n\u001b[32mEpoch 7 ValidationPhase(): 100%|████████████████████████| Time: 0:00:13\u001b[39m\nloss: 0.7595664302508036\naccuracy: 0.7575520833333333\n\u001b[32mEpoch 7 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:05\u001b[39m\nloss: 0.5101636230540113\naccuracy: 0.8338647959183668\n\u001b[32mEpoch 8 ValidationPhase(): 100%|████████████████████████| Time: 0:00:12\u001b[39m\nloss: 0.8600163320700328\naccuracy: 0.7414062499999998\n\u001b[32mEpoch 8 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:05\u001b[39m\nloss: 0.45264889228911626\naccuracy: 0.8527848639455782\n\u001b[32mEpoch 9 ValidationPhase(): 100%|████████████████████████| Time: 0:00:13\u001b[39m\nloss: 0.8109628856182098\naccuracy: 0.753125\n\u001b[32mEpoch 9 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:05\u001b[39m\nloss: 0.40744140095451253\naccuracy: 0.8603316326530616\n\u001b[32mEpoch 10 ValidationPhase(): 100%|███████████████████████| Time: 0:00:12\u001b[39m\nloss: 0.6673034459352494\naccuracy: 0.7921874999999999\n\u001b[32mEpoch 10 TrainingPhase(): 100%|█████████████████████████| Time: 0:01:04\u001b[39m\nloss: 0.3401108379063963\naccuracy: 0.885841836734694\n\u001b[32mEpoch 11 ValidationPhase(): 100%|███████████████████████| Time: 0:00:12\u001b[39m\nloss: 0.7720626552899679\naccuracy: 0.7669270833333333\n"},{"doctype":"documentation","id":"references/FluxTraining.Scheduler","title":"Scheduler","text":" Scheduler ( schedules ... ) Callback for hyperparameter scheduling .  Takes pairs of  HyperParameter types and  Schedule s . See  the tutorial  for more information . Example es   =  length ( learner . data . training )\n lrschedule   =  Schedule ( [ 0  es ,  10  es ] ,  [ 0.1 ,  0.001 ] ,  Animations . sineio ( ) )\n scheduler   =  Scheduler (\n     LearningRate   =>  lrschedule \n ) See also  Schedule ."},{"doctype":"documentation","id":"references/FluxTraining.Permission","title":"Permission","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.Phase","title":"Phase","text":" abstract   type   Phase  Abstract supertype for all phases .  See  subtypes(FluxTraining.Phase) . A  Phase  is used in dispatch for training loop functions  step! and  epoch!  as well as in  Callback  handler methods  on ."}]