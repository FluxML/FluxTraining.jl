[{"doctype":"document","id":"documents/CHANGELOG.md","title":"News","text":"News Unreleased Added Support for  Optimisers.jl  https://github.com/FluxML/FluxTraining.jl/pull/114 0.3.0  04.04.2022 Added Short-hand  Learner  method  Learner(model lossfn data optim callbacks kwargs  The old method still exists but use is discouraged Compatibility with Flux.jl v0.13 once released Pollen.jl doucmentation frontend Changed BREAKING Hyperparameter scheduling now uses ParameterSchedules.jl instead of Animations.jl for defining schedules See  the docs  Fixes on documentation pages 0.2.0 Added New training loop API that is easier to extend Defining a  Phase  and  step  is all you need See  the new tutorial  and  the new reference  Relevant functions  epoch   step   runstep   runepoch Added  CHANGELOG.md  this file AbstractValidationPhase  as supertype for validation phases Documentation for callback helpers on  reference page Changed Batch  renamed to  Step  events  BatchBegin  now  StepBegin   BatchEnd  now  StepEnd CancelBatchException  now  CancelStepException  field  Learner.batch  now  Learner.step Learner.step/batch  is no longer a special  struct  but now a  PropDict  allowing you to set arbitrary fields Learner.model  can now be a  NamedTuple/Tuple  of models for use in custom training loops Likewise  learner.params  now resembles the structure of  learner.model  allowing separate access to parameters of different models Callbacks Added  init  method for callback initilization replacing the  Init  event which required a  Phase  to implement Scheduler  now has internal step counter and no longer relies on  Recorder s history This makes it easier to replace the scheduler without needing to offset the new schedules EarlyStopping  callback now uses criteria from  EarlyStopping.jl Removed Removed old training API Methods  fitbatch   fitbatchphase   fitepoch   fitepochphase  have all been removed"},{"doctype":"documentation","id":"references/FluxTraining.HyperParameter","title":"HyperParameter","text":"A hyperparameter is any state that influences the training and is not a parameter of the model Hyperparameters can be scheduled using the  Scheduler  callback"},{"doctype":"documentation","id":"references/FluxTraining.savemodel","title":"savemodel","text":""},{"doctype":"documentation","id":"references/FluxTraining.CheckModelLossStep","title":"CheckModelLossStep","text":""},{"doctype":"documentation","id":"references/FluxTraining.CancelStepException","title":"CancelStepException","text":"learner phase _ xs ys batches learner phase xs ys _ state isnan state loss throw Throw during fitting to cancel the currently running step This prematurely ends the current step without throwing an error Must be thrown inside the context of  runstep  Examples"},{"doctype":"documentation","id":"references/FluxTraining.ConflictResolution","title":"ConflictResolution","text":"A conflict resolution strategy for resolving write/write conflicts of two callbacks See  resolveconflict "},{"doctype":"documentation","id":"references/FluxTraining.replacecallback!","title":"replacecallback!","text":"Replace existing callback of type  C  on learner with  callback  Return the replaced callback If  learner  doesn't have a callback of type  C  add  callback  and return  nothing "},{"doctype":"document","id":"documents/docs/tutorials/hyperparameters.md","title":"Hyperparameter scheduling","text":"ParameterSchedulers Shifted Sin es length traindl schedule Sequence Sin λ0 λ1 period es es Shifted Sin λ0 λ1 period es es es learner model model data opt lossfn schedule Hyperparameter scheduling When training neural networks you often have to tune hyperparameters In  FluxTraining.jl  the following definition is used A hyperparameter is any state that influences the training and is not a parameter of the model Common hyperparameters to worry about are the learning rate batch size and regularization strength In recent years it has also become common practice to schedule some hyperparameters The cyclical learning rate schedule introduced in  L Smith 2015  for example changes the learning rate every step to speed up convergence FluxTraining.jl  provides an extensible interface for hyperparameter scheduling that is not restricted to optimizer hyperparameters as in many other training frameworks To use it you have to create a  Scheduler  a callback that can be passed to a  Learner  Scheduler s constructor takes pairs of hyperparameter types and associated schedules As an example LearningRate  is a hyperparameter type representing the optimizer's step size and schedule  Exp(γ=0.9  represents an exponential decay scheduling We can create the callback scheduling the learning rate according to  Scheduler(LearningRate  schedule  Schedule s are built around  ParameterSchedulers.jl  See that package's documentation for more details on how to construct them One-cycle learning rate Let's define a  Schedule  that follows the above-mentioned cyclical learning rate schedule The idea is to start with a small learning rate gradually increase it and then slowly decrease it again For example we could start with a learning rate of 0.01 increase it to 0.1 over 3 epochs and then down to 0.001 over 7 epochs Let's also use cosine annealing a common practice that makes sure the values are interpolated more smoothly In code that looks like this For convenience you can also use the  onecycle  helper to create this  Schedule  See  ParameterSchedulers.jl documentation  for more details on warm-up schedules Extending You can create and schedule your own hyperparameters To do this you will need to define a type for your hyperparameter e.g  abstract type MyParam  HyperParameter end  how to set the hyperparameter by implementing  sethyperparameter learner Type{MyParam value what state needs to be accessed to set the hyperparameter by implementing  stateaccess Type{MyParam  See  custom callbacks  for more info on why this is needed Kinds of hyperparameters Hyperparameters don't need to belong to the optimizer For example you could create a hyperparameter for batch size That is not implemented here because this package is agnostic of the data iterators and the implementation would differ for every type of iterator"},{"doctype":"documentation","id":"references/FluxTraining.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.tests","title":"tests","text":""},{"doctype":"documentation","id":"references/FluxTraining.print_epoch_table","title":"print_epoch_table","text":""},{"doctype":"documentation","id":"references/FluxTraining.log_to","title":"log_to","text":"Log  loggable  to  backend  with  group  to index  i  loggable  is any  Loggables.Loggable group  can be a  String  or a tuple of  String s implying some grouping which can be used by a supporting backend i  is a step counter and unique for every group"},{"doctype":"documentation","id":"references/FluxTraining.@pack_History!","title":"@pack_History!","text":""},{"doctype":"documentation","id":"references/FluxTraining.AbstractCallback","title":"AbstractCallback","text":"Supertype of  SafeCallback  Callback  When implementing callbacks you should subtype  SafeCallback  instead"},{"doctype":"documentation","id":"references/FluxTraining.getindexperm","title":"getindexperm","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.ValidationPhase","title":"ValidationPhase","text":"A regular validation phase It iterates over batches in  learner.data.validation  and performs a forward pass Throws the following events  EpochBegin   StepBegin   LossBegin   StepEnd   EpochEnd  Throws the following events in this order EpochBegin  when an epoch starts StepBegin  when a step starts LossBegin  after the forward pass but before loss calculation StepEnd  when a step ends and EpochEnd  when an epoch ends It writes the following step state to  learner.state  grouped by the event from which on it is available StepBegin  xs  and  ys  encoded input and target batch LossBegin  ŷs  model output StepEnd  loss  loss"},{"doctype":"documentation","id":"references/FluxTraining.iterpairs","title":"iterpairs","text":"Iterators over the Cartesian product of  a  with itself skipping any pairs  a b  where  a  b "},{"doctype":"documentation","id":"references/FluxTraining.Callback","title":"Callback","text":"Supertype of all callbacks Callbacks add custom functionality to the training loop by hooking into different  Events.Event s Any  Callback  can be used by passing it to  Learner  See  subtypes(FluxTraining.Callback  for implementations Extending See  Custom callbacks  for a less succinct tutorial format Create a  struct MyCallback  that subtypes  FluxTraining.Callback  Add event handlers by implementing methods for  on event phase callback learner  Methods should always dispatch on your callback and may dispatch on specific  Phases.Phase s and  Events.Event s For example to implement an event handler that runs at the end of every step during training  on(::StepEnd AbstractTrainingPhase MyCallback learner  Define what state the callback accesses and/or modifies by implementing  stateaccess MyCallback  While  learner  is always passed as an argument to  on  event handlers by default a callback can not read or write to its fields See  stateaccess  for more detail If a callback needs to write some state that other callbacks should be able to access it can store it in  learner.cbstate  if you add a permission in  stateaccess  If the callback needs some one-time initialization you can implement  init  which will be run at least once before any step is run"},{"doctype":"documentation","id":"references/FluxTraining.StopOnNaNLoss","title":"StopOnNaNLoss","text":"Stops the training when a NaN loss is encountered This callback is added by default to every  Learner  unless you pass in  usedefaultcallbacks  false "},{"doctype":"documentation","id":"references/FluxTraining.Protected","title":"Protected","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.StepBegin","title":"StepBegin","text":"Event  called at the beginning of a batch"},{"doctype":"document","id":"documents/LICENSE.md","title":"LICENSE","text":"MIT License Copyright c 2020 lorenzoh  lorenz.ohly@gmail.com Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and/or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE"},{"doctype":"documentation","id":"references/FluxTraining.AbstractMetric","title":"AbstractMetric","text":"Abstract type for metrics passed to  Metrics  For most use cases you should use  Metric  the standard implementation Interface If  Metric  doesn't fit your use case you can create a new subtype of  AbstractMetric  and implement the following methods to make it compatible with  Metrics  reset metric step metric learner stepvalue metric epochvalue metric metricname metric"},{"doctype":"document","id":"documents/docs/tutorials/mnist.ipynb","title":"Training an image classifier","text":"Pkg Pkg add DataLoaders DataLoader MLDataPattern splitobs Flux xs ys Float32 reshape img img Flux Data MNIST images Float32 Flux onehot y y Flux Data MNIST labels traindata valdata splitobs xs ys trainiter valiter DataLoader traindata buffered DataLoader valdata buffered model Chain Conv relu pad stride Conv relu pad GlobalMeanPool Flux flatten Dense lossfn Flux Losses logitcrossentropy optimizer Flux ADAM learner model lossfn callbacks optimizer learner trainiter validiter Training an image classifier Let's put  FluxTraining.jl  to train a model on the MNIST dataset MNIST is simple enough that we can focus on the part where  FluxTraining.jl  comes in the training If you want to see examples of using FluxTraining.jl on larger datasets see the documentation of  FastAI.jl  Setup If you want to run this tutorial yourself you can find the notebook file  here  To make data loading and batching a bit easier we'll install some additional dependencies Now we can import everything we'll need Overview There are 4 pieces that you always need to construct and train a  Learner  a model data an optimizer and a loss function Building a  Learner Let's look at the  data  first FluxTraining.jl  is agnostic of the data source The only requirements are it is iterable and each iteration returns a tuple  xs ys the model can take in  xs  i.e  model(xs  works and the loss function can take model outputs and  ys  i.e  lossfn(model(xs ys  returns a scalar Glossing over the details as it's not the focus of this tutorial here's the code for getting a data iterator of the MNIST dataset We use  DataLoaders.DataLoader  to create an iterator of batches from our dataset Next let's create a simple  Flux.jl   model  that we'll train to classify the MNIST digits We'll use  categorical cross entropy  as a  loss function  and  ADAM  as an  optimizer  Now we're ready to create a  Learner  At this point you can also add any callbacks like  ToGPU  to run the training on your GPU if you have one available Some callbacks are also  included by default  Since we're classifying digits we also use the  Metrics  callback to track the accuracy of the model's predictions Training With a  Learner  inplace training is as simple as calling  fit learner nepochs dataiters "},{"doctype":"documentation","id":"references/FluxTraining.@pack_History","title":"@pack_History","text":""},{"doctype":"documentation","id":"references/FluxTraining.edgesrunafter","title":"edgesrunafter","text":"Return a vector of  Edge s representing dependencies defined by  runafter "},{"doctype":"documentation","id":"references/FluxTraining.LinearRunner","title":"LinearRunner","text":""},{"doctype":"documentation","id":"references/FluxTraining.protect","title":"protect","text":""},{"doctype":"documentation","id":"references/FluxTraining.formataccess","title":"formataccess","text":""},{"doctype":"documentation","id":"references/FluxTraining.resolveconflict","title":"resolveconflict","text":"Define a conflict resolution strategy for resolving a write/write conflict between two callbacks The default is  NotDefined  which will result in an error and a message to implement this method To implement dispatch on the callback types that you which to resolve in any order and return one of the following Unresolvable   if the callbacks must not be used together RunFirst cb  if one of the callbacks needs to run first or NoConflict   if the callbacks may run together in any order"},{"doctype":"document","id":"documents/docs/callbacks/reference.md","title":"Callback reference","text":"Callback reference Included callbacks FluxTraining.jl  comes with many callbacks included Some of them are added to  Learner  by default here marked with a   Callback   Description   Metrics  Tracks loss and additional metrics on a per-step and per-epoch base  Recorder  Records training stats like number of steps and epochs  ProgressPrinter  Prints a progress bar for the current epoch during training  MetricsPrinter  Prints out metrics after every epoch  StopOnNaNLoss  Stops training early if a step loss is  NaN   SanityCheck  Performs sanity checks on data model and loss before training  ToGPU  Trains using a CUDA GPU if available  Checkpointer  Saves the model after every epoch  EarlyStopping  Stops training early when a criterion is met  Scheduler  Schedules hyperparameters  LogMetrics  Logs metrics to a logging backend  LogHyperParams  Logs hyperparameters to a logging backend  LogVisualization  Logs visualization to a logging backend  LogHistograms  Logs model weight histograms to a logging backend To construct a learner without default callbacks pass  usedefaultcallbacks=false  when constructing it For working with callbacks on an existing  Learner  see setcallbacks addcallback getcallback replacecallback removecallback There are also some utilities for creating callbacks CustomCallback  to quickly hook a function into an event throttle  to run a callback only after every  n  events or every  t  seconds Extension API The following types and functions can be used to create custom callbacks Read the  custom callbacks guide  for more context Callback stateaccess runafter resolveconflict"},{"doctype":"documentation","id":"references/FluxTraining.Loggables","title":"Loggables","text":""},{"doctype":"documentation","id":"references/FluxTraining.setlearningrate!","title":"setlearningrate!","text":""},{"doctype":"documentation","id":"references/FluxTraining.stepvalue","title":"stepvalue","text":""},{"doctype":"documentation","id":"references/FluxTraining.@unpack_History","title":"@unpack_History","text":""},{"doctype":"documentation","id":"references/FluxTraining.ProtectedException","title":"ProtectedException","text":""},{"doctype":"documentation","id":"references/FluxTraining.FrequencyThrottle","title":"FrequencyThrottle","text":""},{"doctype":"documentation","id":"references/FluxTraining.RunFirst","title":"RunFirst","text":"Return  RunFirst(cb1/cb2  from  resolveconflict cb1 cb2  to indicate that one of the callbacks should always run before the other"},{"doctype":"documentation","id":"references/FluxTraining.Events.StepEnd","title":"StepEnd","text":"Event  called at the end of a batch"},{"doctype":"documentation","id":"references/FluxTraining.SanityCheck","title":"SanityCheck","text":"Callback that runs sanity  Check s when the  Learner  is initialized If  usedefault  is  true  it will run all checks in FluxTraining.CHECKS in addition to the ones you pass in"},{"doctype":"documentation","id":"references/FluxTraining.ToDevice","title":"ToDevice","text":"Moves model and step data to a device using  movedatafn  for step data and  movemodelfn  for the model For example  ToDevice(Flux.gpu Flux.gpu  moves them to a GPU if available See  ToGPU  By default only moves  step.xs  and  step.ys  but this can be extended to other state by implementing  on(::StepBegin MyCustomPhase ToDevice learner "},{"doctype":"documentation","id":"references/FluxTraining._on","title":"_on","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Audio","title":"Audio","text":""},{"doctype":"document","id":"documents/docs/imagenette_demo.ipynb","title":"imagenette_demo","text":"DataLoaders Flux DataAugmentation DeepLearningTasks DLDatasets MLDataPattern LearnBase ProgressBars FluxModels task ImageClassification sz labeltoint metadata ImageNette labeltoclass obsfn image label image labeltoint label trainds valds DLDatasets loaddataset ImageNette split bs traindl taskdataloader task trainds bs obsfn valdl taskdataloader task valds bs obsfn model gpu Chain xresnet18 FluxModels classificationhead task nclasses learner model traindl valdl ADAM Flux Losses logitcrossentropy callbacks metrics schedule Schedules onecycleschedule length traindl learner"},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Image","title":"Image","text":""},{"doctype":"documentation","id":"references/FluxTraining.CustomCallback","title":"CustomCallback","text":"cb learner println A callback that runs  f(learner  every time an event of type  Event  during a phase of type in  Phase  If  f  needs to access learner state pass  access  a named tuple in the same form as  stateaccess  Instead of using  CustomCallback  it is recommended to properly implement a  Callback  Examples We can get a quick idea of when a new epoch starts as follows"},{"doctype":"documentation","id":"references/FluxTraining._dataiters","title":"_dataiters","text":""},{"doctype":"documentation","id":"references/FluxTraining.ToGPU","title":"ToGPU","text":"Callback that moves model and batch data to the GPU during training Convenience for  ToDevice Flux.gpu "},{"doctype":"documentation","id":"references/FluxTraining.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.TESTSET_MACROS","title":"TESTSET_MACROS","text":""},{"doctype":"documentation","id":"references/FluxTraining.on","title":"on","text":"Handle  event  with  Callback   callback  By default this event handler does nothing for a callback To see events which an  AbstractCallback  handles use Extending You can add event handlers to  Callback s by implementing a method for  on  See also  Callback  and  custom callbacks  A method of  on  should  always  dispatch on the callback type i.e  on(event phase cb::MyCallback learner  It may also dispatch on specific  Event s and  Phase  It should not dispatch on a specific type for  learner "},{"doctype":"documentation","id":"references/FluxTraining.accuracy","title":"accuracy","text":""},{"doctype":"documentation","id":"references/FluxTraining.Check","title":"Check","text":""},{"doctype":"documentation","id":"references/FluxTraining.throttle","title":"throttle","text":"Throttle  Event  type for  callback  so that it is triggered either only every  freq th time  or every  seconds  seconds Examples If you want to only sporadically log metrics  LogMetrics  or images  LogVisualization   throttle  can be used as follows Every 10 steps Or every 5 seconds"},{"doctype":"documentation","id":"references/FluxTraining.Callbacks","title":"Callbacks","text":""},{"doctype":"documentation","id":"references/FluxTraining.LearningRate","title":"LearningRate","text":"Hyperparameter for the optimizer's learning rate See  Scheduler  and  hyperparameter scheduling "},{"doctype":"documentation","id":"references/FluxTraining.Metric","title":"Metric","text":"cb Flux mse device gpu name cb expensivemetric P Implementation of  AbstractMetric  that can be used with the  Metrics  callback Arguments Positional metricfn(ŷs ys  should return a number Keyword statistic  is a  OnlineStats.Statistic  that is updated after every step The default is  OnlineStats.Mean name  is used for printing device  is a function applied to  ŷs  and  ys  before passing them to  metricfn  The default is  Flux.cpu  so that the callback works if  metricfn  doesn't support arrays from other device types If for example  metricfn  works on  CurArray s you can pass  device  Flux.gpu  phase  Phase  a sub)type of  Phase  that restricts for which phases the metric is computed Examples Metric(accuracy Metric(Flux.mse device  gpu name  Mean Squared Error Metric(Flux.mae device  gpu If a metric is expensive to compute and you don't want it to slow down the training phase you can compute it on the validation phase only"},{"doctype":"documentation","id":"references/FluxTraining.FitException","title":"FitException","text":"Abstract types for exceptions that can be thrown during fitting to change its control flow See  CancelStepException   CancelEpochException   CancelFittingException "},{"doctype":"document","id":"documents/docs/background/lossfunction.md","title":"Loss functions","text":"xs ys dataiter ŷs model xs lossfn ŷs ys Number Loss functions A loss function compares model outputs to true targets resulting in a loss For a loss function to be compatible with the standard supervised training loop the following properties must hold Firstly the loss function should accept the model outputs and targets and return a single scalar value Given a  data iterator   dataiter  and a  model   model  The loss function must also be differentiable so that gradients can be calculated during training See  models  for more on how to check this Creating loss functions Flux.jl comes with a lot of commonly used loss functions built-in in its submodule  Flux.Losses  See  Flux.jl loss functions  for a complete reference You can also write your own loss functions If you are using non-mutating array operations there is a good chance that it will be differentiable and also be compatible with GPU arrays from  CUDA.jl "},{"doctype":"documentation","id":"references/FluxTraining.setupoptimstate","title":"setupoptimstate","text":""},{"doctype":"documentation","id":"references/FluxTraining._combinename","title":"_combinename","text":""},{"doctype":"documentation","id":"references/FluxTraining.testbatch","title":"testbatch","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.AbstractTrainingPhase","title":"AbstractTrainingPhase","text":"An abstract type for phases where parameter updates are being made This exists so callbacks can dispatch on it and work with custom training phases The default implementation for supervised tasks is  TrainingPhase "},{"doctype":"documentation","id":"references/FluxTraining.stateaccess","title":"stateaccess","text":"model params step xs ys Return a named tuple determining what learner state  callback  can access The default is    the empty named tuple meaning no state can be accessed Implementations of  stateaccess  should always return the least permissions possible Extending For example the  ToGPU  callback needs to write both the model and the batch data so its  stateaccess  implementation is When defining  stateaccess  be careful that you do return a  NamedTuple   x  Read  is one but  x  Read  without the comma is parsed as an assignment with value  Read  Defines what  Learner  state is accessed when calling  sethyperparameter  and  gethyperparameter  This is needed so that  Scheduler  can access the state"},{"doctype":"document","id":"documents/README.md","title":"FluxTraining.jl","text":"learner model lossfn learner trainiter validiter FluxTraining.jl Docs master A Julia package for using and writing powerful extensible training loops for deep learning models What does it do Implements a training loop to take the boilerplate out of training deep learning models Lets you add features to training loops through reusable  callbacks Comes with callbacks for many common use cases like  hyperparameter scheduling   metrics  tracking and  logging   checkpointing   early stopping  and  more Is extensible by creating  custom reusable callbacks  or even  custom training loops When should you use FluxTraining.jl You don't want to implement your own metrics tracking and hyperparameter scheduling or  insert common training feature here  for the 10th time You want to use composable and reusable components that enhance your training loop You want a simple training loop with reasonable defaults that can grow to the needs of your project How do you use it Install like any other Julia package using the package manager After installation import it create a  Learner  from a  Flux.jl  model data iterators an optimizer and a loss function Finally train with  fit  Next you may want to read Getting started A full example training an image classifier on the MNIST dataset The  documentation of FastAI.jl  which features many end-to-end examples Acknowledgements The design of FluxTraining.jl's two-way callbacks is adapted from  fastai s training loop"},{"doctype":"documentation","id":"references/FluxTraining.GarbageCollect","title":"GarbageCollect","text":"Every  nsteps  steps forces garbage collection Use this if you get memory leaks from for example parallel data loading Performs an additional C-call on Linux systems that can sometimes help"},{"doctype":"documentation","id":"references/FluxTraining.runtests","title":"runtests","text":"Equivalent to  ReTest.retest(FluxTraining pattern kwargs  This function is defined automatically in any module containing a  testset  possibly nested within submodules"},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Graph","title":"Graph","text":""},{"doctype":"documentation","id":"references/FluxTraining.LogVisualization","title":"LogVisualization","text":"Logs images created by  visfn(learner.step  to  backends  every  freq  steps"},{"doctype":"documentation","id":"references/FluxTraining.Events","title":"Events","text":""},{"doctype":"documentation","id":"references/FluxTraining.NoConflict","title":"NoConflict","text":"Return from  resolveconflict  to indicate that while the callbacks modify the same state they can be used together without any problems"},{"doctype":"documentation","id":"references/FluxTraining.defaultcallbacks","title":"defaultcallbacks","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.Event","title":"Event","text":"Abstract type for events that callbacks can hook into"},{"doctype":"documentation","id":"references/FluxTraining.Learner","title":"Learner","text":"Holds and coordinates all state of the training  model  is trained by optimizing  lossfn  with  optimizer  on  data  Arguments Positional arguments model  A Flux.jl model or a  NamedTuple  of models lossfn  Loss function with signature  lossfn(model(x y  Number  Keyword arguments optional data    Data iterators A 2-tuple will be treated as  trainingdataiter validdataiter  You can also pass in an empty tuple    and use the  epoch  method with a  dataiter  as third argument A data iterator is an iterable over batches For regular supervised training each batch should be a tuple  xs ys  optimizer  ADAM  The optimizer used to update the  model s weights callbacks    A list of callbacks that should be used If  usedefaultcallbacks  true  this will be extended by the default callbacks usedefaultcallbacks  true  Whether to add some basic callbacks Included are  Metrics   Recorder   ProgressPrinter   StopOnNaNLoss  and  MetricsPrinter  cbrunner  LinearRunner  Callback runner to use Fields Use this as a reference when implementing callbacks model   optimizer  and  lossfn  are stored as passed in data  is a  PropDict  of data iterators usually  training  and  validation  params  An instance of  model s parameters of type  Flux.Params  If  model  is a  NamedTuple  then  params  is a  NamedTuple  as well step PropDict  State of the last step Contents depend on the last run  Phase  cbstate PropDict  Special state container that callbacks can save state to for other callbacks Its keys depend on what callbacks are being used See the  custom callbacks guide  for more info"},{"doctype":"documentation","id":"references/FluxTraining.setindexperm!","title":"setindexperm!","text":""},{"doctype":"documentation","id":"references/FluxTraining.CancelFittingException","title":"CancelFittingException","text":"Throw during fitting to cancel it"},{"doctype":"documentation","id":"references/FluxTraining.metricname","title":"metricname","text":""},{"doctype":"documentation","id":"references/FluxTraining.ES","title":"ES","text":""},{"doctype":"documentation","id":"references/FluxTraining.runafter","title":"runafter","text":""},{"doctype":"documentation","id":"references/FluxTraining.MetricsPrinter","title":"MetricsPrinter","text":"Callback that prints metrics after every epoch Relies on the metrics computed by  Metrics  so will error if no  Metrics  callback is used This callback is added by default to every  Learner  unless you pass in  usedefaultcallbacks  false "},{"doctype":"documentation","id":"references/FluxTraining.hasconflict","title":"hasconflict","text":""},{"doctype":"documentation","id":"references/FluxTraining._update!","title":"_update!","text":""},{"doctype":"documentation","id":"references/FluxTraining.runchecks","title":"runchecks","text":""},{"doctype":"documentation","id":"references/FluxTraining.TimeThrottle","title":"TimeThrottle","text":""},{"doctype":"documentation","id":"references/FluxTraining.ConditionalCallback","title":"ConditionalCallback","text":"Wrapper callback that only forwards events to the wrapped callback if  CallbackCondition   condition  is met See  throttle "},{"doctype":"documentation","id":"references/FluxTraining.LogHyperParams","title":"LogHyperParams","text":"Callback that logs hyperparameters to one or more  LoggerBackend s See also  LoggerBackend   Loggables.Loggable   log_to   TensorBoardBackend Example"},{"doctype":"documentation","id":"references/FluxTraining.History","title":"History","text":""},{"doctype":"documentation","id":"references/FluxTraining.Scheduler","title":"Scheduler","text":"es length learner data training lrschedule ParameterSchedulers Step λ γ step_sizes scheduler lrschedule Callback for hyperparameter scheduling Takes pairs of  HyperParameter  types and  ParameterSchedulers.jl schedules  See  the tutorial  for more information Example"},{"doctype":"documentation","id":"references/FluxTraining.Phases.TrainingPhase","title":"TrainingPhase","text":"A regular training phase for supervised learning It iterates over batches in  learner.data.training  and updates the model parameters using  learner.optim  after calculating the gradients Throws the following events in this order EpochBegin  when an epoch starts StepBegin  when a step starts LossBegin  after the forward pass but before loss calculation BackwardBegin  after loss calculation but before backward pass BackwardEnd  after the bacward pass but before the optimization step StepEnd  when a step ends and EpochEnd  when an epoch ends It writes the following step state to  learner.state  grouped by the event from which on it is available StepBegin  xs  and  ys  encoded input and target batch LossBegin  ŷs  model output BackwardBegin  loss  loss BackwardEnd  grads  calculated gradients"},{"doctype":"document","id":"documents/docs/background/model.md","title":"Models","text":"xs ys first dataiter ŷs model xs Flux Zygote xs ys first dataiter lossfn Flux mse grads Zygote gradient Flux params model lossfn model xs ys Models FluxTraining.jl works with all  Flux.jl compatible models Unless you are using a  custom training loop  a  model  is expected to take a single input  xs  which corresponds to the encoded inputs returned by your  data iterator  This means the following has to work model  also has to be differentiable If you're composing Flux.jl layers this is likely the case You can always make sure by testing Creating models The simplest way to create a Flux.jl-compatible model is to use layers from Flux.jl A good entrypoint is  this tutorial in Flux's documentation There is also a large number of packages that provide complete model architectures or domain-specific layers Below is a non-exhaustive list Metalhead.jl  implements common model architectures for computer vision GraphNeuralNetworks.jl  provides layers and utilities for graph neural networks Transformers.jl  implements transformer models including pretrained language models"},{"doctype":"documentation","id":"references/FluxTraining.Checkpointer","title":"Checkpointer","text":"Saves  learner.model  to  folder  after every  AbstractTrainingPhase  Use  FluxTraining loadmodel  to load a model"},{"doctype":"documentation","id":"references/FluxTraining.Events.BackwardEnd","title":"BackwardEnd","text":"Event  called between calculating gradients and updating parameters"},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Histogram","title":"Histogram","text":""},{"doctype":"documentation","id":"references/FluxTraining.CheckIteratesTuples","title":"CheckIteratesTuples","text":""},{"doctype":"documentation","id":"references/FluxTraining.phasedataiter","title":"phasedataiter","text":""},{"doctype":"documentation","id":"references/FluxTraining.EarlyStopping","title":"EarlyStopping","text":"model lossfn callbacks Disjunction InvalidValue TimeLimit callback Disjunction InvalidValue TimeLimit model lossfn callbacks callback Stop training early when  criteria  are met See  EarlyStopping.jl  for available stopping criteria Passing an integer  n  uses the simple patience criterion stop if the validation loss hasn't increased for  n  epochs You can control which phases are taken to measure the out-of-sample loss and the training loss with keyword arguments  trainphase  default  AbstractTrainingPhase  and  testphase  default  AbstractValidationPhase  Examples"},{"doctype":"documentation","id":"references/FluxTraining.sethyperparameter!","title":"sethyperparameter!","text":"Sets hyperparameter  H  to  value  on  learner  returning the modified learner"},{"doctype":"documentation","id":"references/FluxTraining.Write","title":"Write","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.EpochEnd","title":"EpochEnd","text":"Event  called at the end of an epoch"},{"doctype":"documentation","id":"references/FluxTraining.callbackgraph","title":"callbackgraph","text":"Creates a directed acyclic graph from a list of  callbacks  Ordering is given through  runafter  and  resolveconflict  If a write conflict cannot be resolved i.e  resolveconflict  is not implemented throws an error"},{"doctype":"documentation","id":"references/FluxTraining.Phases","title":"Phases","text":""},{"doctype":"documentation","id":"references/FluxTraining.TensorBoardBackend","title":"TensorBoardBackend","text":"TensorBoard backend for logging callbacks Takes the same arguments as  TensorBoardLogger.TBLogger "},{"doctype":"document","id":"documents/docs/background/optimizer.md","title":"Optimizers","text":"Optimizers An optimizer takes the calculated gradients from a training step and uses them to update the parameters of a  model  FluxTraining.jl currently only supports optimizers from  Flux.jl  A complete reference of optimizers in Flux.jl can be found  here "},{"doctype":"documentation","id":"references/FluxTraining.testlearner","title":"testlearner","text":"Construct a  Learner  with a simple optimization problem This learner should be used in tests that require training a model e.g for callbacks"},{"doctype":"documentation","id":"references/FluxTraining.epochvalue","title":"epochvalue","text":""},{"doctype":"document","id":"documents/docs/getting_started.md","title":"Getting started","text":"learner model lossfn data traindata valdata learner Getting started Let's look at a simple training example In  FluxTraining.jl  a  Learner  holds all state necessary for training To get started you need a  model training and validation  data iterators a  loss function  and an  optimizer First we define the necessary pieces Then we construct a  Learner  And train for 10 epochs"},{"doctype":"documentation","id":"references/FluxTraining.garbagecollect","title":"garbagecollect","text":""},{"doctype":"document","id":"documents/docs/custom_training.ipynb","title":"Custom training loops","text":"grads gradient params lossfn model xs ys update! optim params sum grads grads Array Grads undef Threads nthreads Threads i xs_ ys_ enumerate scatter xs ys Threads nthreads grads i gradient params lossfn model xs_ ys_ gs sum grads update! optim params gs xs_fake mgen batchsize xs cat xs_true xs_fake ys onehot vcat trues batchsize falses batchsize grads gradient paramscrit lossfncrit mcrit xs ys update! optim paramscrit grads grads gradient paramsgen xs_fake mgen batchsize ys_fake onehot falses batchsize lossfncrit crit xs_fake ys_fake grads gradient executionctx params model_ params_ xs_ ys_ lossfn model xs_ ys_ update! optim params grads model DataParallel model grads gradient params lossfn model xs ys update! optim params sum grads Custom training loops How can we compose changes to the training loop together As an example we want to be able to do data-parallel training or GAN training but also data-parallel GAN training I want to give some thoughts on this and propose possible solutions to make this possible in  FastAI.jl  Currently implementing custom training behavior is possible by subtyping  FitPhase  and implementing  fitbatchphase!(learner phase::MyPhase  However this is not composable i.e it doesn't allow you to combine a  DataParallelTrainingPhase  and  GANTrainingPhase  Below are some examples that show how the contents of  fitbatchphase  can be changed to illustrate this For simplicity they don't include callbacks and state handling Regular training 1 Data-parallel training on CPU 2 GAN training 3 Solutions I have found two approaches to deal with this  Both focus on removing execution logic from  fitbatchphase  making them composable with custom  Phase s  like  GANTrainingPhase  that change the  semantics  of the training loop On one hand there are extensions to the training loop that change the  execution  e.g parallel and distributed CPU and GPU training on the other hand you have those that change the  semantics  e.g GAN training The proposed solutions make the assumption that different  semantics  don't need to be composed but should be composable with different execution contexts S1 Abstract gradient step and possibly others out Modifications to the execution of the training loop could be implemented by wrapping in an execution context In the below example  gradientphase  could dispatch to the regular gradient calculation in 1 or the data-parallel approach 2 depending on  executionctx  This would mean that only  semantic  changes to the training loop would use overloading of  fitbatchphase  with a custom  FitPhase  Changes to the  execution  work by dispatching on execution contexts e.g  gradientphase(::Linear   or  gradientphase(::DataParallel   Advantages implementation definitely doable Disadvantages implementation dependent on requirements i.e unsure which pieces of the training step need to be overloadable and which state needs to be passed to closures S2 Wrapper for  model The idea is to wrap the  model  in an execution context e.g  DataParallel(model  The wrapper is then responsible for exhibiting the correct behavior on the forward and backward pass This is  what PyTorch does  No changes to the training loop would need to be made The implementation for the forward pass should be straightforward and similar to the above sketch 2 however I'm not sure how to make sure that the backward pass is also computed in parallel can custom gradient definitions include multi-threading code what about the loss function that is not wrapped Advantages no changes needed to state and event handling Disadvantages not sure if such a simple API is possible to implement for all scenarios bit unelegant model is not a pure function anymore"},{"doctype":"documentation","id":"references/FluxTraining.epoch!","title":"epoch!","text":"Train  learner  for one epoch on  dataiter  Iterates through  dataiter  and  step s for each batch/item If no data iterator is passed in use  learner.data[phasedataiter(phase  Extending The default implementation iterates over every batch in  dataiter  and calls  step  for each This behavior can be overloaded by implementing  epoch!(learner MyPhase dataiter  If you're implementing a custom  epoch  method it is recommended you make use of  runepoch  to get begin and end events as well as proper handling of  CancelEpochException s See the default implementation for reference"},{"doctype":"documentation","id":"references/FluxTraining.SmoothLoss","title":"SmoothLoss","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.Phase","title":"Phase","text":"Abstract supertype for all phases See  subtypes(FluxTraining.Phase  A  Phase  is used in dispatch for training loop functions  step  and  epoch  as well as in  Callback  handler methods  on "},{"doctype":"documentation","id":"references/FluxTraining.setcallbacks!","title":"setcallbacks!","text":"Set  learner s callbacks to  callbacks  removing all current callbacks"},{"doctype":"documentation","id":"references/FluxTraining.Unresolvable","title":"Unresolvable","text":"Return from  resolveconflict  to indicate that two callbacks are incompatible and cannot be used together"},{"doctype":"documentation","id":"references/FluxTraining","title":"FluxTraining","text":""},{"doctype":"documentation","id":"references/FluxTraining.testbatches","title":"testbatches","text":""},{"doctype":"document","id":"documents/docs/overview.md","title":"Getting started","text":"Getting started There are some user-centric tutorials that will introduce you to features of  FluxTraining.jl  one at a time Alternatively you can read more about how  FluxTraining.jl  is implemented and how you can extend it Tutorials MNIST training Hyperparameter scheduling Interfaces Training loop Find out how  the training loop  is built and how to customize it Callbacks Callbacks are a powerful feature of  FluxTraining.jl  allowing you to add functionality to the training loop at different points Find out how to  use callbacks  when training what callbacks come  included with  FluxTraining.jl  or how callbacks work and how to  implement your own"},{"doctype":"documentation","id":"references/FluxTraining.accesses","title":"accesses","text":"Enumerate all valid state accesses of permissions of kind  perm  accesses((x  Read Read  x   accesses((x  Read Write  "},{"doctype":"document","id":"documents/docs/callbacks/tipstricks.md","title":"Tips & tricks","text":"methods Any Any Any learner nothing nothing callbacks GraphPlot gplot learner callbacks graph nodelabel learner callbacks cbs layout stressmajorize_layout Tips  tricks Listing event handlers for a callback Use  Base.methods  to check what events a callback handles Visualize the callback dependency graph You can use  GraphPlot.jl  to visualize the dependencies between callbacks the target of an arrow depends on the origin As an example for a detected dependency we can see that  MetricsPrinter  runs after  Metrics   MetricsPrinter  prints the values of all metrics so  Metrics  needs to run first"},{"doctype":"documentation","id":"references/FluxTraining.Phases.AbstractValidationPhase","title":"AbstractValidationPhase","text":"An abstract type for phases where no parameter updates are being made This exists so callbacks can dispatch on it and work with custom validation phases The default implementation for supervised tasks is  ValidationPhase "},{"doctype":"documentation","id":"references/FluxTraining.NotDefined","title":"NotDefined","text":"The default implementation of  resolveconflict  If a conflict is detected this ensures an error message is printed"},{"doctype":"documentation","id":"references/FluxTraining.findconflicts","title":"findconflicts","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Text","title":"Text","text":""},{"doctype":"documentation","id":"references/FluxTraining.PropDict","title":"PropDict","text":"Like a  Dict{Symbol  but attribute syntax can be used to access values"},{"doctype":"document","id":"documents/docs/callbacks/usage.md","title":"How to use callbacks","text":"model data lossfn nothing nothing nothing learner model lossfn callbacks data data learner callbacks cbs How to use callbacks Callbacks allow injecting functionality at many points during the training loop To use them simply pass each callback to  Learner  Some useful callbacks are added by default Below the callbacks of  learner  are shown Both the explicitly passed callbacks and the default ones are included See  callback reference  for a list of all callbacks included in  FluxTraining.jl  and their documentation Ordering The order the callbacks are passed in doesn't matter  FluxTraining.jl  creates a dependency graph that makes sure the callbacks are run in the correct order Read  custom callbacks  to find out how to create callbacks yourself"},{"doctype":"documentation","id":"references/FluxTraining.Events.LossBegin","title":"LossBegin","text":"Event  called between calculating  y_pred  and calculating loss"},{"doctype":"documentation","id":"references/FluxTraining.Recorder","title":"Recorder","text":"Maintains a  History  It's stored in  learner.cbstate.history "},{"doctype":"documentation","id":"references/FluxTraining.LogMetrics","title":"LogMetrics","text":"logcb model lossfn callbacks logcb Callback that logs step and epoch metrics to one or more  LoggerBackend s See also  LoggerBackend   Loggables.Loggable   log_to   TensorBoardBackend Example"},{"doctype":"documentation","id":"references/FluxTraining.fit!","title":"fit!","text":"learner learner traindl valdl Train  learner  for  nepochs  of training and validation each Use data iterators that are passed in If none are given use  learner.data.training  and  learner.data.validation  Examples"},{"doctype":"documentation","id":"references/FluxTraining.SanityCheckException","title":"SanityCheckException","text":""},{"doctype":"documentation","id":"references/FluxTraining.UnsafeCallback","title":"UnsafeCallback","text":""},{"doctype":"documentation","id":"references/FluxTraining.CancelEpochException","title":"CancelEpochException","text":"learner phase _ batch batches learner phase batch learner step loss throw Throw during fitting to cancel the currently running epoch This prematurely ends the current epoch without throwing an error Must be thrown inside the context of  runepoch  Examples"},{"doctype":"documentation","id":"references/FluxTraining.Permission","title":"Permission","text":""},{"doctype":"documentation","id":"references/FluxTraining.runstep","title":"runstep","text":"Run  stepfn  inside the context of a step Calls  stepfn(handle state  where  handle(e  can be called to dispatch events and  state  is a  PropDict  which step data gradients and losses can be written to Return  state  Takes care of dispatching  StepBegin  and  StepEnd  events as well as handling  CancelStepException s"},{"doctype":"documentation","id":"references/FluxTraining.removecallback!","title":"removecallback!","text":"Remove the first callback of type  C  from  learner  and return it If there is none return  nothing "},{"doctype":"documentation","id":"references/FluxTraining.CallbackRunner","title":"CallbackRunner","text":""},{"doctype":"documentation","id":"references/FluxTraining.addcallback!","title":"addcallback!","text":"Adds  callback  to  learner  and updates the dependency graph"},{"doctype":"documentation","id":"references/FluxTraining._gradient","title":"_gradient","text":""},{"doctype":"documentation","id":"references/FluxTraining.CheckDataIteratorTrain","title":"CheckDataIteratorTrain","text":""},{"doctype":"documentation","id":"references/FluxTraining.reset!","title":"reset!","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.BackwardBegin","title":"BackwardBegin","text":"Event  called between calculating loss and calculating gradients"},{"doctype":"documentation","id":"references/FluxTraining.Loss","title":"Loss","text":""},{"doctype":"documentation","id":"references/FluxTraining.runepoch","title":"runepoch","text":"Run  epochfn  inside the context of an epoch Calls  epochfn(handle  where  handle(e  can be called to dispatch events Takes care of dispatching  EpochBegin  and  EpochEnd  events as well as handling  CancelEpochException s"},{"doctype":"document","id":"documents/docs/features.md","title":"Features","text":"cb CoolFeature🕶️Callback learner model lossfn callbacks cb data trainiter validiter learner nepochs Features This page gives a run-down of many features  FluxTraining.jl  brings to the table  Most features are implemented as callbacks and using them is as simple as passing the callback when constructing a  Learner  and training with  fit  Metrics By default  Learner  will track only the loss function You can track other metric with the  Metrics  callback See also  Metric   AbstractMetric  Hyperparameter scheduling The  Scheduler  callback takes care of hyperparameter scheduling See the  Hyperparameter scheduling tutorial  and also  Scheduler   Schedule   HyperParameter  Logging For logging use the logging callbacks LogMetrics LogHyperParams LogHistograms They each can have multiple logging backends but right now the only one implemented in  FluxTraining.jl  is  TensorBoardBackend  See also  LoggerBackend   log_to  and  Loggables.Loggable  There is also an external package  Wandb.jl  that implements a logging backend for  Weights&Biases  Checkpointing Use the  Checkpointer  callback to create model checkpoints after every epoch Early Stopping Use  EarlyStopping  to stop when a stopping criterion is met Supports all criteria in  EarlyStopping.jl "},{"doctype":"documentation","id":"references/FluxTraining.LogHistograms","title":"LogHistograms","text":"Callback that logs histograms of model weights to  LoggerBackend s  backends  every  freq  steps If histograms should be logged every step pass  freq  nothing"},{"doctype":"documentation","id":"references/FluxTraining.shouldrun","title":"shouldrun","text":""},{"doctype":"documentation","id":"references/FluxTraining.init!","title":"init!","text":"Initialize a callback Default is to do nothing Extending To extend for a callback implement  init!(cb::MyCallback learner   init  can set up internal state of a callback that depends on  learner  and can also initialize shared callback state in  learner.cbstate  Just like  on  event handlers the state access permissions must be correctly defined using  stateaccess  to do so init  must also be idempotent i.e running it twice on the same  Learner  should have the same effect as runnning it once"},{"doctype":"documentation","id":"references/FluxTraining.##InlineTest-01b48f5c342f65df7fcd07f28f0d2cacbb09f0a0.__init__","title":"__init__","text":""},{"doctype":"documentation","id":"references/FluxTraining.setfieldperm!","title":"setfieldperm!","text":""},{"doctype":"documentation","id":"references/FluxTraining.getfieldperm","title":"getfieldperm","text":""},{"doctype":"documentation","id":"references/FluxTraining.CHECKS","title":"CHECKS","text":""},{"doctype":"document","id":"documents/docs/reference/training.md","title":"Training loop API reference","text":"Training loop API reference The training loop API centers around the abstract type  Phase  and the function  step  To  implement a custom training  you need to Usage fit for  n  epochs of supervised training and validation using  fit learner n train for an epoch using  epoch learner phase dataiter Extending subtype  Phase implement  step You can optionally overwrite default  epoch  implementation implement  phasedataiter  to define which data iterator should be used when  epoch  is called without one create custom  Callback  and  Event s with event handlers that dispatch on your  Phase  subtype Control flow Inside callback handlers and  step  implementations you can throw  CancelFittingException  to stop the training and  CancelEpochException  and  CancelStepException  to skip the current epoch or step"},{"doctype":"documentation","id":"references/FluxTraining.Loggables.File","title":"File","text":""},{"doctype":"documentation","id":"references/FluxTraining.Metrics","title":"Metrics","text":"cb cb Flux mse device gpu Flux mae device gpu Callback that tracks metrics during training You can pass any number of  metrics  with every argument being an  AbstractMetric  like  Metric  or a function  f(ŷs ys  val This callback is added by default to every  Learner  unless you pass in  usedefaultcallbacks  false  A metric tracking  learner.lossfn   Loss  is included by default The computed metrics can be access in  learner.cbstate.metricsstep  and  learner.cbstate.metricsepoch  for steps and epochs respectively Examples Track  accuracy  Pass in  Metric s"},{"doctype":"documentation","id":"references/FluxTraining.log_parameters","title":"log_parameters","text":""},{"doctype":"documentation","id":"references/FluxTraining.model!","title":"model!","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.EpochBegin","title":"EpochBegin","text":"Event  called at the beginning of an epoch"},{"doctype":"documentation","id":"references/FluxTraining.CallbackCondition","title":"CallbackCondition","text":"Supertype for conditions to use with  ConditionalCallback  To implement a  CallbackCondition  implement  shouldrun MyCondition event phase  See  FrequencyThrottle   TimeThrottle  and  throttle "},{"doctype":"documentation","id":"references/FluxTraining.onecycle","title":"onecycle","text":"Creates a one-cycle  Schedule  over  nsteps  steps from  start_val  over  max_val  to  end_val  Examples"},{"doctype":"documentation","id":"references/FluxTraining.loadmodel","title":"loadmodel","text":"Loads a model that was saved to  path  using  FluxTraining savemodel "},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Loggable","title":"Loggable","text":"Abstract type for data that  LoggerBackend s can log See  subtypes(FluxTraining.Loggables.Loggable  and  LoggerBackend"},{"doctype":"documentation","id":"references/FluxTraining.Read","title":"Read","text":""},{"doctype":"documentation","id":"references/FluxTraining.step!","title":"step!","text":"Run one step of training for  learner  on batch Behavior is customized through  phase  Extending This is a required method for custom  Phase s to implement To implement  step  it is recommended you make use of  runstep  to get begin and end events as well as proper handling of  CancelStepException s See the implementations of  TrainingPhase  and  ValidationPhase  for reference"},{"doctype":"documentation","id":"references/FluxTraining.ProgressPrinter","title":"ProgressPrinter","text":"Prints a progress bar of the currently running epoch"},{"doctype":"documentation","id":"references/FluxTraining.getcallback","title":"getcallback","text":"Find callback of type  C  in  learner s callbacks and return it If there is none return  nothing "},{"doctype":"documentation","id":"references/FluxTraining.handle","title":"handle","text":""},{"doctype":"document","id":"documents/docs/tutorials/training.md","title":"Training loop","text":"model batch params optimizer lossfn xs ys batch grads gradient params ŷs model xs loss lossfn ŷs ys loss update! optimizer params grads learner phase MyTrainingPhase batch xs ys batch learner phase xs xs ys ys state state grads gradient learner params state ŷs learner model state xs state loss learner lossfn state ŷs state ys loss update! learner optimizer learner params grads learner phase MyTrainingPhase batch xs ys batch learner phase xs xs ys ys state state grads gradient learner params state ŷs learner model state xs state loss learner lossfn state ŷs state ys loss update! learner optimizer learner params grads i learner MyTrainingPhase dataiter learner phase batch xs ys batch learner phase xs xs ys ys _ state state ŷs learner model state xs state loss learner lossfn state ŷs state ys learner phase dataiter learner phase batch dataiter learner phase batch phase MyTrainingPhase withepoch learner phase batch dataiter learner phase batch learner step loss throw Training loop FluxTraining.jl comes with a training loop for standard supervised learning problems but for different tasks like self-supervised learning being able to write custom training logic is essential The package's training loop API requires little boilerplate to convert a regular Flux.jl training loop while making it compatible with existing callbacks Supervised training step-by-step We'll explore the API step-by-step by converting a basic training loop and then discuss ways in which more complex training loops can be implemented using the same approach The central piece of a training loop is the logic for a single training step and in many cases that will be all you need to implement Below is the definition of a basic vanilla Flux.jl training step It takes a batch of data calculates the loss gradients and finally updates the parameters of the model To make a training step work with FluxTraining.jl and its callbacks we need to store data for a step so that callbacks can access it e.g  Metrics  uses  ys  and  ŷs  to evaluate metrics for each step and dispatch events so the callbacks are triggered We first need to create a  Phase  and implement a method for  FluxTraining.step  that dispatches on the phase type  Phase s are used to define different training behaviors using the same API and to define callback functionality that is only run during certain phases For example  Scheduler  only runs during  AbstractTrainingPhase s but not during  ValidationPhase  Let's implement such a phase and method moving the arguments inside a  Learner  in the process Now we can already train a model using this implementation for example using  epoch learner MyTrainingPhase dataiter  However no callbacks would be called since we haven't yet put in any logic that dispatches events or stores the step state We can do both by using the helper function  runstep  which takes care of runnning our step logic dispatching a  StepBegin  and  StepEnd  event before and after and handling control flow exceptions like  CancelStepException  Additionally  runstep  gives us a function  handle  which we can use to dispatch events inside the step and  state  a container for storing step state Let's use  runstep  and store the variables of interest inside  state  Now callbacks like  Metrics  can access variables like  ys  through  learner.step  which is set to the last  state  Finally we can use  handle  to dispatch additional events The result is the full implementation of FluxTraining.jl's own  TrainingPhase  Now we can use  epoch  to train a  Learner  with full support for  all callbacks  Validation The implementation of  ValidationPhase  is even simpler it runs the forward pass and stores variables so that callbacks like  Metrics  can access them Epoch logic We didn't need to implement a custom  epoch  method for our phase since the default is fine here it just iterates over every batch and calls  step  In fact let's have a look at how  epoch  is implemented Here  runepoch  similarly to  runstep  takes care of epoch start/stop events and control flow If you want more control over your training loop you can use it to write training loops that directly use  step  Tips Here are some additional tips for making it easier to implement complicated training loops You can pass named tuples of models to the  Learner  constructor For example for generative adversarial training you can pass in  generator   critic    and then refer to them inside the  step  implementation e.g using  learner.model.generator  The models parameters will have the same structure i.e  learner.params.generator  corresponds to  params(learner.model.generator  You can store any data you want in  state  When defining a custom phase instead of subtyping  Phase  you can subtype  AbstractTrainingPhase  or  AbstractValidationPhase  so that some context-specific callbacks will work out of the box with your phase type For example  Scheduler  sets hyperparameter values only during  AbstractTrainingPhase "},{"doctype":"documentation","id":"references/FluxTraining._resolveconflict","title":"_resolveconflict","text":""},{"doctype":"documentation","id":"references/FluxTraining.SafeCallback","title":"SafeCallback","text":""},{"doctype":"document","id":"documents/docs/callbacks/custom.md","title":"Custom callbacks","text":"Printer event phase printer Printer learner println event phase printer Printer learner println learner step loss Printer step loss Printer step loss cbstate history C1 C2 NotImplemented C1 C2 C1 C2 cb1 C1 cb2 C2 cb1 Custom callbacks FluxTraining.jl s callback system is built around multiple dispatch so you specify which part of the training you want to hook into by dispatching on  Phase s and  Event s See  Training loop  and  Events  as a reference to phases and events A guided example There are 4 things you need to do to implement a custom callback Create a callback  struct  that subtypes  Callback Write event handlers with  on Define what state the callback accesses by implementing  stateaccess Optionally define dependencies on other callbacks with  runafter Let's go through them one at a time by implementing a simple callback that prints something after every batch Callback  struct A callback definition has to subtype the abstract  Callback  type It can include fields to use as internal state but we don't need that here Event handlers Now we need to add an event handler so that  Printer  can run some code when a step ends Event handlers can be defined by adding a method to  FluxTraining.on  It takes as arguments an  Event  a  Phase  the callback and the learner on(event::Event phase::Phase callback::Callback learner The  event   phase  and  callback  are used to dispatch In this case we want to run code at the end of a step so the event we need to dispatch on is  StepEnd  We want it to run in any phase so we use the abstract type  Phase  The third argument type is the callback we want to add an event handler to This gives us We can now pass an instance of  Printer  when creating a  Learner  and the message will be printed at the end of every step State As seen above the callback handler  on  receives as the last argument a  Learner  instance allowing the callback to access and modify state If we wanted to print the last step's loss instead of a generic message we could update our definition of  on  see  Learner  for in-depth documentation of the  Learner s state The ability to modify any state is very powerful but it can quickly become problematic when it is unclear which callbacks modify what state and what the correct order should be Because of that  FluxTraining.jl  prevents callbacks from reading and modifying state by default If we tried to use the above redefinition of  on  we would get the following error To fix that error we need to implement  stateaccess  a function that specifies what state a callback is allowed to read and write In our case we want to read the loss of the current step see  stateaccess  for more information on how to implement it After that definition the above code will run fine This might seem bothersome but this extra information makes it possible to analyze state dependencies before any code is run and saves you from running into nasty hard-to-find bugs that can occur when using many callbacks together Dependencies Let's improve our callback a bit by adding the current step number to the printed message so it will look like this  Step 14 loss 0.0032  For that we need to know what the current number of steps is One way to go about this is to add a field to  Printer  that starts at  0  and is incremented every step Luckily there already is a callback that tracks this kind of statistics the  Recorder  It uses a special piece of state  learner.cbstate  to store a  History  with this information Callback state learner.cbstate  is an object where callbacks can store state that they want to make available to other callbacks Like any other piece of state the callback writing to it needs to add a  Write  permission to it using  stateaccess  What makes  cbstate  special is that when creating the callback graph it is checked that every entry in  cbstate  that is accessed is being created first The update to the event handler looks like this We also need to update the definition of  stateaccess  now Since  Printer  depends on  Recorder  now an error will be thrown if you try to use  Printer  without  Recorder  And that's it pass  Printer  to a  Learner  and test it out The upside of jumping through some additional hoops is that using the callback in the wrong context will always result in an error so the user can have peace of mind Conflict resolution When creating a  Learner  a dependency graph is created The graph is then analyzed to find possible conflicts for example when two callbacks update the same state Conflicts are detected automatically and will result in an error Conflicts happen when the same state is being modified by multiple callbacks and it is unclear which order of running them if any is valid Resolving conflicts There are two methods for resolving conflicts  runafter  and  resolveconflict   runafter  allows you to define list of callbacks that should run before the callback For example  Recorder  needs to run after all metrics resolveconflict  provides more granular control to handle a possible conflict between two callbacks It takes two callbacks and defines how to resolve a conflict Callback execution By default a topological ordering of the callbacks is created from the dependency graph and the callbacks are executed serially This behavior can be overwritten with custom callback executors for example to create a  Dagger.jl  node from the graph to allow callbacks to safely run in parallel where valid"},{"doctype":"documentation","id":"references/FluxTraining.CheckDataIteratorValid","title":"CheckDataIteratorValid","text":""},{"doctype":"documentation","id":"references/FluxTraining.errorwriteconflict","title":"errorwriteconflict","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Value","title":"Value","text":""},{"doctype":"documentation","id":"references/FluxTraining.TestModel","title":"TestModel","text":""},{"doctype":"documentation","id":"references/FluxTraining.LoggerBackend","title":"LoggerBackend","text":"Backend for logging callbacks like To add support for logging  Loggables.Loggable   L  to backend  B  implement log_to backend::B loggable::L names i See also  LogMetrics   LogHyperParams   log_to"},{"doctype":"document","id":"documents/docs/background/dataiterator.md","title":"Data iterators","text":"batch dataiter Data iterators A data iterator is an iterator over batches of the data that is used for one step of fitting You can use different data iterators with this package as long as they have the following properties Firstly you must be able to iterate over a data iterator The data iterator must also be compatible with the other components of the  Learner  For the standard supervised learning step  TrainingPhase  and  ValidationPhase  this means batch  is a tuple  xs ys  of encoded inputs and targets xs  is a valid input to the  model  so  ŷs  model(xs  and ys  can be compared to the model output with the  loss function  i.e  lossfn(ŷs ys If you are working with a  custom training loop  you may need to satisfy additional or different properties Creating data iterators The simplest data iterator is a vector of preloaded batches This is what we're using in the  MNIST tutorial  This is a fine approach for smaller datasets but has some limitations First of all there is no principled way for doing things like splitting subsetting and shuffling data For this we recommend using  MLDataPattern.jl  which provides this functionality and many more utilities for defining and working with datasets Another issue is that of memory load if the whole dataset is too large to be preloaded in to memory we have to load individual batches during training To do this in a way that doesn't slow down the training itself we suggest using  DataLoaders.jl  DataLoaders.jl is compatible with MLDataPattern.jl and allows you to easily create efficient data iterators for out-of-memory datasets The documentation of DataLoaders.jl also has a lot more information on working with large dataset for deep learning"}]