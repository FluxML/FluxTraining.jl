{"attributes":{"backlinks":[],"path":"/home/runner/.julia/packages/FluxTraining/cI3Oq/docs/custom_training.ipynb","title":"Custom training loops"},"tag":"document","children":[{"attributes":{},"tag":"h1","children":[{"mimes":{"text/plain":"Custom training loops"}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"How can we compose changes to the training loop together?"}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"As an example, we want to be able to do data"}},{"mimes":{"text/plain":"-"}},{"mimes":{"text/plain":"parallel training or GAN training, but also data"}},{"mimes":{"text/plain":"-"}},{"mimes":{"text/plain":"parallel GAN training"}},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"I want to give some thoughts on this and propose possible solutions to make this possible in "}},{"attributes":{},"tag":"em","children":[{"mimes":{"text/plain":"FastAI"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":"jl"}}]},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"Currently, implementing custom training behavior is possible by subtyping "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"FitPhase"}}]},{"mimes":{"text/plain":" and implementing "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"fitbatchphase!(learner, phase::MyPhase)"}}]},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" However, this is not composable, i"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":"e"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" it doesn"}},{"mimes":{"text/plain":"’"}},{"mimes":{"text/plain":"t allow you to combine a "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"DataParallelTrainingPhase"}}]},{"mimes":{"text/plain":" and "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"GANTrainingPhase"}}]},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"Below are some examples that show how the contents of "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"fitbatchphase!"}}]},{"mimes":{"text/plain":" can be changed to illustrate this"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" For simplicity, they don"}},{"mimes":{"text/plain":"’"}},{"mimes":{"text/plain":"t include callbacks and state handling"}},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"h4","children":[{"mimes":{"text/plain":"Regular training (1)"}}]},{"attributes":{"lang":"julia"},"tag":"pre","children":[{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"# inputs: model, xs, ys, lossfn, optim, params\ngrads = gradient(params) do\n    return lossfn(model(xs), ys)\nend\nupdate!(optim, params, sum(grads))"}}]}]},{"attributes":{},"tag":"h4","children":[{"mimes":{"text/plain":"Data"}},{"mimes":{"text/plain":"-"}},{"mimes":{"text/plain":"parallel training on CPU (2)"}}]},{"attributes":{"lang":"julia"},"tag":"pre","children":[{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"# inputs: model, xs, ys, lossfn, optim, params\n\ngrads = Array{Grads}(undef, Threads.nthreads())\n\n# run equally-sized slices of the batch in parallel (naive pseudocode)\nThreads.@threads for (i, (xs_, ys_)) in enumerate(scatter((xs, ys), Threads.nthreads()))\n    grads[i] = gradient(params) do\n        return lossfn(model(xs_), ys_)\n    end\nend\ngs = sum(grads)\n\n# do parameter update with summed gradients\nupdate!(optim, params, gs)"}}]}]},{"attributes":{},"tag":"h4","children":[{"mimes":{"text/plain":"GAN training (3)"}}]},{"attributes":{"lang":"julia"},"tag":"pre","children":[{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"# inputs: mgen, mcrit, paramsgen, paramscrit, xs_true, lossfngen, lossfncrit, optim, batchsize\n\n# critic step\nxs_fake = mgen(batchsize)\nxs = cat(xs_true, xs_fake)\nys = onehot.(vcat(trues(batchsize), falses(batchsize)))\n\ngrads = gradient(paramscrit) do\n    return lossfncrit(mcrit(xs), ys)\nend\nupdate!(optim, paramscrit, grads)\n\n# generator step\ngrads = gradient(paramsgen) do\n    xs_fake = mgen(batchsize)\n    ys_fake = onehot.(falses(batchsize))\n    return -lossfncrit(crit(xs_fake), ys_fake)\nend"}}]}]},{"attributes":{},"tag":"h3","children":[{"mimes":{"text/plain":"Solutions"}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"I have found two approaches to deal with this"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" "}},{"attributes":{},"tag":"strong","children":[{"mimes":{"text/plain":"Both focus on removing execution logic from "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"fitbatchphase!"}}]},{"mimes":{"text/plain":", making them composable with custom "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"Phase"}}]},{"mimes":{"text/plain":"s"}}]},{"mimes":{"text/plain":" like "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"GANTrainingPhase"}}]},{"mimes":{"text/plain":" that change"}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"the "}},{"attributes":{},"tag":"em","children":[{"mimes":{"text/plain":"semantics"}}]},{"mimes":{"text/plain":" of the training loop"}},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"On one hand, there are extensions to the training loop that change the "}},{"attributes":{},"tag":"em","children":[{"mimes":{"text/plain":"execution"}}]},{"mimes":{"text/plain":" (e"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":"g"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" parallel and distributed CPU and GPU training), on the other hand you have those that change the "}},{"attributes":{},"tag":"em","children":[{"mimes":{"text/plain":"semantics"}}]},{"mimes":{"text/plain":" (e"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":"g"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" GAN training)"}},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"The proposed solutions make the assumption that different "}},{"attributes":{},"tag":"em","children":[{"mimes":{"text/plain":"semantics"}}]},{"mimes":{"text/plain":" don"}},{"mimes":{"text/plain":"’"}},{"mimes":{"text/plain":"t need to be composed, but should be composable with different execution contexts"}},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"h4","children":[{"mimes":{"text/plain":"(S1) Abstract gradient step (and possibly others) out"}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"Modifications to the execution of the training loop could be implemented by wrapping in an execution context"}},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"In the below example "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"gradientphase"}}]},{"mimes":{"text/plain":" could dispatch to the regular gradient calculation in (1) or the data"}},{"mimes":{"text/plain":"-"}},{"mimes":{"text/plain":"parallel approach (2) depending on "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"executionctx"}}]},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"This would mean that only "}},{"attributes":{},"tag":"em","children":[{"mimes":{"text/plain":"semantic"}}]},{"mimes":{"text/plain":" changes to the training loop would use overloading of "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"fitbatchphase!"}}]},{"mimes":{"text/plain":" with a custom "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"FitPhase"}}]},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" Changes to the "}},{"attributes":{},"tag":"em","children":[{"mimes":{"text/plain":"execution"}}]},{"mimes":{"text/plain":" work by dispatching on execution contexts, e"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":"g"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"gradientphase(::Linear, ...)"}}]},{"mimes":{"text/plain":" or "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"gradientphase(::DataParallel, ...)"}}]},{"mimes":{"text/plain":"."}}]},{"attributes":{"lang":"julia"},"tag":"pre","children":[{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"# gradientphase passes (possibly modified) state to a closure\n# in the case executionctx::DataParallel, xs_, ys_ will be slices\n# of the batch.\ngrads = gradient(executionctx, params) do model_, params_, xs_, ys_, \n    return lossfn(model(xs_), ys_)\nend\nupdate!(optim, params, grads)"}}]}]},{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"strong","children":[{"mimes":{"text/plain":"Advantages"}}]}]},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"implementation definitely doable"}}]}]}]},{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"strong","children":[{"mimes":{"text/plain":"Disadvantages"}}]}]},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"implementation dependent on requirements, i"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":"e"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" unsure which pieces of the training step need to be overloadable and which state needs to be passed to closures"}},{"mimes":{"text/plain":"."}}]}]}]},{"attributes":{},"tag":"h4","children":[{"mimes":{"text/plain":"(S2) Wrapper for "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"model"}}]}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"The idea is to wrap the "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"model"}}]},{"mimes":{"text/plain":" in an execution context, e"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":"g"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" "}},{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"DataParallel(model)"}}]},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" The wrapper is then responsible for exhibiting the correct behavior on the forward and backward pass"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" This is "}},{"attributes":{"href":"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html?highlight=parallel#torch.nn.DataParallel","title":""},"tag":"a","children":[{"mimes":{"text/plain":"what PyTorch does"}}]},{"mimes":{"text/plain":"."}}]},{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"No changes to the training loop would need to be made"}},{"mimes":{"text/plain":"."}},{"mimes":{"text/plain":" The implementation for the forward pass should be straightforward and similar to the above sketch (2), however I"}},{"mimes":{"text/plain":"’"}},{"mimes":{"text/plain":"m not sure how to make sure that the backward pass is also computed in parallel (can custom gradient definitions include multi"}},{"mimes":{"text/plain":"-"}},{"mimes":{"text/plain":"threading code?, what about the loss function that is not wrapped?)"}},{"mimes":{"text/plain":"."}}]},{"attributes":{"lang":"julia"},"tag":"pre","children":[{"attributes":{},"tag":"code","children":[{"mimes":{"text/plain":"# before training\nmodel = DataParallel(model)\n\n# training step doesn't change at all\ngrads = gradient(params) do\n    return lossfn(model(xs), ys)\nend\nupdate!(optim, params, sum(grads))"}}]}]},{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"strong","children":[{"mimes":{"text/plain":"Advantages"}}]}]},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"no changes needed to state and event handling"}}]}]}]},{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"strong","children":[{"mimes":{"text/plain":"Disadvantages"}}]}]},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"not sure if such a simple API is possible to implement for all scenarios"}}]}]},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"mimes":{"text/plain":"bit unelegant; model is not a pure function anymore"}}]}]}]}]}