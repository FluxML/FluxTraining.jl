<HTML><head><title>Custom training loops</title><link href=../template/hugobook.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >FluxTraining.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p><a href=../README.md.html title= >README</a></p></li><li><p><a href=getting_started.md.html title= >Getting started</a></p></li><li><p><a href=features.md.html title= >Features</a></p></li><li><p>Tutorials</p><ul><li><p><a href=tutorials/mnist.ipynb.html title= >Training an image classifier</a></p></li><li><p><a href=tutorials/hyperparameters.md.html title= >Hyperparameter scheduling</a></p></li><li><p><a href=callbacks/custom.md.html title= >Writing a custom callback</a></p></li></ul></li><li><p>How to</p><ul><li><p><a href=callbacks/usage.md.html title= >Use callbacks</a></p></li><li><p><a href=callbacks/tipstricks.md.html title= >Tips &amp; tricks</a></p></li></ul></li><li><p>Reference</p><ul><li><p><a href=../REFERENCE.html title= >Docstrings</a></p></li><li><p><a href=training/basics.md.html title= >Training loop</a></p></li><li><p><a href=callbacks/reference.md.html title= >Callbacks</a></p></li></ul></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><h1 id=custom-training-loops >Custom training loops</h1><p>How can we compose changes to the training loop together?</p><p>As an example, we want to be able to do data-parallel training or GAN training, but also data-parallel GAN training.</p><p>I want to give some thoughts on this and propose possible solutions to make this possible in <em>FastAI.jl</em>.</p><p>Currently, implementing custom training behavior is possible by subtyping <code>FitPhase</code> and implementing <code>fitbatchphase!(learner, phase::MyPhase)</code>. However, this is not composable, i.e. it doesn’t allow you to combine a <code>DataParallelTrainingPhase</code> and <code>GANTrainingPhase</code>.</p><p>Below are some examples that show how the contents of <code>fitbatchphase!</code> can be changed to illustrate this. For simplicity, they don’t include callbacks and state handling.</p><h4 id=regular-training-1 >Regular training (1)</h4><pre lang=julia ><code># inputs: model, xs, ys, lossfn, optim, params
grads = gradient(params) do
    return lossfn(model(xs), ys)
end
update!(optim, params, sum(grads))</code></pre><h4 id=data-parallel-training-on-cpu-2 >Data-parallel training on CPU (2)</h4><pre lang=julia ><code># inputs: model, xs, ys, lossfn, optim, params

grads = Array{Grads}(undef, Threads.nthreads())

# run equally-sized slices of the batch in parallel (naive pseudocode)
Threads.@threads for (i, (xs_, ys_)) in enumerate(scatter((xs, ys), Threads.nthreads()))
    grads[i] = gradient(params) do
        return lossfn(model(xs_), ys_)
    end
end
gs = sum(grads)

# do parameter update with summed gradients
update!(optim, params, gs)</code></pre><h4 id=gan-training-3 >GAN training (3)</h4><pre lang=julia ><code># inputs: mgen, mcrit, paramsgen, paramscrit, xs_true, lossfngen, lossfncrit, optim, batchsize

# critic step
xs_fake = mgen(batchsize)
xs = cat(xs_true, xs_fake)
ys = onehot.(vcat(trues(batchsize), falses(batchsize)))

grads = gradient(paramscrit) do
    return lossfncrit(mcrit(xs), ys)
end
update!(optim, paramscrit, grads)

# generator step
grads = gradient(paramsgen) do
    xs_fake = mgen(batchsize)
    ys_fake = onehot.(falses(batchsize))
    return -lossfncrit(crit(xs_fake), ys_fake)
end</code></pre><h3 id=solutions >Solutions</h3><p>I have found two approaches to deal with this. <strong>Both focus on removing execution logic from <code>fitbatchphase!</code>, making them composable with custom <code>Phase</code>s</strong> like <code>GANTrainingPhase</code> that change</p><p>the <em>semantics</em> of the training loop.</p><p>On one hand, there are extensions to the training loop that change the <em>execution</em> (e.g. parallel and distributed CPU and GPU training), on the other hand you have those that change the <em>semantics</em> (e.g. GAN training).</p><p>The proposed solutions make the assumption that different <em>semantics</em> don’t need to be composed, but should be composable with different execution contexts.</p><h4 id=s1-abstract-gradient-step-and-possibly-others-out >(S1) Abstract gradient step (and possibly others) out</h4><p>Modifications to the execution of the training loop could be implemented by wrapping in an execution context.</p><p>In the below example <code>gradientphase</code> could dispatch to the regular gradient calculation in (1) or the data-parallel approach (2) depending on <code>executionctx</code>.</p><p>This would mean that only <em>semantic</em> changes to the training loop would use overloading of <code>fitbatchphase!</code> with a custom <code>FitPhase</code>. Changes to the <em>execution</em> work by dispatching on execution contexts, e.g. <code>gradientphase(::Linear, ...)</code> or <code>gradientphase(::DataParallel, ...)</code>.</p><pre lang=julia ><code># gradientphase passes (possibly modified) state to a closure
# in the case executionctx::DataParallel, xs_, ys_ will be slices
# of the batch.
grads = gradient(executionctx, params) do model_, params_, xs_, ys_, 
    return lossfn(model(xs_), ys_)
end
update!(optim, params, grads)</code></pre><p><strong>Advantages</strong></p><ul><li><p>implementation definitely doable</p></li></ul><p><strong>Disadvantages</strong></p><ul><li><p>implementation dependent on requirements, i.e. unsure which pieces of the training step need to be overloadable and which state needs to be passed to closures.</p></li></ul><h4 id=s2-wrapper-for-model >(S2) Wrapper for <code>model</code></h4><p>The idea is to wrap the <code>model</code> in an execution context, e.g. <code>DataParallel(model)</code>. The wrapper is then responsible for exhibiting the correct behavior on the forward and backward pass. This is <a href=https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html?highlight=parallel#torch.nn.DataParallel title= >what PyTorch does</a>.</p><p>No changes to the training loop would need to be made. The implementation for the forward pass should be straightforward and similar to the above sketch (2), however I’m not sure how to make sure that the backward pass is also computed in parallel (can custom gradient definitions include multi-threading code?, what about the loss function that is not wrapped?).</p><pre lang=julia ><code># before training
model = DataParallel(model)

# training step doesn't change at all
grads = gradient(params) do
    return lossfn(model(xs), ys)
end
update!(optim, params, sum(grads))</code></pre><p><strong>Advantages</strong></p><ul><li><p>no changes needed to state and event handling</p></li></ul><p><strong>Disadvantages</strong></p><ul><li><p>not sure if such a simple API is possible to implement for all scenarios</p></li><li><p>bit unelegant; model is not a pure function anymore</p></li></ul></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#custom-training-loops >Custom training loops</a><ul></ul></li></ul></nav></aside></main></body></HTML>