<HTML><head><title>Training loop</title><script src=../../template/highlight.min.js ></script><script src=../../template/julia.min.js ></script><script src=../../template/loadhighlightjs.js ></script><link href=../../template/ansi.css rel=stylesheet ></link><link href=../../template/hugobook.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta><meta name=viewport content=width=device-width, initial-scale=1 ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >FluxTraining.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p><a href=../../README.md.html title= >README</a></p></li><li><p><a href=../getting_started.md.html title= >Getting started</a></p></li><li><p><a href=../features.md.html title= >Features</a></p></li><li><p>Tutorials</p><ul><li><p><a href=mnist.ipynb.html title= >Training an image classifier</a></p></li><li><p><a href=hyperparameters.md.html title= >Hyperparameter scheduling</a></p></li><li><p><a href=../callbacks/custom.md.html title= >Custom callbacks</a></p></li><li><p><a href=training.md.html title= >Custom training loops</a></p></li></ul></li><li><p>How to</p><ul><li><p><a href=../callbacks/usage.md.html title= >Use callbacks</a></p></li><li><p><a href=../callbacks/tipstricks.md.html title= >Tips &amp; tricks</a></p></li></ul></li><li><p>Reference</p><ul><li><p><a href=../../REFERENCE.html title= >Docstrings</a></p></li><li><p><a href=../callbacks/reference.md.html title= >Callbacks</a></p></li><li><p><a href=../../CHANGELOG.md.html title= >Changelog</a></p></li></ul></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><h1 id=training-loop >Training loop</h1><p>FluxTraining.jl comes with a training loop for standard supervised learning problems, but for different tasks like self-supervised learning, being able to write custom training logic is essential. The package’s training loop API requires little boilerplate to convert a regular Flux.jl training loop while making it compatible with existing callbacks.</p><h2 id=supervised-training-step-by-step >Supervised training, step-by-step</h2><p>We’ll explore the API step-by-step by converting a basic training loop and then discuss ways in which more complex training loops can be implemented using the same approach. The central piece of a training loop is the logic for a single training step, and in many cases, that will be all you need to implement. Below is the definition of a basic vanilla Flux.jl training step. It takes a batch of data, calculates the loss, gradients and finally updates the parameters of the model.</p><pre lang=julia ><code>function step!(model, batch, params, optimizer, lossfn)
    xs, ys = batch
    grads = gradient(params) do
        ŷs = model(xs)
        loss = lossfn(ŷs, ys)
        return loss
    end
    update!(optimizer, params, grads)
end
</code></pre><p>To make a training step work with FluxTraining.jl and its callbacks, we need to</p><ul><li><p>store data for a step so that callbacks can access it (e.g. <a href=../../REFERENCE/FluxTraining.Metrics.html ><code>Metrics</code></a> uses <code>ys</code> and <code>ŷs</code> to evaluate metrics for each step); and</p></li><li><p>dispatch events so the callbacks are triggered</p></li></ul><p>We first need to create a <a href=../../REFERENCE/FluxTraining.Phases.Phases.Phase.html ><code>FluxTraining.Phases.Phase</code></a> and implement a method for <a href=../../REFERENCE/FluxTraining.step!.html ><code>step!</code></a> that dispatches on the phase type. <code>Phase</code>s are used to define different training behaviors using the same API and to define callback functionality that is only run during certain phases. For example, <a href=../../REFERENCE/FluxTraining.Scheduler.html ><code>Scheduler</code></a> only runs during <a href=../../REFERENCE/FluxTraining.Phases.Phases.AbstractTrainingPhase.html ><code>FluxTraining.Phases.AbstractTrainingPhase</code></a>s but not during <a href=../../REFERENCE/FluxTraining.Phases.Phases.ValidationPhase.html ><code>FluxTraining.Phases.ValidationPhase</code></a>. Let’s implement such a phase and method, moving the arguments inside a <a href=../../REFERENCE/FluxTraining.Learner.html ><code>Learner</code></a> in the process.</p><pre lang=julia ><code>struct MyTrainingPhase &lt;: FluxTraining.AbstractTrainingPhase

function FluxTraining.step!(learner, phase::MyTrainingPhase, batch)
    xs, ys = batch
    grads = gradient(learner.params) do
        ŷs = learner.model(xs)
        loss = learner.lossfn(ŷs, ys)
        return loss
    end
    update!(learner.optimizer, learner.params, grads)
end
</code></pre><p>Now we can already train a model using this implementation, for example using <a href=../../REFERENCE/FluxTraining.epoch!.html ><code>epoch!</code></a><code>(learner, MyTrainingPhase(), dataiter)</code>. However, no callbacks would be called, since we haven’t yet put in any logic that dispatches events or stores the step state. We can do both by using the helper function <a href=../../REFERENCE/FluxTraining.runstep.html ><code>FluxTraining.runstep</code></a> which takes care of runnning our step logic, dispatching a <a href=../../REFERENCE/FluxTraining.Events.Events.StepBegin.html ><code>FluxTraining.Events.StepBegin</code></a> and <a href=../../REFERENCE/FluxTraining.Events.Events.StepEnd.html ><code>FluxTraining.Events.StepEnd</code></a> event before and after and handling control flow exceptions like <a href=../../REFERENCE/FluxTraining.CancelStepException.html ><code>CancelStepException</code></a>. Additionally, <code>runstep</code> gives us a function <code>handle</code> which we can use to dispatch events inside the step, and <code>state</code> a container for storing step state. Let’s use <code>runstep</code> and store the variables of interest inside <code>state</code>:</p><pre lang=julia ><code>function step!(learner, phase::MyTrainingPhase, batch)
    xs, ys = batch
    runstep(learner, phase, (xs=xs, ys=ys)) do handle, state
        state.grads = gradient(learner.params) do
            state.ŷs = learner.model(state.xs)
            state.loss = learner.lossfn(state.ŷs, state.ys)
            return loss
        end
        update!(learner.optimizer, learner.params, grads)
    end
end
</code></pre><p>Now callbacks like <a href=../../REFERENCE/FluxTraining.Metrics.html ><code>Metrics</code></a> can access variables like <code>ys</code> through <code>learner.step</code> (which is set to the last <code>state</code>). Finally, we can use <code>handle</code> to dispatch additional events:</p><pre lang=julia ><code>using FluxTraining.Events: LossBegin, BackwardBegin, BackwardEnd

function step!(learner, phase::MyTrainingPhase, batch)
    xs, ys = batch
    runstep(learner, phase, (xs=xs, ys=ys)) do handle, state
        state.grads = gradient(learner.params) do
            state.ŷs = learner.model(state.xs)
            handle(LossBegin())
            state.loss = learner.lossfn(state.ŷs, state.ys)
            handle(BackwardBegin())
            return loss
        end
        handle(BackwardEnd())
        update!(learner.optimizer, learner.params, grads)
    end
end
</code></pre><p>The result is the full implementation of FluxTraining.jl’s own <a href=../../REFERENCE/FluxTraining.Phases.Phases.TrainingPhase.html ><code>FluxTraining.Phases.TrainingPhase</code></a>! Now we can use <code>epoch!</code> to train a <code>Learner</code> with full support for <a href=../callbacks/reference.md.html title= >all callbacks</a>:</p><pre lang=julia ><code>for i in 1:10
    epoch!(learner, MyTrainingPhase(), dataiter)
end
</code></pre><h2 id=validation >Validation</h2><p>The implementation of <a href=../../REFERENCE/FluxTraining.Phases.Phases.ValidationPhase.html ><code>FluxTraining.Phases.ValidationPhase</code></a> is even simpler; it runs the forward pass and stores variables so that callbacks like <a href=../../REFERENCE/FluxTraining.Metrics.html ><code>Metrics</code></a> can access them.</p><pre lang=julia ><code>struct ValidationPhase &lt;: AbstractValidationPhase end

function step!(learner, phase::ValidationPhase, batch)
    xs, ys = batch
    runstep(learner, phase, (xs=xs, ys=ys)) do _, state
        state.ŷs = learner.model(state.xs)
        state.loss = learner.lossfn(state.ŷs, state.ys)
    end
end
</code></pre><h2 id=epoch-logic >Epoch logic</h2><p>We didn’t need to implement a custom <code>epoch!</code> method for our phase since the default is fine here: it just iterates over every batch and calls <code>step!</code>. In fact, let’s have a look at how <code>epoch!</code> is implemented:</p><pre lang=julia ><code>function epoch!(learner, phase::Phase, dataiter)
    runepoch(learner, phase) do handle
        for batch in dataiter
            step!(learner, phase, batch)
        end
    end
end
</code></pre><p>Here, <a href=../../REFERENCE/FluxTraining.runepoch.html ><code>FluxTraining.runepoch</code></a>, similarly to <code>runstep</code>, takes care of epoch start/stop events and control flow. If you want more control over your training loop, you can use it to write training loops that directly use <code>step!</code>:</p><pre lang=julia ><code>phase = MyTrainingPhase()
withepoch(learner, phase) do handle
    for batch in dataiter
        step!(learner, phase, batch)
        if learner.step.loss &lt; 0.1
            throw(CancelFittingException(&quot;Low loss reached.&quot;))
        end
    end
end
</code></pre><h2 id=tips >Tips</h2><p>Here are some additional tips for making it easier to implement complicated training loops.</p><ul><li><p>You can pass (named) tuples of models to the <code>Learner</code> constructor. For example, for generative adversarial training, you can pass in <code>(generator = ..., critic = ...)</code> and then refer to them inside the <code>step!</code> implementation, e.g. using <code>learner.model.generator</code>. The models’ parameters will have the same structure, i.e. <code>learner.params.generator</code> corresponds to <code>params(learner.model.generator)</code>.</p></li><li><p>You can store any data you want in <code>state</code>.</p></li><li><p>When defining a custom phase, instead of subtyping <code>Phase</code> you can subtype <a href=../../REFERENCE/FluxTraining.Phases.Phases.AbstractTrainingPhase.html ><code>FluxTraining.Phases.AbstractTrainingPhase</code></a> or <a href=../../REFERENCE/FluxTraining.Phases.Phases.AbstractValidationPhase.html ><code>FluxTraining.Phases.AbstractValidationPhase</code></a> so that some context-specific callbacks will work out of the box with your phase type. For example, <a href=../../REFERENCE/FluxTraining.Scheduler.html ><code>Scheduler</code></a> sets hyperparameter values only during <code>AbstractTrainingPhase</code>.</p></li></ul></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#training-loop >Training loop</a><ul><li><a href=#supervised-training-step-by-step >Supervised training, step-by-step</a><ul></ul></li><li><a href=#validation >Validation</a><ul></ul></li><li><a href=#epoch-logic >Epoch logic</a><ul></ul></li><li><a href=#tips >Tips</a><ul></ul></li></ul></li></ul></nav></aside></main></body></HTML>