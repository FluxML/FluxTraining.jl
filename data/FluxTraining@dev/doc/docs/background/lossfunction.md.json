{"attributes":{"backlinks":[{"tag":"document","title":"Getting started","docid":"FluxTraining@dev/doc/docs/getting_started.md"},{"tag":"document","title":"Data iterators","docid":"FluxTraining@dev/doc/docs/background/dataiterator.md"}],"path":"/home/runner/.julia/packages/FluxTraining/HjEA2/docs/background/lossfunction.md","title":"Loss functions"},"tag":"document","children":[{"attributes":{},"tag":"md","children":[{"attributes":{},"tag":"h1","children":["Loss functions"],"type":"node"},{"attributes":{},"tag":"p","children":["A loss function compares model outputs to true targets, resulting in a loss. For a loss function to be compatible with the standard supervised training loop, the following properties must hold."],"type":"node"},{"attributes":{},"tag":"p","children":["Firstly, the loss function should accept the model outputs and targets, and return a single scalar value. Given a ",{"attributes":{"href":"/doc/docs/background/dataiterator.md","title":"","document_id":"FluxTraining@dev/doc/docs/background/dataiterator.md"},"tag":"reference","children":["data iterator"],"type":"node"}," ",{"attributes":{},"tag":"code","children":["dataiter"],"type":"node"}," and a ",{"attributes":{"href":"/doc/docs/background/model.md","title":"","document_id":"FluxTraining@dev/doc/docs/background/model.md"},"tag":"reference","children":["model"],"type":"node"}," ",{"attributes":{},"tag":"code","children":["model"],"type":"node"},":"],"type":"node"},{"attributes":{"lang":"julia"},"tag":"codeblock","children":[{"attributes":{},"tag":"julia","children":[{"attributes":{},"tag":"=","children":[{"attributes":{},"tag":"tuple","children":[{"attributes":{},"tag":"Identifier","children":["xs"],"type":"node"},{"attributes":{},"tag":",","children":[","],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"Identifier","children":["ys"],"type":"node"}],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"=","children":["="],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"Identifier","children":["dataiter"],"type":"node"}],"type":"node"},{"attributes":{},"tag":"NewlineWs","children":["\n"],"type":"node"},{"attributes":{},"tag":"=","children":[{"attributes":{},"tag":"Identifier","children":["ŷs"],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"=","children":["="],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{},"tag":"Identifier","children":["model"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":"Identifier","children":["xs"],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"NewlineWs","children":["\n"],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{},"tag":"call","children":[{"attributes":{},"tag":"Identifier","children":["lossfn"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":"Identifier","children":["ŷs"],"type":"node"},{"attributes":{},"tag":",","children":[","],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"Identifier","children":["ys"],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"isa","children":["isa"],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"Identifier","children":["Number"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"p","children":["The loss function must also be differentiable, so that gradients can be calculated during training. See ",{"attributes":{"href":"/doc/docs/background/model.md","title":"","document_id":"FluxTraining@dev/doc/docs/background/model.md"},"tag":"reference","children":["models"],"type":"node"}," for more on how to check this."],"type":"node"},{"attributes":{},"tag":"h2","children":["Creating loss functions"],"type":"node"},{"attributes":{},"tag":"p","children":["Flux.jl comes with a lot of commonly used loss functions built-in in its submodule ",{"attributes":{},"tag":"code","children":["Flux.Losses"],"type":"node"},". See ",{"attributes":{"href":"https://fluxml.ai/Flux.jl/stable/models/losses/","title":""},"tag":"a","children":["Flux.jl loss functions"],"type":"node"}," for a complete reference."],"type":"node"},{"attributes":{},"tag":"p","children":["You can also write your own loss functions. If you are using non-mutating array operations, there is a good chance that it will be differentiable and also be compatible with GPU arrays from ",{"attributes":{"href":"https://github.com/JuliaGPU/CUDA.jl","title":""},"tag":"a","children":["CUDA.jl"],"type":"node"},"."],"type":"node"}],"type":"node"}],"type":"node"}