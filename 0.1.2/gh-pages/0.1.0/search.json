[{"body":"public   throttle   —   function Throttle  event  for  callback  so that it is triggered either only every freq ’ th time  or every  seconds  seconds .","id":"docstrings/FluxTraining.throttle.html"},{"body":"private   edgesrunafter   —   function Return a vector of  Edge s representing dependencies defined by  runafter .","id":"docstrings/FluxTraining.edgesrunafter.html"},{"body":"1 .  Callback  struct A callback definition has to subtype the abstract  Callback  type .  It can include fields to use as internal state, but we don ’ t need that here .","id":"docs/callbacks/custom.html#callback-struct"},{"body":"private   callbackgraph   —   function Creates a directed acyclic graph from a list of  callbacks . Ordering is given through  runafter  and  resolveconflict . If a write conflict cannot be resolved (i . e .   resolveconflict ) is not implemented), throws an error .","id":"docstrings/FluxTraining.callbackgraph.html"},{"body":"Resolving conflicts There are two methods for resolving conflicts,  runafter  and  resolveconflict . runafter  allows you to define list of callbacks that should run before the callback .  For example,  Recorder  needs to run after all metrics: resolveconflict  provides more granular control to handle a possible conflict between two callbacks .  It takes two callbacks and defines how to resolve a conflict:","id":"docs/callbacks/custom.html#resolving-conflicts"},{"body":"Interface If  Metric  doesn ’ t fit your use case, you can create a new subtype of  AbstractMetric  and implement the following methods to make it compatible with  Metrics : reset! (metric) step! (metric, learner) stepvalue (metric) epochvalue (metric) metricname (metric)","id":"docstrings/FluxTraining.AbstractMetric.html#interface"},{"body":"Basics Learner  holds all training state .  Use  fit!(learner, n)  to train it for  n  epochs .","id":"docs/features.html#basics"},{"body":"Project status The core interfaces of  FluxTraining . jl  are stable now and it comes with a good set of features to get started . However, there is still a lot of work to be done !  This page gives some ideas for new features and improvements that could be added to  FluxTraining . jl .  If you want to have something on this list, open a GitHub issue or, better yet, a merge request .","id":"docs/status.html#project-status"},{"body":"public   fit!   —   function Fit  learner  with  Phase s  phases .  See  [ ./docs/training/basics.md ]  for more info on the training loop . Shorthand for  fit!(learner, repeat([TrainingPhase(), ValidationPhase()], n)) , i . e . a very basic training loop of  n  epochs of training followed by validation .","id":"docstrings/FluxTraining.fit!.html"},{"body":"Hyperparameter scheduling The  Scheduler  callback takes care of hyperparameter scheduling .  See the  [ Hyperparameter scheduling tutorial ]  and also  Scheduler ,  Schedule ,  HyperParameter .","id":"docs/features.html#hyperparameter-scheduling"},{"body":"Improvements Checkpointer : Add conditions for checkpointing, e . g .  only save a checkpoint when validation loss is decreasing .","id":"docs/status.html#improvements"},{"body":"Callbacks Callbacks are a powerful feature of  FluxTraining . jl , allowing you to add functionality to the training loop at different points . Find out how to  use callbacks  when training what callbacks come  included with  FluxTraining . jl ; or how callbacks work and how to  implement your own","id":"docs/overview.html#callbacks"},{"body":"Overview You decide what you want to read next ! There are some user - centric tutorials that will introduce you to features of  FluxTraining . jl  one at a time Alternatively you can read more about how  FluxTraining . jl  is implemented and how you can extend it","id":"docs/overview.html#overview"},{"body":"Visualize the callback dependency graph You can use  GraphPlot . jl  to visualize the dependencies between callbacks: (the target of an arrow depends on the origin) As an example for a detected dependency, we can see that  Recorder  runs after  Loss .   Recorder  records the values of all metrics, so  Loss  which is a subtype of  AbstractMetric  needs to run first .","id":"docs/callbacks/tipstricks.html#visualize-the-callback-dependency-graph"},{"body":"public   BackwardEnd   —   struct Event  called between calculating gradients and updating parameters .","id":"docstrings/FluxTraining.Events.BackwardEnd.html"},{"body":"Examples metrics = Metrics(accuracy) metrics = Metrics(Metric(Flux.mse, device = gpu), Metric(Flux.mae, device = gpu))","id":"docstrings/FluxTraining.Metrics.html#examples"},{"body":"public   AbstractTrainingPhase   —   type An abstract type for phases where parameter updates are being made .  This exists so callbacks can dispatch on it and work with custom training phases . The default implementation is  TrainingPhase .","id":"docstrings/FluxTraining.Phases.AbstractTrainingPhase.html"},{"body":"public   Recorder   —   struct Maintains a  History .  It ’ s stored in  learner.cbstate.history .","id":"docstrings/FluxTraining.Recorder.html"},{"body":"Tutorials MNIST training Hyperparameter scheduling","id":"docs/overview.html#tutorials"},{"body":"private   resolveconflict   —   function Define a conflict resolution strategy for resolving a write/write conflict between two callbacks . The default is  [ NotDefined() ] , which will result in an error and a message to implement this method . To implement, dispatch on the callback types that you which to resolve (in any order) and return one of the following: Unresolvable ()  if the callbacks must not be used together RunFirst (cb)  if one of the callbacks needs to run first; or NoConflict ()  if the callbacks may run together in any order","id":"docstrings/FluxTraining.resolveconflict.html"},{"body":"Conflict resolution When creating a  Learner , a dependency graph is created .  The graph is then analyzed to find possible conflicts (for example, when two callbacks update the same state) . Conflicts are detected automatically and will result in an error .  Conflicts happen when the same state is being modified by multiple callbacks and it is unclear which order of running them (if any) is valid .","id":"docs/callbacks/custom.html#conflict-resolution"},{"body":"One - cycle learning rate Let ’ s define a  Schedule  that follows the above - mentioned cyclical learning rate schedule . The idea is to start with a small learning rate, gradually increase it, and then slowly decrease it again . For example, we could start with a learning rate of 0 . 01, increase it to 0 . 1 over 3 epochs, and then down to 0 . 001 over 7 epochs .  Let ’ s also use cosine annealing, a common practice that makes sure the values are interpolated more smoothly . In code, that looks like this: For convenience, you can also use the  onecycle  helper to create this  Schedule .","id":"docs/tutorials/hyperparameters.html#one-cycle-learning-rate"},{"body":"Name Module Visibility Category  AbstractMetric   FluxTraining   private   type   BatchState   FluxTraining   private   struct   Callback   FluxTraining   private   type   CancelBatchException   FluxTraining   public   struct   CancelFittingException   FluxTraining   public   struct   Checkpointer   FluxTraining   public   struct   ConflictResolution   FluxTraining   private   type   CustomCallback   FluxTraining   public   parametric type   EarlyStopping   FluxTraining   public   struct   BackwardBegin   FluxTraining.Events   public   struct   BackwardEnd   FluxTraining.Events   public   struct   BatchBegin   FluxTraining.Events   public   struct   BatchEnd   FluxTraining.Events   public   struct   EpochBegin   FluxTraining.Events   public   struct   EpochEnd   FluxTraining.Events   public   struct   Event   FluxTraining.Events   public   type   Init   FluxTraining.Events   public   struct   LossBegin   FluxTraining.Events   public   struct   Events   FluxTraining.Events   private   module   FitException   FluxTraining   private   type   GarbageCollect   FluxTraining   public   function   HyperParameter   FluxTraining   private   parametric type   Learner   FluxTraining   public   struct   LearningRate   FluxTraining   public   type   LogHistograms   FluxTraining   public   struct   LogHyperParams   FluxTraining   public   struct   LogMetrics   FluxTraining   public   struct   LogVisualization   FluxTraining   public   struct   Loggable   FluxTraining.Loggables   private   type   Loggables   FluxTraining.Loggables   private   module   LoggerBackend   FluxTraining   private   type   Metric   FluxTraining   public   parametric type   Metrics   FluxTraining   public   struct   MetricsPrinter   FluxTraining   public   struct   AbstractTrainingPhase   FluxTraining.Phases   public   type   Phase   FluxTraining.Phases   public   type   TrainingPhase   FluxTraining.Phases   public   struct   ValidationPhase   FluxTraining.Phases   public   struct   Phases   FluxTraining.Phases   private   module   ProgressPrinter   FluxTraining   public   struct   PropDict   FluxTraining   private   parametric type   Recorder   FluxTraining   public   struct   SanityCheck   FluxTraining   public   struct   Schedule   FluxTraining   public   parametric type   Scheduler   FluxTraining   public   struct   StopOnNaNLoss   FluxTraining   public   struct   TensorBoardBackend   FluxTraining   public   struct   ToGPU   FluxTraining   public   struct   accesses   FluxTraining   private   function   callbackgraph   FluxTraining   private   function   edgesrunafter   FluxTraining   private   function   fit!   FluxTraining   public   function   fitepochphase!   FluxTraining   private   function   iterpairs   FluxTraining   private   function   loadmodel   FluxTraining   public   function   log_to   FluxTraining   private   function   on   FluxTraining   private   function   onecycle   FluxTraining   public   function   resolveconflict   FluxTraining   private   function   sethyperparameter!   FluxTraining   private   function   stateaccess   FluxTraining   private   function   throttle   FluxTraining   public   function ","id":"docstrings.html#docstring-index"},{"body":"Arguments metricfn(ŷs, ys)  should return a number . statistic  is a  OnlineStats.Statistic  that is updated after every step . The default is  OnlineStats.Mean() name  is used for printing . device = cpu  is a function applied to  ys  and  ŷs  before calling metricfn .  If  metricfn  works on the GPU, you may want to pass Flux.gpu  here for better performance .","id":"docstrings/FluxTraining.Metric.html#arguments"},{"body":"Fields xs ys ŷs loss grads ( ! ) If used in callbacks, some fields may be  nothing  as they are reset after every step .","id":"docstrings/FluxTraining.BatchState.html#fields"},{"body":"private   PropDict   —   parametric type Like a  Dict{Symbol} , but attribute syntax can be used to access values .","id":"docstrings/FluxTraining.PropDict.html"},{"body":"Examples","id":"docstrings/FluxTraining.PropDict.html#examples"},{"body":"Logging and I/O Checkpointer ` Metrics * MetricsPrinter * ProgressPrinter * Recorder * SanityCheck *","id":"docs/callbacks/reference.html#logging-and-io"},{"body":"Overview There are 4 pieces that you always need to construct and train a  Learner : a model data an optimizer; and a loss function","id":"docs/tutorials/mnist.html#overview"},{"body":"private   on   —   function Handle  event  with  callback .  Can dispatch on an  Phase  and receives  learner  as an additional argument . If not overwritten with a more specific method, does nothing . To see events which an  AbstractCallback  handles, use","id":"docstrings/FluxTraining.on.html"},{"body":"public   LogHyperParams   —   struct Callback that logs hyperparameters to one or more  LoggerBackend s . See also  LoggerBackend ,  Loggables.Loggable ,  log_to , TensorBoardBackend","id":"docstrings/FluxTraining.LogHyperParams.html"},{"body":"Extending Training and validation are currently the only two  Phase s implemented, but the interface can be extended by implementing  fitepochphase!  and  fitbatchphase! .  An example use case is implementing GAN training .","id":"docs/training/basics.html#extending"},{"body":"4 .  Dependencies Let ’ s improve our callback a bit by adding the current step number to the printed message, so it will look like this:  \"Step 14 loss: 0.0032\" . For that we need to know what the current step number is .  One way to go about this is to add a field to  Printer  that starts at  0  and is incremented every batch . Luckily, there already is a callback that tracks this kind of statistics, the  Recorder .  It uses a special piece of state,  learner.cbstate , to store a  History  with this information . learner.cbstate  is an object where callbacks can store state that they want to make available to other callbacks .  Like any other piece of state, the callback writing to it needs to add a  Write()  permission to it using  stateaccess . What makes  cbstate  special is that when creating the callback graph, it is checked that every entry in  cbstate  that is accessed is being created first . The update to the event handler looks like this: We also need to update the definition of  stateaccess  now: Since  Printer  depends on  Recorder  now, an error will be thrown if you try to use  Printer  without  Recorder . And that ’ s it, pass  Printer  to a  Learner  and test it out !  The upside of jumping through some additional hoops is that using the callback in the wrong context will always result in an error, so the user can have peace of mind .","id":"docs/callbacks/custom.html#dependencies"},{"body":"private   accesses   —   function Enumerate all valid state accesses of permissions of kind  perm . accesses((x = Read(),), Read()) === [(:x,)] accesses((x = Read(),), Write()) === []","id":"docstrings/FluxTraining.accesses.html"},{"body":"public   LearningRate   —   type Hyperparameter for the optimizer ’ s learning rate . See  Scheduler  and  hyperparameter scheduling .","id":"docstrings/FluxTraining.LearningRate.html"},{"body":"Features This page gives a run - down of many features  FluxTraining . jl  brings to the table . Most features are implemented as callbacks and using them is as simple as passing the callback when constructing the  Learner :","id":"docs/features.html#features"},{"body":"private   iterpairs   —   function Iterators over the Cartesian product of  a  with itself, skipping any pairs  (a, b)  where  a == b .","id":"docstrings/FluxTraining.iterpairs.html"},{"body":"public   LogVisualization   —   struct Logs images created by  visfn(learner.batch)  to  backends  every  freq  steps . See also  BatchState .","id":"docstrings/FluxTraining.LogVisualization.html"},{"body":"public   TrainingPhase   —   struct A regular training phase .  It iterates over batches in  learner.data.training  and updates the model parameters using  learner.optim  after calculating the gradients . Throws the following events:  EpochBegin ,  BatchBegin , LossBegin ,  BackwardBegin ,  BackwardEnd ,  BatchEnd , EpochEnd .","id":"docstrings/FluxTraining.Phases.TrainingPhase.html"},{"body":"private   stateaccess   —   function Return a named tuple determining what learner state  callback can access . The default is  (;) , the empty named tuple, meaning no state can be accessed . Implementations of  stateaccess  should always return the least permissions possible . For example, the  ToGPU  callback needs to write both the model and the batch data, so its  stateaccess  implementation is: Be careful when defining  stateaccess  that you do return a  NamedTuple .   (x = Read(),)  is one but  (x = Read())  (without the comma) is parsed as an assignment with value  Read() . Defines what  Learner  state is accessed when calling sethyperparameter!  and  gethyperparameter .  This is needed so that  Scheduler  can access the state .","id":"docstrings/FluxTraining.stateaccess.html"},{"body":"Custom callbacks FluxTraining . jl ’ s callback system is built around multiple dispatch, so you specify which part of the training you want to  “ hook into ”  by dispatching on  Phase s and  Event s .  See  Training loop  and  Events  as a reference to phases and events .","id":"docs/callbacks/custom.html#custom-callbacks"},{"body":"private   Events   —   module Provides the abstract  Event  type and concrete event types . Init  is called once for every callback .  Use for initialization code that needs access to some  Learner  state . Events in  TrainingPhase  and  ValidationPhase : EpochBegin  and  EpochEnd , called at the beginning and end of each epoch . BatchBegin  and  BatchEnd , called at the beginning and end of each batch . LossBegin , called after the forward pass but before the loss calculation . TrainingPhase  only: BackwardBegin , called after forward pass and loss calculation but before gradient calculation . BackwardEnd , called after gradient calculation but before parameter update .","id":"docstrings/FluxTraining.Events.html"},{"body":"public   LogMetrics   —   struct Callback that logs step and epoch metrics to one or more  LoggerBackend s . See also  LoggerBackend ,  Loggables.Loggable ,  log_to , TensorBoardBackend Example:","id":"docstrings/FluxTraining.LogMetrics.html"},{"body":"FluxTraining . jl Docs (master) A powerful, extensible neural net training library . FluxTraining . jl  gives you an endlessly extensible training loop for deep learning .  It is inspired by  fastai . It exposes a small set of extensible interfaces and uses them to implement hyperparameter scheduling metrics logging training history; and model checkpointing Install using  ]add FluxTraining . Read  getting started  first and the  user guide  if you want to know more .  See also the  reference  for detailed function documentation . FluxTraining . jl  is part of an ongoing effort to improve Julia ’ s deep learning infrastructure and will be the training library for the work - in - progress  FastAI . jl .  Drop by on the  Julia Zulip  and say hello in the stream  #ml-ecosystem-coordination .","id":"README.html#fluxtrainingjl"},{"body":"public   Phase   —   type Abstract supertype for all phases .  See  subtypes(FluxTraining.Phase) .","id":"docstrings/FluxTraining.Phases.Phase.html"},{"body":"Logging For logging, use the logging callbacks: LogMetrics LogHyperParams LogHistograms They each can have multiple logging backends, but right now the only one implemented in  FluxTraining . jl  is  TensorBoardBackend .  See also  LoggerBackend ,  log_to , and  Loggables.Loggable .","id":"docs/features.html#logging"},{"body":"Example See also  Schedule .","id":"docstrings/FluxTraining.Scheduler.html#example"},{"body":"API The following types and functions can be used to create custom callbacks .  Read the  custom callbacks guide  for more context . Callback stateaccess runafter resolveconflict","id":"docs/callbacks/reference.html#api"},{"body":"public   StopOnNaNLoss   —   struct Stops the training when a NaN loss is encountered .","id":"docstrings/FluxTraining.StopOnNaNLoss.html"},{"body":"private   fitepochphase!   —   function Fit  learner  for one epoch . Customize by deriving custom phase from  Phase .","id":"docstrings/FluxTraining.fitepochphase!.html"},{"body":"Building a  Learner Let ’ s look at the  data  first . FluxTraining . jl  is agnostic of the data source .  The only requirements are: it is iterable and each iteration returns a tuple  (xs, ys) the model can take in  xs , i . e .   model(xs)  works; and the loss function can take model outputs and  ys , i . e .   lossfn(model(xs), ys)  returns a scalar Glossing over the details as it ’ s not the focus of this tutorial, here ’ s the code for getting a data iterator of the MNIST dataset .  We use  DataLoaders.DataLoader  to create an iterator of batches from our dataset . Next, let ’ s create a simple  Flux . jl   model  that we ’ ll train to classify the MNIST digits . We ’ ll use  categorical cross entropy  as a  loss function  and  ADAM  as an  optimizer . Now we ’ re ready to create a  Learner .  At this point you can also add any callbacks, like  ToGPU  to run the training on your GPU if you have one available .  Some callbacks are also  included by default . Since we ’ re classifying digits, we also use the  Metrics  callback to track the accuracy of the model ’ s predictions:","id":"docs/tutorials/mnist.html#building-a-learner"},{"body":"Listing event handlers for a callback Use  Base.methods  to check what events a callback handles:","id":"docs/callbacks/tipstricks.html#listing-event-handlers-for-a-callback"},{"body":"Documentation More complete training tutorials including data pipelines","id":"docs/status.html#documentation"},{"body":"Using callbacks Callbacks allow injecting functionality at many points during the training loop . To use them, pass a list of callbacks to  Learner : Some useful callbacks are added by default: See  callback reference  for a list of all callbacks included in  FluxTraining . jl  and their documentation . The order the callbacks are passed in doesn ’ t matter .   FluxTraining . jl  creates a dependency graph that makes sure the callbacks are run in the correct order .  Read  custom callbacks  to find out how to create callbacks yourself .","id":"docs/callbacks/usage.html#using-callbacks"},{"body":"3 .  State As seen above, the callback handler  on  receives as the last argument a  Learner  instance, allowing the callback to access and modify state . If we wanted to print the last batch ’ s loss instead of a generic message, we could update our definition of  on : (see  Learner  for in - depth documentation of the  Learner ’ s state) The ability to modify any state is very powerful, but it can quickly become problematic when it is unclear which callbacks modify what state and what the correct order should be . Because of that,  FluxTraining . jl  prevents callbacks from reading and modifying state by default .  If we tried to use the above redefinition of  on , we would get the following error: To fix that error, we need to implement  stateaccess , a function that specifies what state a callback is allowed to read and write . In our case, we want to read the loss of the current batch: (see  stateaccess  for more information on how to implement it) After that definition, the above code will run fine . This might seem bothersome, but this extra information makes it possible to analyze state dependencies before any code is run and saves you from running into nasty, hard - to - find bugs that can occur when using many callbacks together .","id":"docs/callbacks/custom.html#state"},{"body":"Examples Schedule  is an alias for  Animations.Animation , see the documentation for more detailed information .","id":"docstrings/FluxTraining.Schedule.html#examples"},{"body":"Setup if you want to run this tutorial yourself, you can find the notebook file  here . To make data loading and batching a bit easier, we ’ ll install some additional dependencies: Now we can import everything we ’ ll need .","id":"docs/tutorials/mnist.html#setup"},{"body":"private   sethyperparameter!   —   function Sets hyperparameter  H  to  value  on  learner .","id":"docstrings/FluxTraining.sethyperparameter!.html"},{"body":"public   Init   —   struct Called once when the learner is created/the callback is added . Hook into this for callback initialization that depends on the Learner ’ s state .","id":"docstrings/FluxTraining.Events.Init.html"},{"body":"Keyword arguments usedefaultcallbacks = true : Whether to add some basic callbacks .  Included are  Metrics ,  Recorder ,  ProgressPrinter , StopOnNaNLoss , and  MetricsPrinter . cbrunner = LinearRunner() : Callback runner to use .","id":"docstrings/FluxTraining.Learner.html#keyword-arguments"},{"body":"Interfaces","id":"docs/overview.html#interfaces"},{"body":"public   CancelBatchException   —   struct Throw during fitting to cancel the currently running batch .","id":"docstrings/FluxTraining.CancelBatchException.html"},{"body":"public   CustomCallback   —   parametric type A callback that runs  f(learner)  every time  TEvent  is triggered in  TPhase . If  f  needs to access learner state, pass  access , a named tuple in the same form as  stateaccess .","id":"docstrings/FluxTraining.CustomCallback.html"},{"body":"Callback reference","id":"docs/callbacks/reference.html#callback-reference"},{"body":"public   EarlyStopping   —   struct Stops training if validation loss hasn ’ t decreased in  patience  epochs .","id":"docstrings/FluxTraining.EarlyStopping.html"},{"body":"Arguments model data : Tuple of data iterators in the order  (traindata, valdata, [testdata]) . Must be iterable and return tuples of  (xs, ys) lossfn : Function with signature  lossfn(model(x), y) -> Number optimizer callbacks... : Any other unnamed arguments are callbacks","id":"docstrings/FluxTraining.Learner.html#arguments"},{"body":"A guided example There are 4 things you need to do to implement a custom callback: Create a callback  struct  that subtypes  Callback Write event handlers with  on Define what state the callback accesses by implementing  stateaccess (Optionally) define dependencies on other callbacks with  runafter Let ’ s go through them one at a time by implementing a simple callback that prints something after every batch .","id":"docs/callbacks/custom.html#a-guided-example"},{"body":"Training With a  Learner  inplace, training is as simple as calling  fit! (learner, nepochs) .","id":"docs/tutorials/mnist.html#training"},{"body":"Tips  &  tricks","id":"docs/callbacks/tipstricks.html#tips--tricks"},{"body":"private   BatchState   —   struct Stores data of the last processed batch .","id":"docstrings/FluxTraining.BatchState.html"},{"body":"Included callbacks FluxTraining . jl  comes included with many callbacks .  Some of them are added to  Learner  by default, here marked with a  * .","id":"docs/callbacks/reference.html#included-callbacks"},{"body":"public   BatchEnd   —   struct Event  called at the end of a batch .","id":"docstrings/FluxTraining.Events.BatchEnd.html"},{"body":"Callback execution By default, a topological ordering of the callbacks is created from the dependency graph and the callbacks are executed serially . This behavior can be overwritten with custom callback executors, for example to create a  Dagger . jl  node from the graph to allow callbacks to safely run in parallel where valid .","id":"docs/callbacks/custom.html#callback-execution"},{"body":"private   Loggables   —   module Defines  Loggables.Loggable  and its subtypes .","id":"docstrings/FluxTraining.Loggables.html"},{"body":"public   EpochEnd   —   struct Event  called at the end of an epoch .","id":"docstrings/FluxTraining.Events.EpochEnd.html"},{"body":"public   Scheduler   —   struct Callback for hyperparameter scheduling .  Takes pairs of  HyperParameter types and  Schedule s . See  the tutorial .","id":"docstrings/FluxTraining.Scheduler.html"},{"body":"public   Metric   —   parametric type Implementation of  AbstractMetric  that can be used with the Metrics  callback .","id":"docstrings/FluxTraining.Metric.html"},{"body":"public   EpochBegin   —   struct Event  called at the beginning of an epoch .","id":"docstrings/FluxTraining.Events.EpochBegin.html"},{"body":"2 .  Event handlers Now we need to add an event handler so that  Printer  can run some code when a batch ends . Event handlers can be defined by adding a method to  FluxTraining.on .  It takes as arguments an  event , a  phase , the callback and the learner: on(event::Event, phase::Phase, callback::Callback, learner) The  event ,  phase  and  callback  are used to dispatch . In this case, we want to run code at the end of a batch, so the event we need to dispatch on is  BatchEnd .  We want it to run in any phase, so we use the abstract type  Phase .  The third argument type is the callback we want to add an event handler to .  This gives us: We can now pass an instance of  Printer  when creating a  Learner  and the message will be printed at the end of every batch .","id":"docs/callbacks/custom.html#event-handlers"},{"body":"private   LoggerBackend   —   type Backend for logging callbacks like . To add support for logging  Loggables.Loggable   L  to backend  B , implement log_to (backend::B, loggable::L, names, i) See also  LogMetrics ,  LogHyperParams ,  log_to","id":"docstrings/FluxTraining.LoggerBackend.html"},{"body":"Training loop Find out how  the training loop  is built and how to customize it .","id":"docs/overview.html#training-loop"},{"body":"private   Callback   —   type Callbacks can add custom functionality to the training loop . See  custom callbacks  for more info .","id":"docstrings/FluxTraining.Callback.html"},{"body":"public   BackwardBegin   —   struct Event  called between calculating loss and calculating gradients","id":"docstrings/FluxTraining.Events.BackwardBegin.html"},{"body":"Hyperparameter scheduling When training neural networks, you often have to tune hyperparameters . In  FluxTraining . jl  the following definition is used: A hyperparameter is any state that influences the training and is not a parameter of the model . Common hyperparameters to worry about are the learning rate, batch size and regularization strength . In recent years, it has also become common practice to schedule some hyperparameters .  The cyclical learning rate schedule introduced in  L .  Smith 2015 , for example, changes the learning rate every step to speed up convergence . FluxTraining . jl  provides an extensible interface for hyperparameter scheduling that is not restricted to optimizer hyperparameters as in many other training frameworks .  To use it, you have to create a  Scheduler , a callback that can be passed to a  Learner . Scheduler ’ s constructor takes pairs of hyperparameter types and associated schedules . As an example LearningRate  is a hyperparameter type representing the optimizer ’ s step size; and schedule = Schedule([0, 10], [0.01, 0.001])  represents a linear scheduling over 10 epochs from  0.01  to  0.001 We can create the callback scheduling the learning rate according to  Scheduler(LearningRate => schedule) . Schedule s are built around  Animations . jl .  See that package ’ s documentation or  Schedule ’ s for more details on how to construct them .","id":"docs/tutorials/hyperparameters.html#hyperparameter-scheduling"},{"body":"public   Metrics   —   struct Callback that tracks metrics during training . metrics  can be both  AbstractMetric s or functions like  f(ŷs, ys)  which will be converted to  Metric s . A metric tracking  lossfn  is included by default .","id":"docstrings/FluxTraining.Metrics.html"},{"body":"Checkpointing Use the  Checkpointer  callback to create model checkpoints after every epoch .","id":"docs/features.html#checkpointing"},{"body":"public   Schedule   —   parametric type Describes how the values of a hyperparameter change over the training .","id":"docstrings/FluxTraining.Schedule.html"},{"body":"public   ProgressPrinter   —   struct Prints a progress bar of the currently running epoch .","id":"docstrings/FluxTraining.ProgressPrinter.html"},{"body":"Ecosystem Unlike fastai,  FluxTraining  focuses on the training part of the deep learning pipeline .  Other packages you may find useful are Metalhead . jl  and  FluxModels . jl  for models Augmentor . jl  and  DataAugmentation . jl  for data augmentation DataLoaders . jl  for parallel data loading; and DLDatasets . jl  for datasets","id":"docs/ecosystem.html#ecosystem"},{"body":"public   SanityCheck   —   struct Callback that runs sanity  Check s when the  Learner  is initialized . If  usedefault  is  true , it will run all checks in FluxTraining . CHECKS in addition to the ones you pass in .","id":"docstrings/FluxTraining.SanityCheck.html"},{"body":"public   ToGPU   —   struct Callback that moves model and batch data to the GPU during training .","id":"docstrings/FluxTraining.ToGPU.html"},{"body":"Extending You can create and schedule your own hyperparameters . To do this, you will need to define a type for your hyperparameter, e . g .   abstract type MyParam <: HyperParameter end , how to set the hyperparameter by implementing  sethyperparameter! (learner, ::Type{MyParam}, value) what state needs to be accessed to set the hyperparameter by implementing  stateaccess (::Type{MyParam}) .  See  custom callbacks  for more info on why this is needed . Hyperparameters don ’ t need to belong to the optimizer !  For example, you could create a hyperparameter for batch size .  That is not implemented here because this package is agnostic of the data iterators and the implementation would differ for every type of iterator .","id":"docs/tutorials/hyperparameters.html#extending"},{"body":"private   FitException   —   type Abstract types for exceptions that can be thrown during fitting, to change its control flow . See  CancelBatchException ,  CancelEpochException ,  CancelFittingException .","id":"docstrings/FluxTraining.FitException.html"},{"body":"private   ConflictResolution   —   type A conflict resolution strategy for resolving write/write conflicts of two callbacks . See  resolveconflict .","id":"docstrings/FluxTraining.ConflictResolution.html"},{"body":"Training loop EarlyStopping StopOnNaNLoss * Scheduler ToGPU","id":"docs/callbacks/reference.html#training-loop"},{"body":"Metrics By default,  Learner  will track only the loss function .  You can track other metric with the  Metrics  callback .  See also  Metric ,  AbstractMetric .","id":"docs/features.html#metrics"},{"body":"public   CancelFittingException   —   struct Throw during fitting to cancel it .","id":"docstrings/FluxTraining.CancelFittingException.html"},{"body":"private   Loggable   —   type Abstract type for data that  LoggerBackend s can log . See  subtypes(FluxTraining.Loggables.Loggable)  and  LoggerBackend","id":"docstrings/FluxTraining.Loggables.Loggable.html"},{"body":"public   ValidationPhase   —   struct A regular validation phase .  It iterates over batches in  learner.data.validation  and performs a forward pass . Throws the following events:  EpochBegin ,  BatchBegin , LossBegin ,  BatchEnd ,  EpochEnd .","id":"docstrings/FluxTraining.Phases.ValidationPhase.html"},{"body":"Utilities There are also some callback utilities: CustomCallback throttle","id":"docs/callbacks/reference.html#utilities"},{"body":"Fields (Use this as a reference when implementing callbacks) model ,  optimizer , and  lossfn  are stored as passed in data  is a  NamedTuple  of  (training = ..., validation = ..., test = ...) . Some values might be  nothing  if you didn ’ t pass in multiple data iterators . params : an instance of  model ’ s parameters of type  Flux.Params batch : State of the current batch, including: batch.xs : model inputs batch.ys : target outputs batch.ŷs : model outputs, i . e .   model(xs) batch.loss : batch loss, i . e .   lossfn(ŷs, ys) batch.gs : batch gradients, instance of  Zygote.Grads ( ! ) Note: Depending on the progress of the step, some fields may be  nothing , e . g .  the  gs  before the backward pass . cbstate:: PropDict : Special state container that callbacks can save state to for other callbacks .  Its keys depend on what callbacks are being used .  See the  custom callbacks guide for more info .","id":"docstrings/FluxTraining.Learner.html#fields"},{"body":"Training loop The training loop, of course, is the core component in  FluxTraining . jl . Once you have a  Learner , the simplest way to start training is to run  fit! (learner, n) .  This will train for  n  epochs of one training phase and one validation phase each . Like the  callback system , the training loop has an extensible interface based on multiple dispatch . The key abstraction are  Phase s .  In fact, the above  fit!(learner, n)  is just a shorthand for running  fit!(learner, [TrainingPhase(), ValidationPhase()])   n  times . Each phase implements its own training logic which is run when calling  fit!(learner, phase) : TrainingPhase  does what you expect it to: it uses the training data iterator ( learner.data.training ), iterates over the batches, computes the forward and backward pass for each batch and updates the model parameters . ValidationPhase  uses the validation data to compute the forward pass If this looks like a regular old training loop, you ’ re right !  What makes the training loop customizable is the callback system .  During training,  Event s are thrown that  callbacks can hook into . For a rundown of events, see  Events .","id":"docs/training/basics.html#training-loop"},{"body":"Example","id":"docstrings/FluxTraining.LogHyperParams.html#example"},{"body":"Getting started In  FluxTraining , a  Learner  holds all state necessary for training .  To get started, you need a  model training and validation  data iterators a  loss function ; and an  optimizer Models and datasets are not included with FluxTraining . jl, so see  ecosystem  for supporting packages . Let ’ s look at a simple training example . First we define the necessary pieces: Then we construct a  Learner : And train for 10 epochs:","id":"docs/getting_started.html#getting-started"},{"body":"public   MetricsPrinter   —   struct Prints metrics after every epoch .  Relies on  Metrics .","id":"docstrings/FluxTraining.MetricsPrinter.html"},{"body":"public   LossBegin   —   struct Event  called between calculating  y_pred  and calculating loss","id":"docstrings/FluxTraining.Events.LossBegin.html"},{"body":"public   onecycle   —   function Creates a one - cycle  Schedule  over  nepochs  epochs from  start_val over  max_val  to  end_val .","id":"docstrings/FluxTraining.onecycle.html"},{"body":"private   AbstractMetric   —   type Abstract type for metrics passed to  Metrics . For most use cases, you should use  Metric , the standard implementation .","id":"docstrings/FluxTraining.AbstractMetric.html"},{"body":"public   GarbageCollect   —   function Every  nsteps  steps, forces garbage collection . Use this if you get memory leaks from, for example, parallel data loading . Performs an additional C - call on Linux systems that can sometimes help .","id":"docstrings/FluxTraining.GarbageCollect.html"},{"body":"private   Phases   —   module","id":"docstrings/FluxTraining.Phases.html"},{"body":"public   Checkpointer   —   struct Saves  learner.model  to  folder  after every  [ AbstractTrainingPhase ] . Use  FluxTraining. loadmodel  to load a model .","id":"docstrings/FluxTraining.Checkpointer.html"},{"body":"public   TensorBoardBackend   —   struct TensorBoard backend for logging callbacks .  Takes the same arguments as  TensorBoardLogger.TBLogger .","id":"docstrings/FluxTraining.TensorBoardBackend.html"},{"body":"private   HyperParameter   —   parametric type A hyperparameter is any state that influences the training and is not a parameter of the model . Hyperparameters can be scheduled using the  Scheduler callback .","id":"docstrings/FluxTraining.HyperParameter.html"},{"body":"private   log_to   —   function Log  loggable  to  backend  with  group  to index  i . loggable  is any  Loggables.Loggable group  can be a  String  or a tuple of  String s implying some grouping which can be used by a supporting backend . i  is a step counter and unique for every group .","id":"docstrings/FluxTraining.log_to.html"},{"body":"public   BatchBegin   —   struct Event  called at the beginning of a batch .","id":"docstrings/FluxTraining.Events.BatchBegin.html"},{"body":"public   Learner   —   struct Holds and coordinates all state of the training .   model  is trained by optimizing  lossfn  with  optimizer  on  data .","id":"docstrings/FluxTraining.Learner.html"},{"body":"public   loadmodel   —   function Loads a model that was saved to  path  using  FluxTraining. savemodel .","id":"docstrings/FluxTraining.loadmodel.html"},{"body":"Features ToTorch  callback: like  ToGPU  but uses  Torch . jl .  Should be implemented using  Requires . jl  to forego the heavy dependency . Weights & Biases  backend for logging callbacks More functional metrics, i . e .   metric(y_pred, y)  (atm the only one is  accuracy ) .   Metrics implemented in fastai  could be an inspiration .","id":"docs/status.html#features"},{"body":"Examples Metric(accuracy) Metric(Flux.mse, device = gpu) Metric(Flux.mae, device = gpu)","id":"docstrings/FluxTraining.Metric.html#examples"},{"body":"Let ’ s put  FluxTraining . jl  to train a model on the MNIST dataset . MNIST is simple enough that we can focus on the part where  FluxTraining . jl  comes in, the training .","id":"docs/tutorials/mnist.html"},{"body":"public   Event   —   type Abstract type for events that callbacks can hook into","id":"docstrings/FluxTraining.Events.Event.html"},{"body":"public   LogHistograms   —   struct Callback that logs histograms of model weights to  LoggerBackend s backends  every  freq  steps . If histograms should be logged every step, pass  freq = nothing","id":"docstrings/FluxTraining.LogHistograms.html"}]