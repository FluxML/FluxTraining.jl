{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an image classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put *FluxTraining.jl* to train a model on the MNIST dataset.\n",
    "\n",
    "MNIST is simple enough that we can focus on the part where *FluxTraining.jl* comes in, the training. If you want to see examples of using FluxTraining.jl on larger datasets, see the documentation of [FastAI.jl](https://github.com/FluxML/FastAI.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you want to run this tutorial yourself, you can find the notebook file [here](https://github.com/lorenzoh/FluxTraining.jl/blob/master/docs/tutorials/mnist.ipynb)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make data loading and batching a bit easier, we'll install an additional dependency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "using Pkg; Pkg.add([\"MLUtils\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import everything we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLUtils: splitobs, unsqueeze\n",
    "using MLDatasets: MNIST\n",
    "using Flux\n",
    "using Flux: onehotbatch\n",
    "using Flux.Data: DataLoader\n",
    "using FluxTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 pieces that you always need to construct and train a [`Learner`](#):\n",
    "\n",
    "- a model\n",
    "- data\n",
    "- an optimizer; and\n",
    "- a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a `Learner`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the **data** first.\n",
    "\n",
    "*FluxTraining.jl* is agnostic of the data source. The only requirements are:\n",
    "\n",
    "- it is iterable and each iteration returns a tuple `(xs, ys)`\n",
    "- the model can take in `xs`, i.e. `model(xs)` works; and\n",
    "- the loss function can take model outputs and `ys`, i.e. `lossfn(model(xs), ys)` returns a scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glossing over the details as it's not the focus of this tutorial, here's the code for getting a data iterator of the MNIST dataset. We use `DataLoaders.DataLoader` to create an iterator of batches from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST(:train)[:]\n",
    "\n",
    "const LABELS = 0:9\n",
    "\n",
    "# unsqueeze to reshape from (28, 28, numobs) to (28, 28, 1, numobs)\n",
    "function preprocess((data, targets))\n",
    "    return unsqueeze(data, 3), onehotbatch(targets, LABELS)\n",
    "end\n",
    "\n",
    "\n",
    "# traindata and testdata contain both inputs (pixel values) and targets (correct labels)\n",
    "traindata = MNIST(Float32, :train)[:] |> preprocess\n",
    "testdata = MNIST(Float32, :test)[:] |> preprocess\n",
    "\n",
    "# create iterators\n",
    "trainiter, testiter = DataLoader(traindata, batchsize=128), DataLoader(testdata, batchsize=256);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a simple *Flux.jl* **model** that we'll train to classify the MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Conv((3, 3), 1 => 16, relu, pad=1, stride=2),  \u001b[90m# 160 parameters\u001b[39m\n",
       "  Conv((3, 3), 16 => 32, relu, pad=1),  \u001b[90m# 4_640 parameters\u001b[39m\n",
       "  GlobalMeanPool(),\n",
       "  Flux.flatten,\n",
       "  Dense(32 => 10),                      \u001b[90m# 330 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m5_130 parameters, 20.867 KiB."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Conv((3, 3), 1 => 16, relu, pad = 1, stride = 2),\n",
    "    Conv((3, 3), 16 => 32, relu, pad = 1),\n",
    "    GlobalMeanPool(),\n",
    "    Flux.flatten,\n",
    "    Dense(32, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use *categorical cross entropy* as a **loss function** and *ADAM* as an **optimizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn = Flux.Losses.logitcrossentropy\n",
    "optimizer = Flux.ADAM();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create a [`Learner`](#). At this point you can also add any callbacks, like [`ToGPU`](#) to run the training on your GPU if you have one available. Some callbacks are also [included by default](../callbacks/reference.md).\n",
    "\n",
    "Since we're classifying digits, we also use the [`Metrics`](#) callback to track the accuracy of the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = Learner(model, lossfn; callbacks=[ToGPU(), Metrics(accuracy)], optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `Learner` in place, training is as simple as calling [`fit!`](#)`(learner, nepochs, dataiters)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 TrainingPhase() ...\n"
     ]
    }
   ],
   "source": [
    "FluxTraining.fit!(learner, 10, (trainiter, testiter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
